
@article{mukherjee_cellphones_2011,
	title = {Do Cellphones Cause Brain Cancer?},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2011/04/17/magazine/mag-17cellphones-t.html},
	abstract = {Description: An excellent overview of the limitations of Epidemiological research, illustrated by the perplexing findings about the link (or lack thereof) between cell phone use and brain cancer. My only complaint is that they did not address how difficult it is to decide when to close the door on further research. How many negative studies can you tolerate before you say, "Enough! Let's move on to something else."},
	journaltitle = {The New York Times},
	author = {Mukherjee, Siddhartha},
	urldate = {2011-05-04},
	date = {2011-04-13},
	keywords = {Brain, Cancer, Cellular Telephones, Medicine and Health, Radiation}
}

@article{kjaergard_association_2002,
	title = {Association between competing interests and authors' conclusions: epidemiological study of randomised clinical trials published in the {BMJ}},
	volume = {325},
	issn = {09598138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.325.7358.249},
	doi = {10.1136/bmj.325.7358.249},
	shorttitle = {Association between competing interests and authors' conclusions},
	abstract = {Description: This article notes that publications noting financial competing interests led to different reporting results compared to publications reporting no competing interests. This effect could not be explained by methodological quality or other factors. Interestingly, publications reporting other types of competing interests did not differ in reporting results. Abstract "Objective: To assess the association between competing interests and authors' conclusions in randomised clinical trials. Design: Epidemiological study of randomised clinical trials published in the {BMJ} from January 1997 to June 2001. Financial competing interests were defined as funding by for profit organisations and other competing interests as personal, academic, or political. Studies: 159 trials from 12 medical specialties. Main outcome measures: Authors' conclusions defined as interpretation of extent to which overall results favoured experimental intervention. Conclusions appraised on 6 point scale; higher scores favour experimental intervention. Results: Authors' conclusions were significantly more positive towards the experimental intervention in trials funded by for profit organisations alone compared with trials without competing interests (mean difference 0.48 ({SE} 0.13), P=0.014), trials funded by both for profit and non-profit organisations (0.30 ({SE} 0.10), P=0.003), and trials with other competing interests (0.45 ({SE} 0.13), P=0.006). Other competing interests and funding from both for profit and non-profit organisations were not significantly associated with authors' conclusions. The association between financial competing interests and authors' conclusions was not explained by methodological quality, statistical power, type of experimental intervention (pharmacological or non-pharmacological), type of control intervention (for example, placebo or active drug), or medical specialty. Conclusions: Authors' conclusions in randomised clinical trials significantly favoured experimental interventions if financial competing interests were declared. Other competing interests were not significantly associated with authors' conclusions."},
	pages = {249--249},
	number = {7358},
	journaltitle = {{BMJ}},
	author = {Kjaergard, L. L},
	urldate = {2011-04-04},
	date = {2002-08}
}

@online{munroe_xkcd:_nodate,
	title = {xkcd: Null Hypothesis},
	url = {http://xkcd.com/892/},
	abstract = {Excerpt: "I can't believe schools are still teaching kids about the null hypothesis."},
	author = {Munroe, Randall},
	urldate = {2011-05-16}
}

@online{graps_introduction_nodate,
	title = {An Introduction to Wavelets},
	url = {http://www.amara.com/IEEEwave/IEEEwavelet.html},
	abstract = {Abstract: "Wavelets are mathematical functions that cut up data into different frequency components, and then study each component with a resolution matched to its scale. They have advantages over traditional Fourier methods in analyzing physical situations where the signal contains discontinuities and sharp spikes. Wavelets were developed independently in the fields of mathematics, quantum physics, electrical engineering, and seismic geology. Interchanges between these fields during the last ten years have led to many new wavelet applications such as image compression, turbulence, human vision, radar, and earthquake prediction. This paper introduces wavelets to the interested technical person outside of the digital signal processing field. I describe the history of wavelets beginning with Fourier, compare wavelet transforms with Fourier transforms, state properties and other special aspects of wavelets, and finish with some interesting applications such as image compression, musical tones, and de-noising noisy data. Keywords: Wavelets, Signal Processing Algorithms, Orthogonal Basis Functions, Wavelet Applications."},
	author = {Graps, Amara Lynn},
	urldate = {2011-05-12}
}

@online{greenman_i_nodate,
	title = {I Love Charts – Ben Greenman’s Museum of Silly Charts},
	url = {http://ilovecharts.tumblr.com/BenGreenman},
	abstract = {Excerpt: "My interest in charts springs primarily from my disinterest in charts. Reality is so messy. Nothing that passes for a fact ever really is one. When I was a kid, infographics were intoxicating because they promised a world of order, and it’s specifically this promise that has come, over time, to seem like a cruel deception. You’ll never really get an elegant presentation of so-called facts that accurately and meaningfully represents reality. So, as much as I’m drawn to those charts, I’m also drawn to charts that obfuscate, or thwart, or somehow sharpen our sense of the absurdity of trying to accurately present and analyze information."},
	author = {Greenman, Ben},
	urldate = {2011-05-11}
}

@online{hofman_who_2011,
	title = {Who Says What to Whom on Twitter},
	url = {http://research.yahoo.com/pub/3386},
	abstract = {Abstract: "We study several longstanding questions in media communications research, in the context of the microblogging service Twitter, regarding the production, flow, and consumption of information. To do so, we exploit a recently introduced feature of Twitter---known as Twitter lists---to distinguish between elite users, by which we mean specifically celebrities, bloggers, and representatives of media outlets and other formal organizations, and ordinary users. Based on this classification, we find a striking concentration of attention on Twitter---roughly 50\% of tweets consumed are generated by just 20K elite users---where the media produces the most information, but celebrities are the most followed. We also find significant homophily within categories: celebrities listen to celebrities, while bloggers listen to bloggers etc; however, bloggers in general rebroadcast more information than the other categories. Next we re-examine the classical ``two-step flow'' theory of communications, finding considerable support for it on Twitter, but also some interesting differences. Third, we find that {URLs} broadcast by different categories of users or containing different types of content exhibit systematically different lifespans. And finally, we examine the attention paid by the different user categories to different news topics."},
	author = {Hofman, Jake and Mason, Winter and Watts, Duncan and Wu, Shaomei},
	date = {2011}
}

@online{jansma_why_nodate,
	title = {Why Twitter can be the Next Big Thing in Scientific Collaboration},
	url = {http://mastersofmedia.hum.uva.nl/2011/05/02/why-twitter-can-be-the-next-big-thing-in-scientific-collaboration/},
	abstract = {Excerpt: "Imagine yourself in the following situation: You, a scientist pur sang, are busy researching and analyzing A and you are having doubts about the values in the model, suspecting a technical error. Without hesitation, you compose a tweet describing the research and the problem, attach a photo made of the model, add a hashtag (e.g. \#science or \#labhelp) and send it to Twitter.  A few moments later six people have replied with an answer and your problem is solved."},
	author = {Jansma, Sander},
	urldate = {2011-05-07}
}

@online{holmes_role_nodate,
	title = {The Role of a Wine Pricing Competition in Teaching Data Mining at Stanford},
	url = {http://www.causeweb.org/webinar/activity/2011-04/},
	abstract = {Excerpt: "We will discuss how we coordinated, held, and judged a wine pricing competition (hosted on Kaggle-in-Class - inclass.kaggle.com) to engage students in applying prediction techniques learned in our data mining class at Stanford. We found that with proper incentives, the competition was very successful in getting students interested in working collaboratively in a race against the clock to eke out additional predictive performance in their models."},
	author = {Holmes, Susan and Ray, Nelson},
	urldate = {2011-05-06}
}

@book{epstein_impure_1996,
	title = {Impure Science: {AIDS}, Activism, and the Politics of Knowledge},
	isbn = {9780520214453},
	shorttitle = {Impure Science},
	abstract = {Excerpt: "This book is a study of how varied classes of {AIDS} experts, diverse conceptions of scientific practice, and distinct claims of knowledge about {AIDS} have all been generated out of relationships of conflict and cooperation in the United States since the early 1980s. Inside a large and often floodlit arean with a diffuse and porous perimeter, an eclectic assortment of actors has sought to assert and assess credible knowledge about {AIDS}: biomedical researchers and health care professionals of different stripes; activists, advocacy groups, and people with {AIDS} or {HIV} infection; health educators and social scientists; politicians and public health officials; government agencies and advisory committees; pharmaceutical and biotechnology companies; writers, journalists, and the institutions of the mainstream and alternative media. 'What we know about {IADS}' is the product of this elaborate, often heated, and in some ways quite peculiar complex of interactions."},
	publisher = {University of California Press},
	author = {Epstein, Steven},
	date = {1996-01-01}
}

@online{bland_multiple_nodate,
	title = {Multiple significance tests and the Bonferroni correction},
	url = {http://www-users.york.ac.uk/~mb55/intro/bonf.htm},
	abstract = {Excerpt: "If we test a null hypothesis which is in fact true, using 0.05 as the critical significance level, we have a probability of 0.95 of coming to a `not significant' (i.e. correct) conclusion. If we test two independent true null hypotheses, the probability that neither test will be significant is 0.95 times 0.95 = 0.90 (Section 6.2). If we test twenty such hypotheses the probability that none will be significant is 0.9520 = 0.36. This gives a probability of 1 - 0.36 = 0.64 of getting at least one significant result; we are more likely to get one than not. The expected number of spurious significant results is 20 times 0.05 = 1. Many medical research studies are published with large numbers of significance tests. These are not usually independent, being carried out on the same set of subjects, so the above calculations do not apply exactly. However, it is clear that if we go on testing long enough we will find something which is `significant'. We must beware of attaching too much importance to a lone significant result among a mass of non-significant ones. It may be the one in twenty which we should get by chance alone. "},
	author = {Bland, Martin},
	urldate = {2011-02-23}
}

@online{the_r_foundation_for_statistical_computing_r:_nodate,
	title = {R: Regulatory Compliance and Validation Issues A Guidance Document for the Use of R in Regulated Clinical Trial Environments},
	url = {http://www.r-project.org/doc/R-FDA.pdf},
	abstract = {Excerpt: "This document will address speci⬚c areas within the {GxP} domain. It is intended to provide a reasonable consensus position on the part of the R Foundation for Statistical Computing (hereafter referred to as the R Foundation) relative to the use of R within these regulated environments and to provide a common foundation for end users to meet their own internal standard operating procedures, documentation requirements and regulatory obligations."},
	author = {The R Foundation for Statistical Computing},
	urldate = {2011-05-03}
}

@online{soukop_using_nodate,
	title = {Using R: Perspectives of a {FDA} Statistical {RevieweR}},
	url = {http://user2007.org/program/presentations/soukup.pdf},
	abstract = {Description: A {PowerPoint} presentation at a recent {UseR} conference that discusses validation requirements in general, and specific guidance for validation of R.},
	author = {Soukop, Mat},
	urldate = {2011-05-03}
}

@online{lew_migrating_nodate,
	title = {Migrating to R for {SAS} / {SPSS} / Stata Users},
	url = {http://scc.stat.ucla.edu/page_attachments/0000/0115/09F_migrating.pdf},
	author = {Lew, Vivan},
	urldate = {2011-05-03}
}

@online{nabble_r_nodate,
	title = {R help list},
	url = {http://r.789695.n4.nabble.com/},
	abstract = {Description: This is a web-based interface to the R-help email help list.},
	author = {Nabble},
	urldate = {2011-05-03}
}

@online{nagoya_university_graduate_school_of_law_csl_nodate,
	title = {{CSL} Metadata Field Index},
	url = {http://gsl-nagoya-u.net/http/pub/csl-fields/},
	abstract = {Description: The fields in Zotero and the fields in {CSL} match up pretty closely, but there are some variants. This page helps you figure out things like how {CSL} expects to see motion\_picture for a {videoRecording} in Zotero.},
	author = {Nagoya University Graduate School of Law},
	urldate = {2011-04-27}
}

@online{zotero_zotero_nodate,
	title = {Zotero Reference Test pane},
	url = {chrome://zotero/content/tools/csledit.xul},
	abstract = {Description: You can load your Zotero style on this webpage and it will show you what the citations and bibliography look like. When you make changes to your style on this page, the citations and bibliography update automatically.},
	author = {Zotero},
	urldate = {2011-04-27}
}

@video{stewart_indecision_2011,
	title = {Indecision 2012 - Premature Ecalculation - The Daily Show with Jon Stewart - 04/25/11 - Video Clip {\textbar} Comedy Central},
	url = {http://www.thedailyshow.com/watch/mon-april-25-2011/indecision-2012---premature-ecalculation},
	abstract = {Description: "Donald Trump may be leading the {GOP} field in the polls, but early numbers are completely and totally meaningless."},
	author = {Stewart, Jon},
	urldate = {2011-04-26},
	date = {2011-04-25}
}

@online{noauthor_why_2011,
	title = {Why Open Science won't work {\textbar} {GrrlScientist} {\textbar} Science {\textbar} guardian.co.uk},
	url = {http://www.guardian.co.uk/science/punctuated-equilibrium/2011/apr/19/1?commentpage=last#end-of-comments},
	urldate = {2011-04-26},
	date = {2011-04-19}
}

@online{tacke_open_nodate,
	title = {Open Science 2.0: How Research and Education
can bene⬚t from Open Innovation and Web 2.0},
	url = {http://www.olivertacke.de/wp-content/uploads/2011/02/Tacke-2010-Open_Science_2.0-preview-100613-OLT.pdf},
	abstract = {Abstract: "Both, Open Innovation and Web 2.0, are concepts used in commerce in order to support the collaboration of di⬚erent people and the emergence of new ideas. The approaches can be adapted to science, thus o⬚ering new opportunities for research and education. If necessary requirements are satis⬚ed, Open Science 2.0 facilitates e.g. the public development of scienti⬚c papers and the conduct of public seminars, both harnessing collective intelligence. This way, it is not only possible to improve the individual outcomes, but also to encourage the exchange between theory and practice."},
	author = {Tacke, Oliver},
	urldate = {2011-04-26}
}

@article{stuckler_global_2011,
	title = {Global Health Philanthropy and Institutional Relationships: How Should Conflicts of Interest Be Addressed?},
	volume = {8},
	url = {http://dx.doi.org/10.1371/journal.pmed.1001020},
	doi = {10.1371/journal.pmed.1001020#pmed.1001020-Robert1},
	shorttitle = {Global Health Philanthropy and Institutional Relationships},
	abstract = {Description: "David Stuckler and colleagues examine five large private global health foundations and report on the scope of relationships between these tax-exempt foundations and for-profit corporations including major food and pharmaceutical companies."},
	pages = {e1001020},
	number = {4},
	journaltitle = {{PLoS} Med},
	shortjournal = {{PLoS} Med},
	author = {Stuckler, David and Basu, Sanjay and {McKee}, Martin},
	urldate = {2011-04-26},
	date = {2011-04-12}
}

@article{miladinovic_instrumental_2011,
	title = {Instrumental variable meta-analysis of individual patient data: application to adjust for treatment non-compliance},
	volume = {11},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/11/55},
	doi = {10.1186/1471-2288-11-55},
	shorttitle = {Instrumental variable meta-analysis of individual patient data},
	abstract = {Abstract: "{BACKGROUND}: Intention-to-treat ({ITT}) is the standard data analysis method which includes all patients regardless of receiving treatment. Although the aim of {ITT} analysis is to prevent bias due to prognostic dissimilarity, it is also a counter-intuitive type of analysis as it counts patients who did not receive treatment, and may lead to "bias toward the null." As treated ({AT}) method analyzes patients according to the treatment actually received rather than intended, but is affected by the selection bias. Both {ITT} and {AT} analyses can produce biased estimates of treatment effect, so instrumental variable ({IV}) analysis has been proposed as a technique to control for bias when using {AT} data. Our objective is to correct for bias in non-experimental data from previously published individual patient data meta-analysis by applying {IV} methods. {METHODS}: Center prescribing preference was used as an {IV} to assess the effects of methotrexate ({MTX}) in preventing debilitating complications of chronic graft-versus-host-disease ({cGVHD}) in patients who received peripheral blood stem cell ({PBSCT}) or bone marrow transplant ({BMT}) in nine randomized controlled trials (1107 patients). {IV} methods are applied using 2-stage logistic, 2-stage probit and generalized method of moments models. {RESULTS}: {ITT} analysis showed a statistically significant detrimental effect with the use of day 11 {MTX}, resulting in {cGVHD} odds ratio ({OR}) of 1.34 (95\% {CI} 1.02-1.76). {AT} results showed no difference in the odds of {cGVHD} with the use of {MTX} [{OR} 1.31 (95\%{CI} 0.99-1.73)]. {IV} analysis further corrected the results toward no difference in the odds of {cGVHD} between {PBSCT} vs. {BMT}, allowing for a possibility of beneficial effects of {MTX} in preventing {cGVHD} in {PBSCT} recipients ({OR} 1.14; 95\%{CI} 0.83-1.56). {CONCLUSION}: All instrumental variable models produce similar results. {IV} estimates correct for bias and do not exclude the possibility that {MTX} may be beneficial, contradicting the {ITT} analysis."},
	pages = {55},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Miladinovic, Branko and Kumar, Ambuj and Hozo, Iztok and Djulbegovic, Benjamin},
	urldate = {2011-04-26},
	date = {2011}
}

@article{kaufman_flexible_2011,
	title = {A flexible Bayesian hierarchical model of preterm birth risk among {US} Hispanic subgroups in relation to maternal nativity and education},
	volume = {11},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/11/51},
	doi = {10.1186/1471-2288-11-51},
	abstract = {Abstract: "{BACKGROUND}: Previous research has documented heterogeneity in the effects of maternal education on adverse birth outcomes by nativity and Hispanic subgroup in the United States. In this article, we considered the risk of preterm birth ({PTB}) using 9 years of vital statistics birth data from New York City. We employed finer categorizations of exposure than used previously and estimated the risk dose-response across the range of education by nativity and ethnicity. {METHODS}: Using Bayesian random effects logistic regression models with restricted quadratic spline terms for years of completed maternal education, we calculated and plotted the estimated posterior probabilities of {PTB} (gestational age {\textless} 37 weeks) for each year of education by ethnic and nativity subgroups adjusted for only maternal age, as well as with more extensive covariate adjustments. We then estimated the posterior risk difference between native and foreign born mothers by ethnicity over the continuous range of education exposures. {RESULTS}: The risk of {PTB} varied substantially by education, nativity and ethnicity. Native born groups showed higher absolute risk of {PTB} and declining risk associated with higher levels of education beyond about 10 years, as did foreign-born Puerto Ricans. For most other foreign born groups, however, risk of {PTB} was flatter across the education range. For Mexicans, Central Americans, Dominicans, South Americans and "Others", the protective effect of foreign birth diminished progressively across the educational range. Only for Puerto Ricans was there no nativity advantage for the foreign born, although small numbers of foreign born Cubans limited precision of estimates for that group. {CONCLUSIONS}: Using flexible Bayesian regression models with random effects allowed us to estimate absolute risks without strong modeling assumptions. Risk comparisons for any sub-groups at any exposure level were simple to calculate. Shrinkage of posterior estimates through the use of random effects allowed for finer categorization of exposures without restricting joint effects to follow a fixed parametric scale. Although foreign born Hispanic women with the least education appeared to generally have low risk, this seems likely to be a marker for unmeasured environmental and behavioral factors, rather than a causally protective effect of low education itself."},
	pages = {51},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Kaufman, Jay and {MacLehose}, Richard and Torrone, Elizabeth and Savitz, David},
	urldate = {2011-04-26},
	date = {2011}
}

@article{strand_methodological_2011,
	title = {Methodological challenges when estimating the effects of season and seasonal exposures on birth outcomes},
	volume = {11},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/11/49},
	doi = {10.1186/1471-2288-11-49},
	abstract = {Abstract: "{BACKGROUND}:Many previous studies have found seasonal patterns in birth outcomes, but with little agreement about which season poses the highest risk. Some of the heterogeneity between studies may be explained by a previously unknown bias. The bias occurs in retrospective cohorts which include all births occurring within a fixed start and end date, which means shorter pregnancies are missed at the start of the study, and longer pregnancies are missed at the end. Our objective was to show the potential size of this bias and how to avoid it.{METHODS}:To demonstrate the bias we simulated a retrospective birth cohort with no seasonal pattern in gestation and used a range of cohort end dates. As a real example, we used a cohort of 114,063 singleton births in Brisbane between 1 July 2005 and 30 June 2009 and examined the bias when estimating changes in gestation length associated with season (using month of conception) and a seasonal exposure (temperature). We used survival analyses with temperature as a time-dependent variable. {RESULTS}:We found strong artificial seasonal patterns in gestation length by month of conception, which depended on the end date of the study. The bias was avoided when the day and month of the start date was just before the day and month of the end date (regardless of year), so that the longer gestations at the start of the study were balanced by the shorter gestations at the end. After removing the fixed cohort bias there was a noticeable change in the effect of temperature on gestation length. The adjusted hazard ratios were flatter at the extremes of temperature but steeper between 15 and 25 degrees Celsius. {CONCLUSIONS}:Studies using retrospective birth cohorts should account for the fixed cohort bias by removing selected births to get unbiased estimates of seasonal health effects."},
	pages = {49},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Strand, Linn and Barnett, Adrian and Tong, Shilu},
	urldate = {2011-04-26},
	date = {2011}
}

@article{moher_resources_2011,
	title = {Resources for authors of reports of randomized trials: harnessing the wisdom of authors, editors, and readers.},
	volume = {12},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/12/1/98},
	doi = {10.1186/1745-6215-12-98},
	shorttitle = {Resources for authors of reports of randomized trials},
	abstract = {Abstract: "The {CONSORT} Statement was developed to help authors improve the quality of reporting randomized trials. To augment the statement we published the {CONSORT} explanation and elaboration paper which included at least one example of good reporting for each {CONSORT} checklist item. We are developing a comprehensive database of examples of good reporting for each checklist item to take advantage of the breadth and variety of trials familiar to authors and readers globally. We invite authors, editors, and readers worldwide to nominate examples of well reported items for the database."},
	pages = {98},
	number = {1},
	journaltitle = {Trials},
	author = {Moher, David and Hopewell, Sally and Schulz, Kenneth and Altman, Douglas},
	urldate = {2011-04-26},
	date = {2011}
}

@article{weijer_ethical_2011,
	title = {Ethical issues posed by cluster randomized trials in health research},
	volume = {12},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/12/1/100},
	doi = {10.1186/1745-6215-12-100},
	abstract = {Abstract: "The cluster randomized trial ({CRT}) is used increasingly in knowledge translation research, quality improvement research, community based intervention studies, public health research, and research in developing countries. However, cluster trials raise difficult ethical issues that challenge researchers, research ethics committees, regulators, and sponsors as they seek to fulfill responsibly their respective roles. Our project will provide a systematic analysis of the ethics of cluster trials. Here we have outlined a series of six areas of inquiry that must be addressed if the cluster trial is to be set on a firm ethical foundation: 1. Who is a research subject? 2. From whom, how, and when must informed consent be obtained? 3. Does clinical equipoise apply to {CRTs}? 4. How do we determine if the benefits outweigh the risks of {CRTs}? 5. How ought vulnerable groups be protected in {CRTs}? 6. Who are gatekeepers and what are their responsibilities? Subsequent papers in this series will address each of these areas, clarifying the ethical issues at stake and, where possible, arguing for a preferred solution. Our hope is that these papers will serve as the basis for the creation of international ethical guidelines for the design and conduct of cluster randomized trials."},
	pages = {100},
	number = {1},
	journaltitle = {Trials},
	author = {Weijer, Charles and Grimshaw, Jeremy and Taljaard, Monica and Binik, Ariella and Boruch, Robert and Brehaut, Jamie and Donner, Allan and Eccles, Martin and Gallo, Antonio and {McRae}, Andrew and Saginur, Raphael and Zwarenstein, Merrick},
	urldate = {2011-04-26},
	date = {2011}
}

@online{munroe_xkcd:_nodate-1,
	title = {xkcd: Significant},
	url = {http://xkcd.com/882/},
	abstract = {Description: This is a cartoon using jelly beans to illustrate the problem wiht multiple comparisons. If you hover your cursor over thie image, you get the following postscript "So, uh, we did the green study again and got no link. It was probably a--''{RESEARCH} {CONFLICTED} {ON} {GREEN} {JELLY} {BEAN} {ACNE} {LINK}; {MORE} {STUDY} {RECOMMENDED}!"},
	author = {Munroe, Randall},
	urldate = {2011-04-25}
}

@online{suber_field_nodate,
	title = {A field guide to misunderstandings about open access ({SPARC})},
	url = {http://www.arl.org/sparc/publications/articles/openaccess_fieldguide.shtml},
	abstract = {Excerpt: "The woods are full of misunderstandings about {OA}.  They thrive in almost every habitat, and the population soars whenever a major institution adopts an {OA} policy.  Contact between new developments and new observers who haven't followed the annual migrations always results in a colorful boomlet of young misunderstandings. Some of these misunderstandings are mistaken for one another, especially in the flurry of activity, because of their similar markings and habitat.  Some are mistaken for understanding by novices unfamiliar with the medley of variant plumage, adaptive camouflage, and deceptive vocalizations.  This field guide should help you identify 25 of the most common visitors to your neck of the woods. Leave your binoculars at home.  All of these can be seen with the naked eye.  With no more than this guide, and some patient observation, every trip to a conference, and even an occasional faculty meeting, can be an enjoyable and educational outing. "},
	author = {Suber, Peter},
	urldate = {2011-04-21}
}

@video{sphweb_cure_2008,
	title = {A Cure for the Biostatistics Blues: The {PBS} Pledge},
	url = {http://www.youtube.com/watch?v=I4obtsBUxUs&feature=youtube_gdata_player},
	shorttitle = {A Cure for the Biostatistics Blues},
	abstract = {In this 6-minute skit, Professor Rod Little provides reassurance to an online student enrolled in his course through his pledge to make statistics exciting and relevant for public health students.

Introduction to Biostatistics 503 Online, Online Certificate in the Foundations of Public Health program, University of Michigan School of Public Health http://www.sph.umich.ed/distance/},
	editora = {{sphweb}},
	editoratype = {collaborator},
	urldate = {2011-04-20},
	date = {2008-07-30}
}

@video{lindsayrenfro_isba_2008,
	title = {{ISBA} 2008 Bayesian Cabaret "Statistician"},
	url = {http://www.youtube.com/watch?v=4B55aXyFN8s&feature=youtube_gdata_player},
	abstract = {Description: "Brad Carlin, Jeff Rosenthal, Mark Glickman, Javier Cano, Kirsty Henry, and .... perform "Statistician" (Pretty Woman; Roy Orbison) at the {ISBA} 2008 Bayesian Cabaret in Hamilton Island, Australia."},
	editora = {{LindsayRenfro}},
	editoratype = {collaborator},
	urldate = {2011-04-20},
	date = {2008-07-28}
}

@online{pcori_patient-centered_nodate,
	title = {Patient-Centered Outcomes Research Institute Home},
	url = {http://www.pcori.org/},
	abstract = {Excerpt: "The Patient-Centered Outcomes Research Institute ({PCORI}) is an independent organization created to help patients, clinicians, purchasers and policy makers make better informed health decisions. {PCORI} will commission research that is responsive to the values and interests of patients and will provide patients and their caregivers with reliable, evidence-based information for the health care choices they face. {PCORI} is committed to transparency and a rigorous stakeholder-driven process that emphasizes patient engagement. {PCORI} will use a series of forums and formal public comment periods to increase awareness of its work and obtain public input and feedback prior to adoption of priorities, agendas, methodological standards, peer review processes or dissemination strategies."},
	author = {{PCORI}},
	urldate = {2011-04-13}
}

@online{basu_my_nodate,
	title = {My post on scientific fraud in writing research paper in a discussion group - Arin's Stream},
	url = {http://arinbasu.posterous.com/netrum-scientific-fraud-in-writing-2},
	abstract = {Excerpt: "Vijay sir has recently posted a very interesting list of scientific fraud in writing in this forum. I'd like to pick up three of them -- selective reporting as scientific fraud, salami slicing of reports, and omission of others' original publications - and post my observations and feelings. I think each of the three is an example of academic dishonesty but there is a deeper societal and cultural systemic basis in each one. I also think using "fraud" to label them as such is too strong a term to use. The reason I think fraud is too strong a term for this set, is in the complexity around each of these, and how people are sometimes compelled under circumstances."},
	author = {Basu, Arin},
	urldate = {2011-04-13}
}

@article{young_when_2009,
	title = {When computers leave the classroom, so does boredom - {SMU}},
	url = {http://www.smu.edu/News/2009/jose-bowen-che-24july2009.aspx},
	abstract = {Excerpt: "College leaders usually brag about their tech-filled "smart" classrooms, but a dean at Southern Methodist University is proudly removing computers from lecture halls. José A. Bowen, dean of the Meadows School of the Arts, has challenged his colleagues to "teach naked"--by which he means, sans machines."},
	journaltitle = {The Chronicle of Higher Education},
	author = {Young, Jeffrey R.},
	urldate = {2011-04-13},
	date = {2009-07-20}
}

@book{steyerberg_clinical_2010,
	edition = {1st Edition.},
	title = {Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating},
	isbn = {1441926488},
	url = {http://www.springer.com/statistics/life+sciences,+medicine+%26+health/book/978-0-387-77243-1},
	shorttitle = {Clinical Prediction Models},
	publisher = {Springer},
	author = {Steyerberg, Ewout W.},
	date = {2010-12-01}
}

@article{tibshirani_model_1999,
	title = {Model Search by Bootstrap "Bumping"},
	volume = {8},
	issn = {1061-8600},
	url = {http://www.jstor.org/stable/1390820},
	doi = {10.2307/1390820},
	abstract = {Abstract: "We propose a bootstrap-based method for enhancing a search through a space of models. The technique is well suited to complex, adaptively fitted models--it provides a convenient method for finding better local minima and for resistant fitting. Applications to regression, classification, and density estimation are described. We also provide results on the asymptotic behavior of bumping estimates."},
	pages = {671--686},
	number = {4},
	journaltitle = {Journal of Computational and Graphical Statistics},
	author = {Tibshirani, Robert and Knight, Keith},
	urldate = {2011-04-07},
	date = {1999-12-01},
	note = {{ArticleType}: research-article / Full publication date: Dec., 1999 / Copyright © 1999 American Statistical Association, Institute of Mathematical Statistics and Interface Foundation of America}
}

@online{allaire_introducing_nodate,
	title = {Introducing {RStudio}},
	url = {http://www.rstudio.org/},
	abstract = {Excerpt: "{RStudio}™ is a new integrated development environment ({IDE}) for R. {RStudio} combines an intuitive user interface with powerful coding tools to help you get the most out of R."},
	author = {Allaire, {JJ} and Cheng, Joe and Paulson, Josh and {DiCristina}, Paul},
	urldate = {2011-04-07}
}

@article{greenland_when_2000,
	title = {When Should Epidemiologic Regressions Use Random Coefficients?},
	volume = {56},
	issn = {1541-0420},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.0006-341X.2000.00915.x/abstract},
	doi = {10.1111/j.0006-341X.2000.00915.x},
	abstract = {Abstract: "Summary.  Regression models with random coefficients arise naturally in both frequentist and Bayesian approaches to estimation problems. They are becoming widely available in standard computer packages under the headings of generalized linear mixed models, hierarchical models, and multilevel models. I here argue that such models offer a more scientifically defensible framework for epidemiologic analysis than the fixed-effects models now prevalent in epidemiology. The argument invokes an antiparsimony principle attributed to L. J. Savage, which is that models should be rich enough to reflect the complexity of the relations under study. It also invokes the countervailing principle that you cannot estimate anything if you try to estimate everything (often used to justify parsimony). Regression with random coefficients offers a rational compromise between these principles as well as an alternative to analyses based on standard variable-selection algorithms and their attendant distortion of uncertainty assessments. These points are illustrated with an analysis of data on diet, nutrition, and breast cancer."},
	pages = {915--921},
	number = {3},
	journaltitle = {Biometrics},
	author = {Greenland, Sander},
	urldate = {2011-04-07},
	date = {2000-09-01},
	langid = {english},
	keywords = {Bayesian statistics, Causal inference, Empirical Bayes estimators, Epidemiologic Methods, Hierarchical regression, Mixed models, Multilevel modeling, Random‐coefficient regression, Relative risk, Risk Assessment, Shrinkage, Variance components}
}

@article{grambsch_effects_1991,
	title = {The effects of transformations and preliminary tests for non-linearity in regression},
	volume = {10},
	issn = {0277-6715},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2068422},
	abstract = {Non-linear relationships between two variables are often detected as a result of a preliminary statistical test for linearity. Common approaches to dealing with non-linearity are to (a) make a linearizing transformation in the independent variable or (b) fit a relationship that is non-linear in the independent variable, such as including a quadratic term. With either approach, the resulting test for association between the two variables can have an inflated type I error. We consider testing the significance of the quadratic term in a quadratic model as a preliminary test for non-linearity. Using simulation experiments and asymptotic arguments, we quantify the type I error inflation and suggest simple modifications of standard practice to protect the size of the type I error. In the case of quadratic regression, the type I error will be increased by roughly 50 per cent. The simple strategy of appropriately correcting the alpha-level is shown to have minimal loss of power if the relationship is truly linear. In the case of a linearizing transformation, the impact on the type I error will depend on the values of the independent variable and on the set of potential linearizing transformations considered. Simulation results suggest that a procedure which adjusts the test statistic according to the results of the preliminary test may offer adequate protection.},
	pages = {697--709},
	number = {5},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Stat Med},
	author = {Grambsch, P M and O'Brien, P C},
	urldate = {2011-04-07},
	date = {1991-05},
	pmid = {2068422},
	keywords = {Aging, Humans, Male, Middle Aged, Models, Statistical, Regression Analysis, Sensation, Vibration}
}

@article{royston_dichotomizing_2006,
	title = {Dichotomizing continuous predictors in multiple regression: a bad idea},
	volume = {25},
	issn = {0277-6715},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/16217841},
	doi = {10.1002/sim.2331},
	shorttitle = {Dichotomizing continuous predictors in multiple regression},
	abstract = {Abstract: "In medical research, continuous variables are often converted into categorical variables by grouping values into two or more categories. We consider in detail issues pertaining to creating just two groups, a common approach in clinical research. We argue that the simplicity achieved is gained at a cost; dichotomization may create rather than avoid problems, notably a considerable loss of power and residual confounding. In addition, the use of a data-derived 'optimal' cutpoint leads to serious bias. We illustrate the impact of dichotomization of continuous predictor variables using as a detailed case study a randomized trial in primary biliary cirrhosis. Dichotomization of continuous data is unnecessary for statistical analysis and in particular should not be applied to explanatory variables in regression models."},
	pages = {127--141},
	number = {1},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Stat Med},
	author = {Royston, Patrick and Altman, Douglas G and Sauerbrei, Willi},
	urldate = {2011-04-07},
	date = {2006-01-15},
	pmid = {16217841},
	keywords = {Age Factors, Albumins, Antimetabolites, Azathioprine, Bilirubin, Cholestasis, Data Interpretation, Statistical, Humans, Liver Cirrhosis, Biliary, Randomized Controlled Trials as Topic, Regression Analysis}
}

@article{ye_measuring_1998,
	title = {On Measuring and Correcting the Effects of Data Mining and Model Selection},
	volume = {93},
	issn = {0162-1459},
	url = {http://www.jstor.org/stable/2669609},
	doi = {10.2307/2669609},
	abstract = {Abstract: "In the theory of linear models, the concept of degrees of freedom plays an important role. This concept is often used for measurement of model complexity, for obtaining an unbiased estimate of the error variance, and for comparison of different models. I have developed a concept of generalized degrees of freedom ({GDF}) that is applicable to complex modeling procedures. The definition is based on the sum of the sensitivity of each fitted value to perturbation in the corresponding observed value. The concept is nonasymptotic in nature and does not require analytic knowledge of the modeling procedures. The concept of {GDF} offers a unified framework under which complex and highly irregular modeling procedures can be analyzed in the same way as classical linear models. By using this framework, many difficult problems can be solved easily. For example, one can now measure the number of observations used in a variable selection process. Different modeling procedures, such as a tree-based regression and a projection pursuit regression, can be compared on the basis of their residual sums of squares and the {GDF} that they cost. I apply the proposed framework to measure the effect of variable selection in linear models, leading to corrections of selection bias in various goodness-of-fit statistics. The theory also has interesting implications for the effect of general model searching by a human modeler."},
	pages = {120--131},
	number = {441},
	journaltitle = {Journal of the American Statistical Association},
	author = {Ye, Jianming},
	urldate = {2011-04-07},
	date = {1998-03-01},
	note = {{ArticleType}: research-article / Full publication date: Mar., 1998 / Copyright © 1998 American Statistical Association}
}

@article{vickers_against_2008,
	title = {Against Diagnosis},
	volume = {149},
	url = {http://www.annals.org/content/149/3/200.abstract},
	abstract = {Abstract: "The act of diagnosis requires that patients be placed in a binary category of either having or not having a certain disease. Accordingly, the diseases of particular concern for industrialized countries—such as type 2 diabetes, obesity, or depression—require that a somewhat arbitrary cut-point be chosen on a continuous scale of measurement (for example, a fasting glucose level {\textgreater}6.9 mmol/L [{\textgreater}125 mg/{dL}] for type 2 diabetes). These cut-points do not adequately reflect disease biology, may inappropriately treat patients on either side of the cut-point as 2 homogenous risk groups, fail to incorporate other risk factors, and are invariable to patient preference. This article discusses risk prediction as an alternative to diagnosis: Patient risk factors (blood pressure, age) are combined into a single statistical model (risk for a cardiovascular event within 10 years) and the results are used in shared decision making about possible treatments. The authors compare and contrast the diagnostic and risk prediction approaches and attempt to identify the types of medical problem to which each is best suited."},
	pages = {200 --203},
	number = {3},
	journaltitle = {Annals of Internal Medicine},
	author = {Vickers, Andrew J. and Basch, Ethan and Kattan, Michael W.},
	urldate = {2011-04-07},
	date = {2008}
}

@article{sun_is_2010,
	title = {Is a subgroup effect believable? Updating criteria to evaluate the credibility of subgroup analyses},
	volume = {340},
	issn = {0959-8138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.c117},
	doi = {10.1136/bmj.c117},
	shorttitle = {Is a subgroup effect believable?},
	abstract = {Excerpt: "Subgroup analyses in randomised controlled trials ({RCTs}) or in meta-analyses of {RCTs} examine whether treatment effects vary according to patient group, way of giving an intervention, or approach to measuring an outcome. Subgroup analyses are common and often associated with claims of difference of treatment effects between subgroups—termed “subgroup effect”, “effect modification”, or “interaction between a subgroup variable and treatment”. A difference in effect between subgroups, if true, is likely to have important implications for clinical practice and policy making. Many subgroup claims are, however, subsequently shown to be false. Thus, investigators, clinicians, and policy makers face the challenge of whether or not to believe apparent differences in effect. "},
	pages = {c117--c117},
	issue = {mar30 3},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sun, X. and Briel, M. and Walter, S. D. and Guyatt, G. H.},
	urldate = {2011-04-04},
	date = {2010-03}
}

@article{lexchin_pharmaceutical_2003,
	title = {Pharmaceutical industry sponsorship and research outcome and quality: systematic review},
	volume = {326},
	issn = {09598138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.326.7400.1167},
	doi = {10.1136/bmj.326.7400.1167},
	shorttitle = {Pharmaceutical industry sponsorship and research outcome and quality},
	pages = {1167--1170},
	number = {7400},
	journaltitle = {{BMJ}},
	author = {Lexchin, J.},
	urldate = {2011-04-04},
	date = {2003-05}
}

@article{jorgensen_cochrane_2006,
	title = {Cochrane reviews compared with industry supported meta-analyses and other meta-analyses of the same drugs: systematic review},
	volume = {333},
	issn = {0959-8138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.38973.444699.0B},
	doi = {10.1136/bmj.38973.444699.0B},
	shorttitle = {Cochrane reviews compared with industry supported meta-analyses and other meta-analyses of the same drugs},
	pages = {782--0},
	number = {7572},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Jorgensen, A. W and Hilden, J. and Gotzsche, P. C},
	urldate = {2011-04-04},
	date = {2006-10}
}

@article{chan_discrepancies_2008,
	title = {Discrepancies in sample size calculations and data analyses reported in randomised trials: comparison of publications with protocols},
	volume = {337},
	issn = {0959-8138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.a2299},
	doi = {10.1136/bmj.a2299},
	shorttitle = {Discrepancies in sample size calculations and data analyses reported in randomised trials},
	pages = {a2299--a2299},
	issue = {dec04 1},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Chan, A.-W. and Hrobjartsson, A. and Jorgensen, K. J and Gotzsche, P. C and Altman, D. G},
	urldate = {2011-04-04},
	date = {2008-12}
}

@article{godlee_negative_2011,
	title = {Negative findings},
	volume = {342},
	issn = {0959-8138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.d2053},
	doi = {10.1136/bmj.d2053},
	pages = {d2053--d2053},
	issue = {mar30 3},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Godlee, F.},
	urldate = {2011-04-04},
	date = {2011-03}
}

@article{sun_influence_2011,
	title = {The influence of study characteristics on reporting of subgroup analyses in randomised controlled trials: systematic review},
	volume = {342},
	issn = {0959-8138},
	url = {http://www.bmj.com/cgi/doi/10.1136/bmj.d1569},
	doi = {10.1136/bmj.d1569},
	shorttitle = {The influence of study characteristics on reporting of subgroup analyses in randomised controlled trials},
	pages = {d1569--d1569},
	issue = {mar28 1},
	journaltitle = {{BMJ}},
	shortjournal = {{BMJ}},
	author = {Sun, X. and Briel, M. and Busse, J. W. and You, J. J. and Akl, E. A. and Mejza, F. and Bala, M. M. and Bassler, D. and Mertz, D. and Diaz-Granados, N. and Vandvik, P. O. and Malaga, G. and Srinathan, S. K. and Dahm, P. and Johnston, B. C. and Alonso-Coello, P. and Hassouneh, B. and Truong, J. and Dattani, N. D. and Walter, S. D. and Heels-Ansdell, D. and Bhatnagar, N. and Altman, D. G. and Guyatt, G. H.},
	urldate = {2011-04-04},
	date = {2011-03}
}

@online{dallal_there_nodate,
	title = {There must be something buried in here somewhere!},
	url = {http://www.jerrydallal.com/LHSP/multtest.htm},
	abstract = {Description: This webpage uses a simulation to illustrate what happens with one hundred simultaneous independent tests of significance.},
	author = {Dallal, Jerry},
	urldate = {2011-02-23}
}

@article{sankoh_comments_1997,
	title = {Some comments on frequently used multiple endpoint adjustment methods in clinical trials},
	volume = {16},
	issn = {0277-6715},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9403954},
	abstract = {Abstract: "Confirmatory clinical trials often classify clinical response variables into primary and secondary endpoints. The presence of two or more primary endpoints in a clinical trial usually means that some adjustments of the observed p-values for multiplicity of tests may be required for the control of the type I error rate. In this paper, we discuss statistical concerns associated with some commonly used multiple endpoint adjustment procedures. We also present limited Monte Carlo simulation results to demonstrate the performance of selected p-value-based methods in protecting the type I error rate."},
	pages = {2529--2542},
	number = {22},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Stat Med},
	author = {Sankoh, A J and Huque, M F and Dubey, S D},
	urldate = {2011-02-23},
	date = {1997-11-30},
	pmid = {9403954},
	keywords = {Algorithms, Clinical Trials as Topic, Computer Simulation, Data Interpretation, Statistical, Humans, Monte Carlo Method, Statistics as Topic}
}

@book{miller_simultaneous_1981,
	edition = {2nd},
	title = {Simultaneous Statistical Inference},
	isbn = {0387905480},
	abstract = {Description: Rupert Miller wrote the classic text on multiple comparisons adjustments back in 1981. The book does not include some recent methods, but does provide a good overview. Dr. Miller also outlines several different philosophical perspectives on multiple comparison adjustments. This book is a gentle introduction to a specialized topic.},
	publisher = {Springer},
	author = {Miller, Rupert G. Jr.},
	date = {1981-03-18}
}

@article{ottenbacher_quantitative_1998,
	title = {Quantitative Evaluation of Multiplicity in Epidemiology and Public Health Research},
	volume = {147},
	url = {http://aje.oxfordjournals.org/content/147/7/615.abstract},
	abstract = {Abstract: "Epidemiologic and public health researchers frequently include several dependent variables, repeated assessments, or subgroup analyses in their investigations. These factors result in multiple tests of statistical significance and may produce type 1 experimental errors. This study examined the type 1 error rate in a sample of public health and epidemiologic research. A total of 173 articles chosen at random from 1996 issues of the American Journal of Public Health and the American Journal of Epidemiology were examined to determine the incidence of type 1 errors. Three different methods of computing type 1 error rates were used: experiment-wise error rate, error rate per experiment, and percent error rate. The results indicate a type 1 error rate substantially higher than the traditionally assumed level of 5\% (p {\textless} 0.05). No practical or statistically significant difference was found between type 1 error rates across the two journals. Methods to determine and correct type 1 errors should be reported in epidemiologic and public health research investigations that include multiple statistical tests."},
	pages = {615 --619},
	number = {7},
	journaltitle = {American Journal of Epidemiology},
	author = {Ottenbacher, Kenneth J.},
	urldate = {2011-02-23},
	date = {1998-04-01}
}

@article{buettner_problems_1997,
	title = {Problems in defining cutoff points of continuous prognostic factors: example of tumor thickness in primary cutaneous melanoma},
	volume = {50},
	issn = {0895-4356},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9393376},
	shorttitle = {Problems in defining cutoff points of continuous prognostic factors},
	abstract = {Abstract: "Continuous prognostic factors are often categorized by defining optimized cutoff points. One component of criticism of this approach is the problem of multiple testing that leads to an overestimation of the true prognostic impact of the variable. The present study focuses on another crucial point by investigating the dependence of optimized cutoff points on the observed distribution of the continuous variable. The continuous variable investigated was the vertical tumor thickness according to Breslow, which is known to be the most important prognostic factor in primary melanoma. Based on the data of 5093 patients, stratified random samples were drawn out of six artificially created distributions of tumor thickness. For each of these samples, Cox models were calculated to explore optimized cutoff points for tumor thickness together with other prognostic variables. The optimized cutoff points for tumour thickness varied considerably with the underlying distribution. Even in samples from the same distribution, the range of cutoff points was amazingly broad and, for some of the distributions, covered the whole region of possible values. The results of the present study demonstrate that optimized cutoff points are extremely data dependent and vary notably even if prerequisites are constant. Therefore, if the classification of a continuous prognostic factor is necessary, it should not be based on the results of one single study, but on consensus discussions including the findings of several investigations."},
	pages = {1201--1210},
	number = {11},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Buettner, P and Garbe, C and Guggenmoos-Holzmann, I},
	urldate = {2011-02-23},
	date = {1997-11},
	pmid = {9393376},
	keywords = {Female, Humans, Male, Melanoma, Middle Aged, Prognosis, Proportional Hazards Models, Skin Neoplasms}
}

@article{kim_permutation_2000,
	title = {Permutation tests for joinpoint regression with applications to cancer rates},
	volume = {19},
	issn = {0277-6715},
	url = {http://shrpssb.umdnj.edu/shrpnwk/COURSE/binf7540/shankar/joinpoint.pdf},
	abstract = {Abstract: "The identification of changes in the recent trend is an important issue in the analysis of cancer mortality and incidence data. We apply a joinpoint regression model to describe such continuous changes and use the grid-search method to fit the regression function with unknown joinpoints assuming constant variance and uncorrelated errors. We find the number of significant joinpoints by performing several permutation tests, each of which has a correct significance level asymptotically. Each p-value is found using Monte Carlo methods, and the overall asymptotic significance level is maintained through a Bonferroni correction. These tests are extended to the situation with non-constant variance to handle rates with Poisson variation and possibly autocorrelated errors. The performance of these tests are studied via simulations and the tests are applied to U.S. prostate cancer incidence and mortality rates."},
	pages = {335--351},
	number = {3},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Stat Med},
	author = {Kim, H J and Fay, M P and Feuer, E J and Midthune, D N},
	urldate = {2011-02-23},
	date = {2000-02-15},
	pmid = {10649300},
	keywords = {Algorithms, Humans, Incidence, Male, Monte Carlo Method, Poisson Distribution, Prostatic Neoplasms, Regression Analysis, United States}
}

@article{rothman_no_1990,
	title = {No adjustments are needed for multiple comparisons},
	volume = {1},
	issn = {1044-3983},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/2081237},
	abstract = {Abstract: "Adjustments for making multiple comparisons in large bodies of data are recommended to avoid rejecting the null hypothesis too readily. Unfortunately, reducing the type I error for null associations increases the type {II} error for those associations that are not null. The theoretical basis for advocating a routine adjustment for multiple comparisons is the "universal null hypothesis" that "chance" serves as the first-order explanation for observed phenomena. This hypothesis undermines the basic premises of empirical research, which holds that nature follows regular laws that may be studied through observations. A policy of not making adjustments for multiple comparisons is preferable because it will lead to fewer errors of interpretation when the data under evaluation are not random numbers but actual observations on nature. Furthermore, scientists should not be so reluctant to explore leads that may turn out to be wrong that they penalize themselves by missing possibly important findings."},
	pages = {43--46},
	number = {1},
	journaltitle = {Epidemiology (Cambridge, Mass.)},
	shortjournal = {Epidemiology},
	author = {Rothman, K J},
	urldate = {2011-02-23},
	date = {1990-01},
	pmid = {2081237},
	keywords = {Bias (Epidemiology), Data Interpretation, Statistical, Humans, Multivariate Analysis, Probability}
}

@book{hsu_multiple_1996,
	edition = {1},
	title = {Multiple Comparisons: Theory and Methods},
	isbn = {0412982811},
	shorttitle = {Multiple Comparisons},
	abstract = {Excerpt: "Multiple Comparisons covers all-pairwise comparisons, multiple comparisons with the best, and multiple comparisons with a control. Confidence intervals methods and stepwise methods are described. Abuses and misconceptions are exposed, and the reader is guided to the correct method for each problem. Connections with bioequivalence, drug stability, and toxicity studies are discussed. Applications are illustrated with real data, analyzed by computer packages. Extension to the General Linear Model is provided. "},
	publisher = {Chapman and Hall/{CRC}},
	author = {Hsu, Jason},
	date = {1996-02-01}
}

@article{thompson_invited_1998,
	title = {Invited commentary: Re: "Multiple comparisons and related issues in the interpretation of epidemiologic data"},
	volume = {147},
	issn = {0002-9262},
	url = {http://aje.oxfordjournals.org/content/147/9/801.long},
	shorttitle = {Invited commentary},
	pages = {801--806},
	number = {9},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {Am. J. Epidemiol},
	author = {Thompson, J R},
	urldate = {2011-02-23},
	date = {1998-05-01},
	pmid = {9583708},
	keywords = {Data Interpretation, Statistical, Epidemiology, Humans, Models, Statistical, Reproducibility of Results, Research Design}
}

@article{savitz_multiple_1995,
	title = {Multiple comparisons and related issues in the interpretation of epidemiologic data},
	volume = {142},
	issn = {0002-9262},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/7572970},
	pages = {904--908},
	number = {9},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {Am. J. Epidemiol},
	author = {Savitz, D A and Olshan, A F},
	urldate = {2011-02-23},
	date = {1995-11-01},
	pmid = {7572970},
	keywords = {Data Interpretation, Statistical, Epidemiology, Humans, Research Design}
}

@article{brown_methods_1997,
	title = {Methods of correcting for multiple testing: operating characteristics},
	volume = {16},
	issn = {0277-6715},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9403953},
	shorttitle = {Methods of correcting for multiple testing},
	abstract = {Abstract: "We examine the operating characteristics of 17 methods for correcting p-values for multiple testing on synthetic data with known statistical properties. These methods are derived p-values only and not the raw data. With the test cases, we systematically varied the number of p-values, the proportion of false null hypotheses, the probability that a false null hypothesis would result in a p-value less than 5 per cent and the degree of correlation between p-values. We examined the effect of each of these factors on family-wise and false negative error rates and compared the false negative error rates of methods with an acceptable family-wise error. Only four methods were not bettered in this comparison. Unfortunately, however, a uniformly best method of those examined does not exist. A suggested strategy for examining corrections uses a succession of methods that are increasingly lax in family-wise error. A computer program for these corrections is available."},
	pages = {2511--2528},
	number = {22},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Stat Med},
	author = {Brown, B W and Russell, K},
	urldate = {2011-02-23},
	date = {1997-11-30},
	pmid = {9403953},
	keywords = {Clinical Trials as Topic, Confidence Intervals, Data Interpretation, Statistical, Humans, Multivariate Analysis, Regression Analysis, Statistics as Topic, Statistics, Nonparametric}
}

@article{chamberlin_method_1931,
	title = {The Method of Multiple Working Hypotheses},
	volume = {39},
	issn = {00221376},
	url = {http://www.jstor.org/stable/30060433},
	abstract = {Excerpt: "This essay on methods of scientific thought appeared in the Journal of Geology, Volume V (1897), pages 837-48 under the heading, "Studies for Students." Since then it has come to be regarded by many as a classic of its kind and its influence has been far-reaching. Requests for copies of it are still frequently received, although the available supply was exhausted many years ago. In response to the continued demand and in the belief that the present generation should be familiar with it, this study for students is reprinted in its original form."},
	pages = {155--165},
	number = {2},
	journaltitle = {The Journal of Geology},
	author = {Chamberlin, T. C.},
	urldate = {2011-02-23},
	date = {1931-02-01},
	note = {{ArticleType}: research-article / Full publication date: Feb. - Mar., 1931 / Copyright © 1931 The University of Chicago Press}
}

@article{swaen_false_2001,
	title = {False positive outcomes and design characteristics in occupational cancer epidemiology studies},
	volume = {30},
	url = {http://ije.oxfordjournals.org/content/30/5/948.abstract},
	doi = {10.1093/ije/30.5.948},
	abstract = {Abstract: "Background: Recently there has been considerable debate about possible false positive study outcomes. Several well-known epidemiologists have expressed their concern and the possibility that epidemiological research may loose credibility with policy makers as well as the general public. Methods: We have identified 75 false positive studies and 150 true positive studies, all published reports and all epidemiological studies reporting results on substances or work processes generally recognized as being carcinogenic to humans. All studies were scored on a number of design characteristics and factors relating to the specificity of the research objective. These factors included type of study design, use of cancer registry data, adjustment for smoking and other factors, availability of exposure data, dose- and duration-effect relationship, magnitude of the reported relative risk, whether the study was considered a ‘fishing expedition', affiliation and country of the first author. Results: The strongest factor associated with the false positive or true positive study outcome was if the study had a specific a priori hypothesis. Fishing expeditions had an over threefold odds ratio of being false positive. Factors that decreased the odds ratio of a false positive outcome included observing a dose-effect relationship, adjusting for smoking and not using cancer registry data. Conclusion: The results of the analysis reported here clearly indicate that a study with a specific a priori study objective should be valued more highly in establishing a causal link between exposure and effect than a mere fishing expedition."},
	pages = {948 --954},
	number = {5},
	journaltitle = {International Journal of Epidemiology},
	author = {Swaen, Gerard {GMH} and Teggeler, Olga and van Amelsvoort, Ludovic {GPM}},
	urldate = {2011-02-23},
	date = {2001-10-01}
}

@article{greenland_empirical-bayes_1991,
	title = {Empirical-Bayes adjustments for multiple comparisons are sometimes useful},
	volume = {2},
	issn = {1044-3983},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/1912039},
	abstract = {Abstract: "Rothman (Epidemiology 1990;1:43-46) recommends against adjustments for multiple comparisons. Implicit in his recommendation, however, is an assumption that the sole objective of the data analysis is to report and scientifically interpret the data. We concur with his recommendation when this assumption is correct and one is willing to abandon frequentist interpretations of the summary statistics. Nevertheless, there are situations in which an additional or even primary goal of analysis is to reach a set of decisions based on the data. In such situations, Bayes and empirical-Bayes adjustments can provide a better basis for the decisions than conventional procedures."},
	pages = {244--251},
	number = {4},
	journaltitle = {Epidemiology (Cambridge, Mass.)},
	shortjournal = {Epidemiology},
	author = {Greenland, S and Robins, J M},
	urldate = {2011-02-23},
	date = {1991-07},
	pmid = {1912039},
	keywords = {Bayes Theorem, El Salvador, Epidemiologic Methods, Humans, Models, Statistical, Prevalence, Risk, Toxoplasmosis}
}

@article{feise_multiple_2002,
	title = {Do multiple outcome measures require p-value adjustment?},
	volume = {2},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/2/8},
	doi = {10.1186/1471-2288-2-8},
	abstract = {Abstract: "{BACKGROUND}: Readers may question the interpretation of findings in clinical trials when multiple outcome measures are used without adjustment of the p-value. This question arises because of the increased risk of Type I errors (findings of false "significance") when multiple simultaneous hypotheses are tested at set p-values. The primary aim of this study was to estimate the need to make appropriate p-value adjustments in clinical trials to compensate for a possible increased risk in committing Type I errors when multiple outcome measures are used. {DISCUSSION}: The classicists believe that the chance of finding at least one test statistically significant due to chance and incorrectly declaring a difference increases as the number of comparisons increases. The rationalists have the following objections to that theory: 1) P-value adjustments are calculated based on how many tests are to be considered, and that number has been defined arbitrarily and variably; 2) P-value adjustments reduce the chance of making type I errors, but they increase the chance of making type {II} errors or needing to increase the sample size.{SUMMARY}:Readers should balance a study's statistical significance with the magnitude of effect, the quality of the study and with findings from other studies. Researchers facing multiple outcome measures might want to either select a primary outcome measure or use a global assessment measure, rather than adjusting the p-value.},
	pages = {8},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Feise, Ronald},
	urldate = {2011-02-23},
	date = {2002}
}

@article{stallard_decision_1999,
	title = {Decision theoretic designs for phase {II} clinical trials with multiple outcomes},
	volume = {55},
	issn = {0006-341X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11315037},
	abstract = {Abstract: "In many phase {II} clinical trials, it is essential to assess both efficacy and safety. Although several phase {II} designs that accommodate multiple outcomes have been proposed recently, none are derived using decision theory. This paper describes a Bayesian decision theoretic strategy for constructing phase {II} designs based on both efficacy and adverse events. The gain function includes utilities assigned to patient outcomes, a reward for declaring the new treatment promising, and costs associated with the conduct of the phase {II} trial and future phase {III} testing. A method for eliciting gain function parameters from medical collaborators and for evaluating the design's frequentist operating characteristics is described. The strategy is illustrated by application to a clinical trial of peripheral blood stem cell transplantation for multiple myeloma."},
	pages = {971--977},
	number = {3},
	journaltitle = {Biometrics},
	shortjournal = {Biometrics},
	author = {Stallard, N and Thall, P F and Whitehead, J},
	urldate = {2011-02-23},
	date = {1999-09},
	pmid = {11315037},
	keywords = {Bayes Theorem, Biometry, Clinical Trials, Phase {II} as Topic, Decision Theory, Hematopoietic Stem Cell Transplantation, Humans, Multiple Myeloma, Outcome Assessment (Health Care), Sample Size, Transplantation, Autologous}
}

@online{noauthor_carlo_nodate,
	title = {Carlo Emilio Bonferroni},
	url = {http://www.aghmed.fsnet.co.uk/bonf/bonf.html},
	abstract = {Excerpt: "I have been intrigued by Bonferroni for some time. His inequalities are widely known, but his life and works are not. I have recently submitted an article for publication in an attempt to rectify this situation, and have also published a short biography in the recent Encyclopaedia of Biostatistics published by Wiley. Links from this page give some information about him. I have also prepared a bibliography of his works which extends the published ones by including volume and page references for most of the articles. The bibliography is also available in {BibTex} format, which I think should be self explanatory if you use a different reference manager. "},
	urldate = {2011-02-23}
}

@article{howel_assessing_1994,
	title = {Assessing cause and effect from trials: a cautionary note},
	volume = {15},
	issn = {0197-2456},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/8001354},
	shorttitle = {Assessing cause and effect from trials},
	pages = {331--334},
	number = {5},
	journaltitle = {Controlled Clinical Trials},
	shortjournal = {Control Clin Trials},
	author = {Howel, D and Bhopal, R},
	urldate = {2011-02-23},
	date = {1994-10},
	pmid = {8001354},
	keywords = {Causality, Clinical Trials as Topic, Confounding Factors (Epidemiology), Data Interpretation, Statistical, Epidemiologic Methods, Humans, Randomized Controlled Trials as Topic, Treatment Outcome}
}

@article{dijkman_how_2009,
	title = {How to work with a subgroup analysis},
	volume = {52},
	issn = {1488-2310},
	url = {http://www.cma.ca/multimedia/staticContent/HTML/N0/l2/cjs/vol-52/issue-6/pdf/pg515.pdf},
	abstract = {Excerpt: "Surgical practice should principally be based on evidence originating from high-quality data such as randomized controlled trials ({RCTs}). Whereas these studies mostly investigate general and representative patient populations, clinical decisions most often depend on individual patient characteristics. To concede to the need of individually based guidelines, many {RCTs} report analyses on specific subgroups of patients.1,2 The main aim of a subgroup analysis is to identify either consistency of or large differences in the magnitude of treatment effect among different categories of patients. Determining whether the observed overall treatment effect is different across certain subgroups may justly provide some patients with its benefits and protect others from its harm."},
	pages = {515--522},
	number = {6},
	journaltitle = {Canadian Journal of Surgery. Journal Canadien De Chirurgie},
	shortjournal = {Can J Surg},
	author = {Dijkman, Bernadette and Kooistra, Bauke and Bhandari, Mohit},
	urldate = {2011-02-23},
	date = {2009-12},
	pmid = {20011190},
	keywords = {Humans, Professional Practice, Randomized Controlled Trials as Topic, Research Design}
}

@article{wang_statistics_2007,
	title = {Statistics in medicine--reporting of subgroup analyses in clinical trials},
	volume = {357},
	issn = {1533-4406},
	url = {http://www.nejm.org/doi/full/10.1056/NEJMsr077003},
	doi = {10.1056/NEJMsr077003},
	abstract = {Excerpt: "Medical research relies on clinical trials to assess therapeutic benefits. Because of the effort and cost involved in these studies, investigators frequently use analyses of subgroups of study participants to extract as much information as possible. Such analyses, which assess the heterogeneity of treatment effects in subgroups of patients, may provide useful information for the care of patients and for future research. However, subgroup analyses also introduce analytic challenges and can lead to overstated and misleading results.1-7 This report outlines the challenges associated with conducting and reporting subgroup analyses, and it sets forth guidelines for their use in the Journal. Although this report focuses on the reporting of clinical trials, many of the issues discussed also apply to observational studies."},
	pages = {2189--2194},
	number = {21},
	journaltitle = {The New England Journal of Medicine},
	shortjournal = {N. Engl. J. Med},
	author = {Wang, Rui and Lagakos, Stephen W and Ware, James H and Hunter, David J and Drazen, Jeffrey M},
	urldate = {2011-02-23},
	date = {2007-11-22},
	pmid = {18032770},
	keywords = {Clinical Trials as Topic, Data Interpretation, Statistical, Quality Control, Statistics as Topic}
}

@article{counsell_miracle_1994,
	title = {The miracle of {DICE} therapy for acute stroke: fact or fictional product of subgroup analysis?},
	volume = {309},
	url = {http://www.bmj.com/content/309/6970/1677.abstract},
	shorttitle = {The miracle of {DICE} therapy for acute stroke},
	abstract = {Abstract: "Objective: To determine whether inappropriate subgroup analysis together with chance could change the conclusion of a systematic review of several randomised trials of an ineffective treatment.Design: 44 randomised controlled trials of {DICE} therapy for stroke were performed (simulated by rolling different coloured dice; two trials per investigator). Each roll of the dice yielded the outcome (death or survival) for that “patient.” Publication bias was also simulated. The results were combined in a systematic review.Setting: Edinburgh. Main outcome measure—Mortality.Results: The “hypothesis generating” trial suggested that {DICE} therapy provided complete protection against death from acute stroke. However, analysis of all the trials suggested a reduction of only 11\% ({SD} 11) in the odds of death. A predefined subgroup analysis by colour of dice suggested that red dice therapy increased the odds by 9\% (22). If the analysis excluded red dice trials and those of poor methodological quality the odds decreased by 22\% (13, 2P=0.09). Analysis of “published” trials showed a decrease of 23\% (13, 2P=0.07) while analysis of only those in which the trialist had become familiar with the intervention showed a decrease of 39\% (17, 2P=0.02).Conclusion: The early benefits of {DICE} therapy were not confirmed by subsequent trials. A plausible (but inappropriate) subset analysis of the effects of treatment led to the qualitatively different conclusion that {DICE} therapy reduced mortality, whereas in truth it was ineffective. Chance influences the outcome of clinical trials and systematic reviews of trials much more than many investigators realise, and its effects may lead to incorrect conclusions about the benefits of treatment."},
	pages = {1677 --1681},
	number = {6970},
	journaltitle = {{BMJ}},
	author = {Counsell, Carl E and Clarke, Mike J and Slattery, Jim and Sandercock, Peter A G},
	urldate = {2011-02-23},
	date = {1994-12-24}
}

@article{moreira_reporting_2001,
	title = {Reporting on methods of subgroup analysis in clinical trials: a survey of four scientific journals},
	volume = {34},
	issn = {0100-879X},
	url = {http://www.scielo.br/pdf/bjmbr/v34n11/4111.pdf},
	shorttitle = {Reporting on methods of subgroup analysis in clinical trials},
	abstract = {Abstract: "Results of subgroup analysis ({SA}) reported in randomized clinical trials ({RCT}) cannot be adequately interpreted without information about the methods used in the study design and the data analysis. Our aim was to show how often inaccurate or incomplete reports occur. First, we selected eight methodological aspects of {SA} on the basis of their importance to a reader in determining the confidence that should be placed in the author's conclusions regarding such analysis. Then, we reviewed the current practice of reporting these methodological aspects of {SA} in clinical trials in four leading journals, i.e., the New England Journal of Medicine, the Journal of the American Medical Association, the Lancet, and the American Journal of Public Health. Eight consecutive reports from each journal published after July 1, 1998 were included. Of the 32 trials surveyed, 17 (53\%) had at least one {SA}. Overall, the proportion of {RCT} reporting a particular methodological aspect ranged from 23 to 94\%. Information on whether the {SA} preceded/followed the analysis was reported in only 7 (41\%) of the studies. Of the total possible number of items to be reported, {NEJM}, {JAMA}, Lancet and {AJPH} clearly mentioned 59, 67, 58 and 72\%, respectively. We conclude that current reporting of {SA} in {RCT} is incomplete and inaccurate. The results of such {SA} may have harmful effects on treatment recommendations if accepted without judicious scrutiny. We recommend that editors improve the reporting of {SA} in {RCT} by giving authors a list of the important items to be reported."},
	pages = {1441--1446},
	number = {11},
	journaltitle = {Brazilian Journal of Medical and Biological Research = Revista Brasileira De Pesquisas Médicas E Biológicas / Sociedade Brasileira De Biofísica ... [et Al},
	shortjournal = {Braz. J. Med. Biol. Res},
	author = {Moreira, Jr, E D and Stein, Z and Susser, E},
	urldate = {2011-02-23},
	date = {2001-11},
	pmid = {11668354},
	keywords = {Data Interpretation, Statistical, Humans, Periodicals as Topic, Randomized Controlled Trials as Topic, Research Design}
}

@article{carneiro_subgroup_2002,
	title = {Subgroup analysis in therapeutic trials},
	volume = {21},
	issn = {0870-2551},
	url = {http://www.spc.pt/DL/RPC/artigos/608.pdf},
	abstract = {Abstract: "Therapy in cardiology must be based on solid scientific evidence, obtained in randomized controlled trials ({RCTs}), since this is the best design that proves causality in medicine. The applicability of clinical trial results to the individual patient depends on a rigorous set of rules that can be summarized in the question "Could my patient have been enrolled in this trial?" If the answer to this question is affirmative, then the possibility of applying the trial results is greater. If it is negative, then the cardiologist should exercise caution in his or her decision. In an {RCT}--of whatever size--it is almost always possible to identify subgroups of patients that show significant differences in treatment effect: for example, studies have shown that, in patients with non-rheumatic atrial fibrillation, oral anticoagulants should be given to prevent stroke, except in those younger than 65 years with no additional risk factors, for whom aspirin is a better option. Subgroup analysis is important because, when the magnitude of the difference is both real and large, it may influence patient management. This analysis should be done with great care, since it has the potential to lead to major errors in data interpretation, identifying differences in treatment effects that are due to chance alone or, more frequently, have no clinical significance. In this article we present a set of guidelines that enable the cardiologist to assess the credibility of an analysis that shows apparent differences in treatment effects across subgroups."},
	pages = {339--346},
	number = {3},
	journaltitle = {Revista Portuguesa De Cardiologia: Orgão Oficial Da Sociedade Portuguesa De Cardiologia = Portuguese Journal of Cardiology: An Official Journal of the Portuguese Society of Cardiology},
	shortjournal = {Rev Port Cardiol},
	author = {Carneiro, António Vaz},
	urldate = {2011-02-23},
	date = {2002-03},
	pmid = {12017805},
	keywords = {Heart Diseases, Humans, Randomized Controlled Trials as Topic}
}

@article{cook_subgroup_2004,
	title = {Subgroup Analysis in Clinical Trials},
	volume = {180},
	issn = {0025-729X},
	url = {http://www.mja.com.au/public/issues/180_06_150304/coo10086_fm.html},
	abstract = {Excerpt: "Clinical trials represent a major investment by investigators, sponsors and participants, and it is reasonable to attempt to gain the maximum information from them. Practitioners and regulatory agencies are keen to know whether there are subgroups of trial participants who are more (or less) likely to be helped (or harmed) by the intervention under investigation, and a recent survey of trials published over 3 months in four leading journals found that 70\% included subgroup analyses.1,2 Furthermore, regulatory guidance documents (such as the Committee for Proprietary Medicinal Products September 2002 document Points to consider on multiplicity issues in clinical trials3) strongly encourage appropriate subgroup analyses. The results of subgroup analyses can also drive changes in practice guidelines. For example, the United States National Institutes of Health issued a clinical alert following the unexpected finding in the {BARI} (Bypass Angioplasty Revascularisation Investigation) trial that mortality after angioplasty in patients with diabetes was nearly double that after bypass-graft surgery (P = 0.003).4

Meaningful information from subgroup analyses within a randomised trial is restricted by multiplicity of testing and low statistical power. There is therefore a tension between our wish to identify heterogeneity in the responses of trial participants to trial interventions and our technical capacity for doing so. Surveys on the adequacy of the reporting of clinical trials consistently find the reporting of subgroup analysis to be characterised by poor practice.2,5-7 Item 18 of the {CONSORT} checklist (Box 1) deals with the multiplicity issues that arise in subgroup analysis.8"},
	pages = {289--291},
	number = {6},
	journaltitle = {The Medical Journal of Australia},
	author = {Cook, David I and Gebski, Val J and Keech, Anthony C},
	urldate = {2011-02-23},
	date = {2004-03-15}
}

@article{simes_subgroup_2004,
	title = {Subgroup Analysis: Application To Individual Patient Decisions},
	volume = {180},
	issn = {0025-729X},
	url = {http://www.mja.com.au/public/issues/180_09_030504/sim10218_fm.html},
	shorttitle = {Subgroup Analysis},
	abstract = {Excerpt: "Clinical trials provide evidence of effectiveness of treatments as an average for a group of patients, yet, in clinical medicine, we usually wish to apply these results to individuals. Can we simply apply the overall trial result for each patient, or can the result be tailored to individual patients in some way? Consider a hypothetical example: a randomised trial comparing treatments A and B shows that treatment A is more effective than B among men (P {\textless} 0.001), but not among women (not signficant). Does this mean men should receive the new treatment, but women should not?"},
	pages = {467--469},
	number = {9},
	journaltitle = {The Medical Journal of Australia},
	author = {Simes, R John and Gebski, Val J and Keech, Anthony C},
	urldate = {2011-02-23},
	date = {2004-05-03}
}

@article{hayward_multivariable_2006,
	title = {Multivariable risk prediction can greatly enhance the statistical power of clinical trial subgroup analysis},
	volume = {6},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/6/18},
	doi = {10.1186/1471-2288-6-18},
	abstract = {Abstract: "{BACKGROUND}: When subgroup analyses of a positive clinical trial are unrevealing, such findings are commonly used to argue that the treatment's benefits apply to the entire study population; however, such analyses are often limited by poor statistical power. Multivariable risk-stratified analysis has been proposed as an important advance in investigating heterogeneity in treatment benefits, yet no one has conducted a systematic statistical examination of circumstances influencing the relative merits of this approach vs. conventional subgroup analysis. {METHODS}: Using simulated clinical trials in which the probability of outcomes in individual patients was stochastically determined by the presence of risk factors and the effects of treatment, we examined the relative merits of a conventional vs. a "risk-stratified" subgroup analysis under a variety of circumstances in which there is a small amount of uniformly distributed treatment-related harm. The statistical power to detect treatment-effect heterogeneity was calculated for risk-stratified and conventional subgroup analysis while varying: 1) the number, prevalence and odds ratios of individual risk factors for risk in the absence of treatment, 2) the predictiveness of the multivariable risk model (including the accuracy of its weights), 3) the degree of treatment-related harm, and 5) the average untreated risk of the study population. {RESULTS}: Conventional subgroup analysis (in which single patient attributes are evaluated "one-at-a-time") had at best moderate statistical power (30\% to 45\%) to detect variation in a treatment's net relative risk reduction resulting from treatment-related harm, even under optimal circumstances (overall statistical power of the study was good and treatment-effect heterogeneity was evaluated across a major risk factor [{OR} = 3]). In some instances a multi-variable risk-stratified approach also had low to moderate statistical power (especially when the multivariable risk prediction tool had low discrimination). However, a multivariable risk-stratified approach can have excellent statistical power to detect heterogeneity in net treatment benefit under a wide variety of circumstances, instances under which conventional subgroup analysis has poor statistical power. {CONCLUSION}: These results suggest that under many likely scenarios, a multivariable risk-stratified approach will have substantially greater statistical power than conventional subgroup analysis for detecting heterogeneity in treatment benefits and safety related to previously unidentified treatment-related harm. Subgroup analyses must always be well-justified and interpreted with care, and conventional subgroup analyses can be useful under some circumstances; however, clinical trial reporting should include a multivariable risk-stratified analysis when an adequate externally-developed risk prediction tool is available."},
	pages = {18},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Hayward, Rodney and Kent, David and Vijan, Sandeep and Hofer, Timothy},
	urldate = {2011-02-23},
	date = {2006}
}

@article{hirji_outcome_2009,
	title = {Outcome based subgroup analysis: a neglected concern},
	volume = {10},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/10/1/33},
	doi = {10.1186/1745-6215-10-33},
	shorttitle = {Outcome based subgroup analysis},
	abstract = {Abstract: "{BACKGROUND}: A subgroup of clinical trial subjects identified by baseline characteristics is a proper subgroup while a subgroup determined by post randomization events or measures is an improper subgroup. Both types of subgroups are often analyzed in clinical trial papers. Yet, the extensive scrutiny of subgroup analyses has almost exclusively attended to the former. The analysis of improper subgroups thereby not only flourishes in numerous disguised ways but also does so without a corresponding awareness of its pitfalls. Comparisons of the grade of angina in a heart disease trial, for example, usually include only the survivors. This paper highlights some of the distinct ways in which outcome based subgroup analysis occurs, describes the hazards associated with it, and proposes a simple alternative approach to counter its analytic bias. {RESULTS}: Data from six published trials show that outcome based subgroup analysis, like proper subgroup analysis, may be performed in a post-hoc fashion, overdone, selectively reported, and over interpreted. Six hypothetical trial scenarios illustrate the forms of hidden bias related to it. That bias can, however, be addressed by assigning clinically appropriate scores to the usually excluded subjects and performing an analysis that includes all the randomized subjects. {CONCLUSION}: A greater level of awareness about the practice and pitfalls of outcome based subgroup analysis is needed. When required, such an analysis should maintain the integrity of randomization. This issue needs greater practical and methodologic attention than has been accorded to it thus far.},
	pages = {33},
	number = {1},
	journaltitle = {Trials},
	author = {Hirji, Karim and Fagerland, Morten},
	urldate = {2011-02-23},
	date = {2009}
}

@article{sun_subgroup_2009,
	title = {Subgroup Analysis of Trials Is Rarely Easy ({SATIRE}): a study protocol for a systematic review to characterize the analysis, reporting, and claim of subgroup effects in randomized trials},
	volume = {10},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/10/1/101},
	doi = {10.1186/1745-6215-10-101},
	shorttitle = {Subgroup Analysis of Trials Is Rarely Easy ({SATIRE})},
	abstract = {Abstract: "{BACKGROUND}: Subgroup analyses in randomized trials examine whether effects of interventions differ between subgroups of study populations according to characteristics of patients or interventions. However, findings from subgroup analyses may be misleading, potentially resulting in suboptimal clinical and health decision making. Few studies have investigated the reporting and conduct of subgroup analyses and a number of important questions remain unanswered. The objectives of this study are: 1) to describe the reporting of subgroup analyses and claims of subgroup effects in randomized controlled trials, 2) to assess study characteristics associated with reporting of subgroup analyses and with claims of subgroup effects, and 3) to examine the analysis, and interpretation of subgroup effects for each study's primary outcome. {METHODS}: We will conduct a systematic review of 464 randomized controlled human trials published in 2007 in the 118 Core Clinical Journals defined by the National Library of Medicine. We will randomly select journal articles, stratified in a 1:1 ratio by higher impact versus lower impact journals. According to 2007 {ISI} total citations, we consider the New England Journal of Medicine, {JAMA}, Lancet, Annals of Internal Medicine, and {BMJ} as higher impact journals. Teams of two reviewers will independently screen full texts of reports for eligibility, and abstract data, using standardized, pilot-tested extraction forms. We will conduct univariable and multivariable logistic regression analyses to examine the association of pre-specified study characteristics with reporting of subgroup analyses and with claims of subgroup effects for the primary and any other outcomes. {DISCUSSION}: A clear understanding of subgroup analyses, as currently conducted and reported in published randomized controlled trials, will reveal both strengths and weaknesses of this practice. Our findings will contribute to a set of recommendations to optimize the conduct and reporting of subgroup analyses, and claim and interpretation of subgroup effects in randomized trials."},
	pages = {101},
	number = {1},
	journaltitle = {Trials},
	author = {Sun, Xin and Briel, Matthias and Busse, Jason and Akl, Elie and You, John and Mejza, Filip and Bala, Malgorzata and Diaz-Granados, Natalia and Bassler, Dirk and Mertz, Dominik and Srinathan, Sadeesh and Vandvik, Per and Malaga, German and Alshurafa, Mohamed and Dahm, Philipp and Alonso-Coello, Pablo and Heels-Ansdell, Diane and Bhatnagar, Neera and Johnston, Bradley and Wang, Li and Walter, Stephen and Altman, Douglas and Guyatt, Gordon},
	urldate = {2011-02-23},
	date = {2009}
}

@online{brookes_subgroup_nodate,
	title = {Subgroup analyses in randomised controlled trials: quantifying the risks of false-positives and false-negatives},
	url = {http://www.hta.ac.uk/execsumm/summ533.shtml},
	abstract = {Excerpt: "Subgroup analyses are common in randomised controlled trials ({RCTs}). There are many easily accessible guidelines on the selection and analysis of subgroups but the key messages do not seem to be universally accepted and inappropriate analyses continue to appear in the literature. This has potentially serious implications because erroneous identification of differential subgroup effects may lead to inappropriate provision or withholding of treatment."},
	author = {Brookes, {ST} and Whitley, E and Peters, {TJ} and Mulheran, {PA} and Egger, M and Davey Smith, G},
	urldate = {2010-05-19}
}

@article{perneger_whats_1998,
	title = {What's wrong with Bonferroni adjustments},
	volume = {316},
	url = {http://www.bmj.com/cgi/content/full/316/7139/1236},
	pages = {1236--1238},
	number = {7139},
	journaltitle = {{BMJ}},
	author = {Perneger, Thomas V},
	urldate = {2009-03-10},
	date = {1998-04-18},
	keywords = {Multiple comparisons}
}