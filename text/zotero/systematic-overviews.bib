
@article{rucker_undue_2008,
	title = {Undue reliance on I{\textasciicircum}2 in assessing heterogeneity may mislead},
	volume = {8},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/8/79/abstract},
	doi = {10.1186/1471-2288-8-79},
	abstract = {Abstract: "{BACKGROUND}: The heterogeneity statistic I{\textasciicircum}2, interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included. {METHODS}: Based on a real meta-analysis, we simulate artificially `inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3, ... and for each trial i, we create a M-inflated trial by drawing a treatment effect estimate from the random effects model, using s\_i{\textasciicircum}2/M as within-trial sampling variance. {RESULTS}: As precision increases, while estimates of the heterogeneity variance tau{\textasciicircum}2 remain unchanged on average, estimates of I{\textasciicircum}2 increase rapidly to nearly 100\%. A similar phenomenon is apparent in a sample of 157 meta-analyses. {CONCLUSION}: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. tau{\textasciicircum}2, rather than I{\textasciicircum}2, is the appropriate measure for this purpose."},
	pages = {79},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Rucker, Gerta and Schwarzer, Guido and Carpenter, James and Schumacher, Martin},
	urldate = {2009-01-02},
	date = {2008},
	keywords = {Systematic overviews, a02}
}

@article{tierney_practical_2007,
	title = {Practical methods for incorporating summary time-to-event data into meta-analysis},
	volume = {8},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/8/1/16},
	doi = {10.1186/1745-6215-8-16},
	abstract = {Abstract: "{BACKGROUND}: In systematic reviews and meta-analyses, time-to-event outcomes are most appropriately analysed using hazard ratios ({HRs}). In the absence of individual patient data ({IPD}), methods are available to obtain {HRs} and/or associated statistics by carefully manipulating published or other summary data. Awareness and adoption of these methods is somewhat limited, perhaps because they are published in the statistical literature using statistical notation. {METHODS}: This paper aims to 'translate' the methods for estimating a {HR} and associated statistics from published time-to-event-analyses into less statistical and more practical guidance and provide a corresponding, easy-to-use calculations spreadsheet, to facilitate the computational aspects. {RESULTS}: A wider audience should be able to understand published time-to-event data in individual trial reports and use it more appropriately in meta-analysis. When faced with particular circumstances, readers can refer to the relevant sections of the paper. The spreadsheet can be used to assist them in carrying out the calculations. {CONCLUSION}: The methods cannot circumvent the potential biases associated with relying on published data for systematic reviews and meta-analysis. However, this practical guide should improve the quality of the analysis and subsequent interpretation of systematic reviews and meta-analyses that include time-to-event outcomes.},
	pages = {16},
	number = {1},
	journaltitle = {Trials},
	author = {Tierney, Jayne and Stewart, Lesley and Ghersi, Davina and Burdett, Sarah and Sydes, Matthew},
	urldate = {2009-03-03},
	date = {2007},
	keywords = {Systematic overviews, a02}
}

@article{eisenberg_pharmacotherapies_2008,
	title = {Pharmacotherapies for smoking cessation: a meta-analysis of randomized controlled trials},
	volume = {179},
	url = {http://www.cmaj.ca/cgi/content/abstract/179/2/135},
	doi = {10.1503/cmaj.070256},
	shorttitle = {Pharmacotherapies for smoking cessation},
	abstract = {Description: "This paper is an illustrative example of the use of Bayesian methods for meta-analysis." Abstract: "Background: Many placebo-controlled trials have demonstrated the efficacy of individual pharmacotherapies approved for smoking cessation. However, few direct or indirect comparisons of such interventions have been conducted. We performed a meta-analysis to compare the treatment effects of 7 approved pharmacologic interventions for smoking cessation. Methods: We searched the {US} Centers for Disease Control and Prevention's Tobacco Information and Prevention database as well as {MEDLINE}, {EMBASE} and the Cochrane Library for published reports of placebo-controlled, double-blind randomized controlled trials of pharmacotherapies for smoking cessation. We included studies that reported biochemically validated measures of abstinence at 6 and 12 months. We used a hierarchical Bayesian random-effects model to summarize the results for each intervention. Results: We identified 70 published reports of 69 trials involving a total of 32 908 patients. Six of the 7 pharmacotherapies studied were found to be more efficacious than placebo: varenicline (odds ratio [{OR}] 2.41, 95\% credible interval [{CrI}] 1.91-3.12), nicotine nasal spray ({OR} 2.37, 95\% {CrI} 1.12-5.13), bupropion ({OR} 2.07, 95\% {CrI} 1.73-2.55), transdermal nicotine ({OR} 2.07, 95\% {CrI} 1.69-2.62), nicotine tablet ({OR} 2.06, 95\% {CrI} 1.12-5.13) and nicotine gum ({OR} 1.71, 95\% {CrI} 1.35-2.21). Similar results were obtained regardless of which measure of abstinence was used. Although the point estimate favoured nicotine inhaler over placebo ({OR} 2.17), these results were not conclusive because the credible interval included unity (95\% {CrI} 0.95-5.43). When all 7 interventions were included in the same model, all were more efficacious than placebo. In our analysis of data from the varenicline trials that included bupropion control arms, we found that varenicline was superior to bupropion ({OR} 2.18, 95\% {CrI} 1.09-4.08). Interpretation: Varenicline, bupropion and the 5 nicotine replacement therapies were all more efficacious than placebo at promoting smoking abstinence at 6 and 12 months."},
	pages = {135--144},
	number = {2},
	journaltitle = {{CMAJ}},
	author = {Eisenberg, Mark J. and Filion, Kristian B. and Yavin, Daniel and Belisle, Patrick and Mottillo, Salvatore and Joseph, Lawrence and Gervais, Andre and O'Loughlin, Jennifer and Paradis, Gilles and Rinfret, Stephane and Pilote, Louise},
	urldate = {2009-01-30},
	date = {2008-07-15},
	keywords = {Systematic overviews, a02}
}

@article{hirji_no_2009,
	title = {No short-cut in assessing trial quality: a case study},
	volume = {10},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/10/1/1},
	doi = {10.1186/1745-6215-10-1},
	shorttitle = {No short-cut in assessing trial quality},
	abstract = {Abstract: "{BACKGROUND}: Assessing the quality of included trials is a central part of a systematic review. Many check-list type of instruments for doing this exist. Using a trial of antibiotic treatment for acute otitis media, Burke et al., {BMJ}, 1991, as the case study, this paper illustrates some limitations of the check-list approach to trial quality assessment. {RESULTS}: The general verdict from the check list type evaluations in nine relevant systematic reviews was that Burke et al. (1991) is a good quality trial. All relevant meta-analyses extensively used its data to formulate therapeutic evidence. My comprehensive evaluation, on the other hand, brought to the surface a series of serious problems in the design, conduct, analysis and report of this trial that were missed by the earlier evaluations. {CONCLUSION}: A check-list or instrument based approach, if used as a short-cut, may at times rate deeply flawed trials as good quality trials. Check lists are crucial but they need to be augmented with an in-depth review, and where possible, a scrutiny of the protocol, trial records, and original data. The extent and severity of the problems I uncovered for this particular trial warrant an independent audit before it is included in a systematic review."},
	pages = {1},
	number = {1},
	journaltitle = {Trials},
	author = {Hirji, Karim},
	urldate = {2009-02-23},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{mills_metastatic_2009,
	title = {Metastatic renal cell cancer treatments: An indirect comparison meta-analysis},
	volume = {9},
	issn = {1471-2407},
	url = {http://www.biomedcentral.com/1471-2407/9/34},
	doi = {10.1186/1471-2407-9-34},
	shorttitle = {Metastatic renal cell cancer treatments},
	abstract = {Abstract: "{BACKGROUND}: Treatment for metastatic renal cell cancer ({mRCC}) has advanced dramatically with understanding of the pathogenesis of the disease. New treatment options may provide improved progression-free survival ({PFS}). We aimed to determine the relative effectiveness of new therapies in this field. {METHODS}: We conducted comprehensive searches of 11 electronic databases from inception to April 2008. We included randomized trials ({RCTs}) that evaluated bevacizumab, sorafenib, and sunitinib. Two reviewers independently extracted data, in duplicate. Our primary outcome was investigator-assessed {PFS}. We performed random-effects meta-analysis with a mixed treatment comparison analysis. {RESULTS}: We included 3 bevacizumab (2 of bevacizumab plus interferon-a [{IFN}-a]), 2 sorafenib, 1 sunitinib, and 1 temsirolimus trials (total n=3,957). All interventions offer advantages for {PFS}. Using indirect comparisons with interferon-alpha as the common comparator, we found that sunitinib was superior to both sorafenib ({HR} 0.58, 95\% {CI}, 0.38-0.86, P={\textless}0.001) and bevacizumab + {IFN}-a ({HR} 0.75, 95\% {CI}, 0.60-0.93, P=0.001). Sorafenib was not statistically different from bevacizumab +{IFN}-a in this same indirect comparison analysis ({HR} 0.77, 95\% {CI}, 0.52-1.13, P=0.23). Using placebo as the similar comparator, we were unable to display a significant difference between sorafenib and bevacizumab alone ({HR} 0.81, 95\% {CI}, 0.58-1.12, P=0.23). Temsirolimus provided significant {PFS} in patients with poor prognosis ({HR} 0.69, 95\% {CI}, 0.57-0.85). {CONCLUSIONS}: New interventions for {mRCC} offer a favourable {PFS} for {mRCC} compared to interferon-alpha and placebo."},
	pages = {34},
	number = {1},
	journaltitle = {{BMC} Cancer},
	author = {Mills, Edward and Rachlis, Beth and O'Regan, Chris and Thabane, Lehana and Perri, Dan},
	urldate = {2009-01-30},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{shojania_how_2007,
	title = {How Quickly Do Systematic Reviews Go Out of Date? A Survival Analysis},
	volume = {147},
	url = {http://www.annals.org/cgi/content/abstract/147/4/224},
	shorttitle = {How Quickly Do Systematic Reviews Go Out of Date?},
	abstract = {Abstract: "Background: Systematic reviews are often advocated as the best source of evidence to guide clinical decisions and health care policy, yet we know little about the extent to which they require updating. Objective: To estimate the average time to changes in evidence that are sufficiently important to warrant updating systematic reviews. Design: Survival analysis of 100 quantitative systematic reviews. Sample: Systematic reviews published from 1995 to 2005 and indexed in {ACP} Journal Club. Eligible reviews evaluated a specific drug or class of drug, device, or procedure and included only randomized or quasi-randomized, controlled trials. Measurements: Quantitative signals for updating were changes in statistical significance or relative changes in effect magnitude of at least 50\% involving 1 of the primary outcomes of the original systematic review or any mortality outcome. Qualitative signals included substantial differences in characterizations of effectiveness, new information about harm, and caveats about the previously reported findings that would affect clinical decision making. Results: The cohort of 100 systematic reviews included a median of 13 studies and 2663 participants per review. A qualitative or quantitative signal for updating occurred for 57\% of reviews (95\% {CI}, 47\% to 67\%). Median duration of survival free of a signal for updating was 5.5 years ({CI}, 4.6 to 7.6 years). However, a signal occurred within 2 years for 23\% of reviews and within 1 year for 15\%. In 7\%, a signal had already occurred at the time of publication. Only 4\% of reviews had a signal within 1 year of the end of the reported search period; 11\% had a signal within 2 years of the search. Shorter survival was associated with cardiovascular topics (hazard ratio, 2.70 [{CI}, 1.36 to 5.34]) and heterogeneity in the original review (hazard ratio, 2.15 [{CI}, 1.12 to 4.11]). Limitation: Judgments of the need for updating were made without involving content experts. Conclusion: In a cohort of high-quality systematic reviews directly relevant to clinical practice, signals for updating occurred frequently and within a relatively short time."},
	pages = {224--233},
	number = {4},
	journaltitle = {Ann Intern Med},
	author = {Shojania, Kaveh G. and Sampson, Margaret and Ansari, Mohammed T. and Ji, Jun and Doucette, Steve and Moher, David},
	urldate = {2009-03-10},
	date = {2007-08-21},
	keywords = {Systematic overviews, a02}
}

@online{grade_working_group_grading_nodate,
	title = {The Grading of Recommendations Assessment, Development and Evaluation (short {GRADE}) Working Group},
	url = {http://www.gradeworkinggroup.org/index.htm},
	abstract = {Excerpt: "The Grading of Recommendations Assessment, Development and Evaluation (short {GRADE}) Working Group began in the year 2000 as an informal collaboration of people with an interest in addressing the shortcomings of present grading systems in health care. The working group has developed a common, sensible and transparent approach to grading quality of evidence and strength of recommendations. Many international organizations have provided input into the development of the approach and have started using it."},
	author = {Grade working group},
	urldate = {2009-03-10},
	keywords = {Systematic overviews, a02}
}

@article{guyatt_grade:_2008,
	title = {{GRADE}: an emerging consensus on rating quality of evidence and strength of recommendations},
	volume = {336},
	url = {http://www.bmj.com/cgi/content/full/336/7650/924},
	doi = {10.1136/bmj.39489.470347.AD},
	shorttitle = {{GRADE}},
	abstract = {Excerpt: "Guideline developers around the world are inconsistent in how they rate quality of evidence and grade strength of recommendations. As a result, guideline users face challenges in understanding the messages that grading systems try to communicate. Since 2006 the {BMJ} has requested in its "Instructions to Authors" on bmj.com that authors should preferably use the Grading of Recommendations Assessment, Development and Evaluation ({GRADE}) system for grading evidence when submitting a clinical guidelines article. What was behind this decision? In this first in a series of five articles we will explain why many organisations use formal systems to grade evidence and recommendations and why this is important for clinicians; we will focus on the {GRADE} approach to recommendations. In the next two articles we will examine how the {GRADE} system categorises quality of evidence and strength of recommendations. The final two articles will focus on recommendations for diagnostic tests and {GRADE}’s framework for tackling the impact of interventions on use of resources."},
	pages = {924--926},
	number = {7650},
	journaltitle = {{BMJ}},
	author = {Guyatt, Gordon H and Oxman, Andrew D and Vist, Gunn E and Kunz, Regina and Falck-Ytter, Yngve and Alonso-Coello, Pablo and Schunemann, Holger J and for the {GRADE} Working Group},
	urldate = {2009-01-03},
	date = {2008-04-26},
	keywords = {Systematic overviews, a02}
}

@article{vecchi_does_2009,
	title = {Does direction of results of abstracts submitted to scientific conferences on drug addiction predict full publication?},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/23},
	doi = {10.1186/1471-2288-9-23},
	abstract = {Abstract: "{BACKGROUND}: Data from scientific literature show that about 63\% of abstracts presented at biomedical conferences will be published in full. Some studies have indicated that full publication is associated with the direction of results (publication bias). No study has looked into the occurrence of publication bias in the field of addiction. Objectives: To investigate whether the significance or direction of results of abstracts presented at the major international scientific conference on addiction is associated with full  publication. {METHODS}: The conference proceedings of the {US} Annual Meeting of the College on Problems of Drug Dependence ({CPDD}), were handsearched for abstracts of randomized controlled trials and controlled clinical trials that evaluated interventions for prevention, rehabilitation and treatment of drug addiction in humans (years searched 1993-2002). Data regarding the study designs and outcomes reported were extracted. Subsequent publication in peer reviewed journals was searched in {MEDLINE} and {EMBASE} databases, as of March 2006. {RESULTS}: Out of 5919 abstracts presented, 581 met the inclusion criteria; 359 (62\%) conference abstracts had been published in a broad variety of peer reviewed journals (average time of publication 2.6 years, {SD} +/- 1.78). The proportion of published studies was almost the same for randomized controlled trials (62.4 \%) and controlled clinical trials (59.5 \%) while studies that reported positive results were significantly more likely to be published (74.5\%) than those that did not report statistical results (60.9\%), negative or null results (47.1\%) and no results (38.6\%). Abstracts reporting positive results had a significantly higher probability of being published in full, while abstracts reporting null or negative results were half as likely to be published compared with positive ones ({HR}=0.48; 95\%{CI} 0.30-0.74). {CONCLUSIONS}: Clinical trials were the minority of abstracts presented at the {CPDD}; we found evidence of possible publication bias in the field of addiction, with negative or null results having half the likelihood of being published than positive ones."},
	pages = {23},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Vecchi, Simona and Belleudi, Valeria and Amato, Laura and Davoli, Marina and Perucci, Carlo},
	urldate = {2009-04-23},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{lelorier_discrepancies_1997,
	title = {Discrepancies between Meta-Analyses and Subsequent Large Randomized, Controlled Trials},
	volume = {337},
	url = {http://content.nejm.org/cgi/content/abstract/337/8/536},
	doi = {10.1056/NEJM199708213370806},
	abstract = {Abstract: "Background: Meta-analyses are now widely used to provide evidence to support clinical strategies. However, large randomized, controlled trials are considered the gold standard in evaluating the efficacy of clinical interventions. Methods: We compared the results of large randomized, controlled trials (involving 1000 patients or more) that were published in four journals (the New England Journal of Medicine, the Lancet, the Annals of Internal Medicine, and the Journal of the American Medical Association) with the results of meta-analyses published earlier on the same topics. Regarding the principal and secondary outcomes, we judged whether the findings of the randomized trials agreed with those of the corresponding meta-analyses, and we determined whether the study results were positive (indicating that treatment improved the outcome) or negative (indicating that the outcome with treatment was the same or worse than without it) at the conventional level of statistical significance (P{\textless}0.05). Results: We identified 12 large randomized, controlled trials and 19 meta-analyses addressing the same questions. For a total of 40 primary and secondary outcomes, agreement between the meta-analyses and the large clinical trials was only fair (kappa = 0.35; 95 percent confidence interval, 0.06 to 0.64). The positive predictive value of the meta-analyses was 68 percent, and the negative predictive value 67 percent. However, the difference in point estimates between the randomized trials and the meta-analyses was statistically significant for only 5 of the 40 comparisons (12 percent). Furthermore, in each case of disagreement a statistically significant effect of treatment was found by one method, whereas no statistically significant effect was found by the other. Conclusions: The outcomes of the 12 large randomized, controlled trials that we studied were not predicted accurately 35 percent of the time by the meta-analyses published previously on the same topics."},
	pages = {536--542},
	number = {8},
	journaltitle = {N Engl J Med},
	author = {{LeLorier}, Jacques and Gregoire, Genevieve and Benhaddad, Abdeltif and Lapierre, Julie and Derderian, Francois},
	urldate = {2009-03-07},
	date = {1997-08-21},
	keywords = {Systematic overviews, a02}
}

@article{moreno_assessment_2009,
	title = {Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/2},
	doi = {10.1186/1471-2288-9-2},
	abstract = {Abstract: "{BACKGROUND}: In meta-analysis, the presence of funnel plot asymmetry is attributed to publication or other small-study effects, which causes larger effects to be observed in the smaller studies. This issue potentially mean inappropriate conclusions are drawn from a meta-analysis. If meta-analysis is to be used to inform decision-making, a reliable way to adjust pooled estimates for potential funnel plot asymmetry is required. {METHODS}: A comprehensive simulation study is presented to assess the performance of different adjustment methods including the novel application of several regression-based methods (which are commonly applied to detect publication bias rather than adjust for it) and the popular Trim \& Fill algorithm. Meta-analyses with binary outcomes, analysed on the log odds ratio scale, were simulated by considering scenarios with and without i) publication bias and; ii) heterogeneity. Publication bias was induced through two underlying mechanisms assuming the probability of publication depends on i) the study effect size; or ii) the p-value. {RESULTS}: The performance of all methods tended to worsen as unexplained heterogeneity increased and the number of studies in the meta-analysis decreased. Applying the methods conditional on an initial test for the presence of funnel plot asymmetry generally provided poorer performance than the unconditional use of the adjustment method. Several of the regression based methods consistently outperformed the Trim \& Fill estimators. {CONCLUSIONS}: Regression-based adjustments for publication bias and other small study effects are easy to conduct and outperformed more established methods over a wide range of simulation scenarios."},
	pages = {2},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Moreno, Santiago and Sutton, Alex and Ades, A and Stanley, Tom and Abrams, Keith and Peters, Jaime and Cooper, Nicola},
	urldate = {2009-02-24},
	date = {2009},
	keywords = {Systematic overviews, a02}
}