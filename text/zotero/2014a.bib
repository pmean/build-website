
@article{howick_search_2014,
	title = {In search of justification for the unpredictability paradox},
	volume = {15},
	rights = {http://creativecommons.org/licenses/by/2.0/},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/15/1/480/abstract},
	doi = {10.1186/1745-6215-15-480},
	abstract = {A 2011 Cochrane Review found that adequately randomized trials sometimes revealed larger, sometimes smaller, and often similar effect sizes to inadequately randomized trials. However, they found no average statistically significant difference in effect sizes between the two study types. Yet instead of concluding that adequate randomization had no effect the review authors postulated the "unpredictability paradox", which states that randomized and non-randomized studies differ, but in an unpredictable direction. However, stipulating the unpredictability paradox is problematic for several reasons: 1) it makes the authors' conclusion that adequate randomization makes a difference unfalsifiable--if it turned out that adequately randomized trials had significantly different average results from inadequately randomized trials the authors could have pooled the results and concluded that adequate randomization protected against bias; 2) it leaves other authors of reviews with similar results confused about whether or not to pool results (and hence which conclusions to draw); 3) it discourages researchers from investigating the conditions under which adequate randomization over- or under-exaggerates apparent treatment benefits; and 4) it could obscure the relative importance of allocation concealment and blinding which may be more important than adequate randomization.},
	pages = {480},
	number = {1},
	journaltitle = {Trials},
	author = {Howick, Jeremy and Mebius, Alexander},
	urldate = {2014-12-11},
	date = {2014-12-10},
	langid = {english}
}

@article{friedrich_inclusion_2007,
	title = {Inclusion of zero total event trials in meta-analyses maintains analytic consistency and incorporates all available data},
	volume = {7},
	rights = {2007 Friedrich et al; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/7/5/abstract},
	doi = {10.1186/1471-2288-7-5},
	abstract = {Meta-analysis handles randomized trials with no outcome events in both treatment and control arms inconsistently, including them when risk difference ({RD}) is the effect measure but excluding them when relative risk ({RR}) or odds ratio ({OR}) are used. This study examined the influence of such trials on pooled treatment effects.
{PMID}: 17244367},
	pages = {5},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Friedrich, Jan O. and Adhikari, Neill {KJ} and Beyene, Joseph},
	urldate = {2014-10-06},
	date = {2007-01-23},
	langid = {english},
	pmid = {17244367}
}

@article{white_eliciting_2007,
	title = {Eliciting and using expert opinions about dropout bias in randomized controlled trials},
	volume = {4},
	issn = {1740-7745},
	doi = {10.1177/1740774507077849},
	abstract = {{BACKGROUND}: The analysis of clinical trials with dropout usually assumes the missing data are ;missing at random', i.e. given an individual's past observed data, their probability of dropout does not depend on their present outcome. However, in many settings this assumption is implausible, so it is sensible to assess the robustness of conclusions to departures from missing at random.
{PURPOSE}: To develop a practical, accessible, approach that allows expert opinions about the degree of departure from missing at random in the analysis of a clinical trial to be meaningfully and accurately elicited and incorporated in sensitivity analysis.
{METHODS}: We elicit experts' prior beliefs about the mean difference between missing and observed outcomes in each trial arm. Then we perform a Bayesian synthesis of the information in the trial data with that in the experts' prior, using (i) a full Bayesian analysis for which we give {WinBUGS} code, and (ii) a simple approximate formula for the estimated treatment effect and its standard error. We illustrate our approach by re-analysing a recent trial of interventions to improve the quality of peer review.
{RESULTS}: In the peer review trial, the approximate formula agreed well with the full Bayesian analysis, and both showed substantially larger standard errors than an analysis assuming missing at random.
{LIMITATIONS}: Strictly, the method is only applicable if the outcome is normally distributed. We did not elicit the full bivariate prior distribution, and instead used a sensitivity analysis. Our approach is not designed to incorporate prior beliefs about the intervention effect itself.
{CONCLUSIONS}: Our proposed approach allows for the greater uncertainty introduced by missing data that are potentially informatively missing. It can therefore claim to be a truly conservative method, unlike methods such as ;last observation carried forward'. It is practical and accessible to non-statisticians. It should be considered as part of the design and analysis of future clinical trials.},
	pages = {125--139},
	number = {2},
	journaltitle = {Clinical Trials (London, England)},
	shortjournal = {Clin Trials},
	author = {White, Ian R. and Carpenter, James and Evans, Stephen and Schroter, Sara},
	date = {2007},
	pmid = {17456512}
}

@online{noauthor_parameters_nodate,
	title = {Parameters and percentiles {\textbar} The Endeavour},
	url = {http://www.johndcook.com/blog/2010/01/31/parameters-from-percentiles/},
	urldate = {2014-08-15}
}

@article{vanpaemel_constructing_2011,
	title = {Constructing informative model priors using hierarchical methods},
	volume = {55},
	issn = {00222496},
	url = {http://yadda.icm.edu.pl/yadda/element/bwmeta1.element.elsevier-27c56527-2ff3-37b5-b85e-fe1a442e7dc4},
	doi = {10.1016/j.jmp.2010.08.005},
	pages = {106--117},
	number = {1},
	journaltitle = {Journal of Mathematical Psychology},
	author = {Vanpaemel, Wolf},
	urldate = {2014-08-15},
	date = {2011-02},
	langid = {english}
}

@article{sung_seven_2005,
	title = {Seven items were identified for inclusion when reporting a Bayesian analysis of a clinical study},
	volume = {58},
	issn = {0895-4356},
	doi = {10.1016/j.jclinepi.2004.08.010},
	abstract = {{OBJECTIVE}: (1) To generate a list of items that experts consider most important when reporting a Bayesian analysis of a clinical study, (2) to report on the extent to which we found these items in the literature, and (3) to identify factors related to the number of items in a report.
{STUDY} {DESIGN} {AND} {SETTING}: Based on opinions from 23 international experts, we determined the items considered most important when publishing a Bayesian analysis. We then performed a literature search to identify articles in which a Bayesian analysis was performed and determined the extent to which we found these items in each report. Finally, we examined the relationship between the number of items in a report and journal- and article-specific attributes.
{RESULTS}: Our final set of seven items described the prior distribution (specification, justification, and sensitivity analysis), analysis (statistical model and analytic technique), and presentation of results (central tendency and variance). There was {\textgreater}99\% probability that more items were reported in studies with a noncontrolled study design and in journals with a methodological focus, lower impact factor, and absence of a word count limit.
{CONCLUSION}: We developed a set of seven items that experts believe to be most important when reporting a Bayesian analysis.},
	pages = {261--268},
	number = {3},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Sung, Lillian and Hayden, Jill and Greenberg, Mark L. and Koren, Gideon and Feldman, Brian M. and Tomlinson, George A.},
	date = {2005-03},
	pmid = {15718115}
}

@article{spiegelhalter_bayesian_2000,
	title = {Bayesian methods in health technology assessment: a review},
	volume = {4},
	issn = {1366-5278},
	shorttitle = {Bayesian methods in health technology assessment},
	abstract = {{BACKGROUND}: Bayesian methods may be defined as the explicit quantitative use of external evidence in the design, monitoring, analysis, interpretation and reporting of a health technology assessment. In outline, the methods involve formal combination through the use of Bayes's theorem of: 1. a prior distribution or belief about the value of a quantity of interest (for example, a treatment effect) based on evidence not derived from the study under analysis, with 2. a summary of the information concerning the same quantity available from the data collected in the study (known as the likelihood), to yield 3. an updated or posterior distribution of the quantity of interest. These methods thus directly address the question of how new evidence should change what we currently believe. They extend naturally into making predictions, synthesising evidence from multiple sources, and designing studies: in addition, if we are willing to quantify the value of different consequences as a 'loss function', Bayesian methods extend into a full decision-theoretic approach to study design, monitoring and eventual policy decision-making. Nonetheless, Bayesian methods are a controversial topic in that they may involve the explicit use of subjective judgements in what is conventionally supposed to be a rigorous scientific exercise.
{OBJECTIVES}: This report is intended to provide: 1. a brief review of the essential ideas of Bayesian analysis 2. a full structured review of applications of Bayesian methods to randomised controlled trials, observational studies, and the synthesis of evidence, in a form which should be reasonably straightforward to update 3. a critical commentary on similarities and differences between Bayesian and conventional approaches 4. criteria for assessing the reporting of a Bayesian analysis 5. a comprehensive list of published 'three-star' examples, in which a proper prior distribution has been used for the quantity of primary interest 6. tutorial case studies of a variety of types 7. recommendations on how Bayesian methods and approaches may be assimilated into health technology assessments in a variety of contexts and by a variety of participants in the research process.
{METHODS}: The {BIDS} {ISI} database was searched using the terms 'Bayes' or 'Bayesian'. This yielded almost 4000 papers published in the period 1990-98. All resultant abstracts were reviewed for relevance to health technology assessment; about 250 were so identified, and used as the basis for forward and backward searches. In addition {EMBASE} and {MEDLINE} databases were searched, along with websites of prominent authors, and available personal collections of references, finally yielding nearly 500 relevant references. A comprehensive review of all references describing use of 'proper' Bayesian methods in health technology assessment (those which update an informative prior distribution through the use of Bayes's theorem) has been attempted, and around 30 such papers are reported in structured form. There has been very limited use of proper Bayesian methods in practice, and relevant studies appear to be relatively easily identified.
{RESULTS}: Bayesian methods in the health technology assessment context 1. Different contexts may demand different statistical approaches. Prior opinions are most valuable when the assessment forms part of a series of similar studies. A decision-theoretic approach may be appropriate where the consequences of a study are reasonably predictable. 2. The prior distribution is important and not unique, and so a range of options should be examined in a sensitivity analysis. Bayesian methods are best seen as a transformation from initial to final opinion, rather than providing a single 'correct' inference. 3. The use of a prior is based on judgement, and hence a degree of subjectivity cannot be avoided. However, subjective priors tend to show predictable biases, and archetypal priors may be useful for identifying a reasonable range of prior opinion.},
	pages = {1--130},
	number = {38},
	journaltitle = {Health Technology Assessment (Winchester, England)},
	shortjournal = {Health Technol Assess},
	author = {Spiegelhalter, D. J. and Myles, J. P. and Jones, D. R. and Abrams, K. R.},
	date = {2000},
	pmid = {11134920}
}

@online{jmckeel_trials_2011,
	title = {Trials and Errors: Why Science Is Failing Us {\textbar} Magazine},
	url = {http://www.wired.com/2011/12/ff_causation/all/1},
	shorttitle = {Trials and Errors},
	abstract = {Dead-end experiments. Useless drugs. Unneeded surgery. The truth is, our "scientific" stories about causation are shadowed by all sorts of mental shortcuts.},
	titleaddon = {{WIRED}},
	author = {jmckeel},
	urldate = {2014-07-24},
	date = {2011-12-16}
}

@online{noauthor_if_nodate,
	title = {If correlation doesn’t imply causation, then what does? {\textbar} {DDI}},
	url = {http://www.michaelnielsen.org/ddi/if-correlation-doesnt-imply-causation-then-what-does/},
	shorttitle = {If correlation doesn’t imply causation, then what does?},
	urldate = {2014-07-24}
}

@article{fan_challenges_2014,
	title = {Challenges of Big Data analysis},
	volume = {1},
	issn = {2095-5138, 2053-714X},
	url = {http://nsr.oxfordjournals.org/content/1/2/293},
	doi = {10.1093/nsr/nwt032},
	abstract = {Big Data bring new opportunities to modern society and challenges to data scientists. On the one hand, Big Data hold great promises for discovering subtle population patterns and heterogeneities that are not possible with small-scale data. On the other hand, the massive sample size and high dimensionality of Big Data introduce unique computational and statistical challenges, including scalability and storage bottleneck, noise accumulation, spurious correlation, incidental endogeneity and measurement errors. These challenges are distinguished and require new computational and statistical paradigm. This paper gives overviews on the salient features of Big Data and how these features impact on paradigm change on statistical and computational methods as well as computing architectures. We also provide various new perspectives on the Big Data analysis and computation. In particular, we emphasize on the viability of the sparsest solution in high-confidence set and point out that exogenous assumptions in most statistical methods for Big Data cannot be validated due to incidental endogeneity. They can lead to wrong statistical inferences and consequently wrong scientific conclusions.},
	pages = {293--314},
	number = {2},
	journaltitle = {National Science Review},
	shortjournal = {Natl Sci Rev},
	author = {Fan, Jianqing and Han, Fang and Liu, Han},
	urldate = {2014-07-23},
	date = {2014-06-01},
	langid = {english}
}

@online{noauthor_cran_nodate,
	title = {{CRAN} Task View: Web Technologies and Services},
	url = {http://cran.r-project.org/web/views/WebTechnologies.html},
	urldate = {2014-07-23}
}

@article{greenhalgh_evidence_2014,
	title = {Evidence based medicine: a movement in crisis?},
	volume = {348},
	issn = {1756-1833},
	url = {http://www.bmj.com/content/348/bmj.g3725},
	doi = {10.1136/bmj.g3725},
	shorttitle = {Evidence based medicine},
	pages = {g3725--g3725},
	issue = {jun13 4},
	journaltitle = {{BMJ}},
	author = {Greenhalgh, T. and Howick, J. and Maskrey, N. and {for the Evidence Based Medicine Renaissance Group}},
	urldate = {2014-07-16},
	date = {2014-06-13},
	langid = {english}
}

@video{james_mccormack_viva_2013,
	title = {Viva La Evidence},
	url = {http://www.youtube.com/watch?v=QUW0Q8tXVUc&feature=youtube_gdata_player},
	abstract = {Viva La Evidence --  a parody of Coldplay's Viva La Vida  - a song all about evidence based healthcare -- a little bit about the history of evidence and then the key principles. Hope you like it.

{CREDITS}

{LYRICS} {AND} {VIDEO} {PRODUCTION} {BY}: James {McCormack}

{GREAT} {VOCALS} {BY}: Shae Scotten and Liam Styles Chang -- 2 guys from a great band called Aivia from Victoria, {BC} - {THANKS} {GUYS} -- check them out at http://www.youtube.com/user/weareaivia

{BACKING} {TRACK}: Purchased and downloaded from http://www.karaoke-version.com/custombackingtrack/coldplay/viva-la-vida.html and written permission was obtained.

{LYRICS}:
I used to view the world
As a place where I gave the word
It had the feeling of eminence
But then along came evidence

I used to give advice
See disarray in my patient's eyes
I listened to the companies sing
"Now the old drug is dead! We got a new thing!"

One minute I held the key
Next the walls were closed on me
And I discovered that my practice stands
Upon pillars of salt and pillars of sand

I hear the evidence bells a ringing
Systematic reviews are singing
Clinical evidence is the maxim
{NNTs} are now my passion

For some reason I can't explain 
Clinical studies are in my brain
Always an honest word
Now it's how I view the world

The beauty of forest plots 
Who knew they were more than dots 
Confidence intervals show the light   
I can figure out wrong from right

Relative numbers they can wait
I want absolute numbers on my plate
Values and preference 
Really make all the difference
 
I hear the evidence bells a ringing
Systematic reviews are singing
Clinical evidence is the maxim
{NNTs} are now my passion

For some reason I can't explain 
Clinical studies are in my brain
Always an honest word
Now it's how I view the world

I hear the evidence bells a ringing
Systematic reviews are singing
Clinical evidence is the maxim
{NNTs} are now my passion

For some reason I can't explain 
Clinical studies are in my brain
Always an honest word
Now it's how I view the world

Copyright Issues
1. The original song is Viva La Vida by Coldplay -- great song
2. In Canada we have a law that states the use of copyrighted material as part of  "Fair dealing for the purpose of research, private study, education, parody or satire does not infringe copyright" 
3. The use is solely for non-commercial purposes
4. The use is solely for the purposes of education - promoting the concepts of evidence-based medicine to healthcare providers and patients
5. It is parody as it equates the original "religiousness" of the song to medications and evidence in medical practice
6. Written permission from the Karaoke (the background music) people to use their content was obtained http://www.karaoke-version.com/custombackingtrack/coldplay/viva-la-vida.html
7. Substantive transformative changes have been made - new lyrics and vocals
8. Where needed/possible (paid or free) copyright for the images and videos used has been obtained},
	editora = {{James McCormack}},
	editoratype = {collaborator},
	urldate = {2014-07-16},
	date = {2013-08-23}
}

@video{james_mccormack_bohemian_2014,
	title = {Bohemian Polypharmacy},
	url = {http://www.youtube.com/watch?v=Lp3pFjKoZl8&feature=youtube_gdata_player},
	abstract = {Bohemian Polypharmacy - a parody of Queen's classic song Bohemian Rhapsody - a song all about polypharmacy - taking more medicines than are clinically indicated.

Hope you like it.

{LYRICS} {AND} {VIDEO} {PRODUCTION}
James {McCormack} - with lyric help from David Scotten and creative input from Pete {McCormack}
Also thanks to those of you who provided some editing comments - you know who you are and thanks for your ideas!!

{GREAT} {VOCALS}
Liam Styles Chang (Lead Vocals)
Shae Scotten (Background Vocals)
Check their band out at http://www.youtube.com/user/weareaivia
Check out their new album "The Four Winds" at http://goo.gl/{pBxW}94

{AWESOME} {ANIMATION} {AND} {GREAT} {CREATIVE} {INPUT}
Sam Gilchrist

{BACKING} {TRACK}
Purchased and downloaded from  http://www.karaoke-version.com 
and written permission was obtained.

{IMAGES}
Almost all images were purchased from shutterstock.com

{LYRICS} - a Spanish version of the lyrics can be found here http://goo.gl/76pIOG courtesy of Martín Cañás

Is this my real life?
Is this my destiny?
I've been caught in a landslide,
Must escape polypharmacy.

I've opened my eyes,
Looked up to the skies now I see,
If I'm not a sick boy, I need no remedy,
But I trusted you, you should know
You started high, never low,
Many guidelines are opinions so they may not apply to me, to me.

Medications, can kill a man,
Like a gun against his head,
Used too many, now he's dead.
Medications, can also help,
So no don't go and throw them all away.

Medications, ooh,
I'll give them a try,
But if I've not improved at all by this time tomorrow,
I won't carry on, carry on unless I'm feeling better.

Too late, this drug I'm on,
Made mincemeat of my mind,
Body's aching all the time.
Come on some drugs have got to go,
Gotta leave these all behind and find what works.

Medications, ooh (is this helping who knows),
I don't wanna try,
Unless you tell me the benefits and harms for all.

Drugs can make a little silhouetto of a man,
Where's the proof, where's the proof, do you have at least one study?
Randomised and blinded,
Really enlightening me.
(Cochrane library) Cochrane library.
(Cochrane library) Cochrane library,
Cochrane library. Did you know?
Magnifico.

I'm not a slow boy, inform me well I may agree 
He's not a slow boy please listen to his plea,
Spare him his life from polypharmacy.

Easy come, easy go, will you start it low?
Specialist! No, we will not start it low.
(Start it low!) Specialist! We will not start it low.
(Start it low!) Specialist! We will not start it low.
(Start it low) Will not start it low.
(Start it low) Will not start it low.
Ah. No, no, no, no, no, no, no.
Medications, medications, medications, start them low.
We've got to find the right dose that will work for me, for me, for me.

So you think you can test me and not tell me why?
So you think you can drug me and leave me to die?
Oh, baby, can't do this to me, baby,
Just gotta get out, just gotta get right outta here.

(Oh, yeah, oh yeah)

Stopping medications,
Anyone can see,
Stopping medications,
Can really make a difference to me.

Is this helping, who knows?

Copyright Issues
1. The original song is Bohemian Rhapsody by Queen - a classic song
2. In Canada we have a law that states the use of copyrighted material as part of  "Fair dealing for the purpose of research, private study, education, parody or satire does not infringe copyright" 
3. The use is solely for non-commercial purposes
4. The use is solely for the purposes of education - promoting the concepts of evidence-based medicine to healthcare providers and patients
5. It is parody as it equates the original context of the song - about relationships - to the love/hate benefit/harm relationships we have with medications
6. Written permission from the Karaoke (the background music) people to use their content was obtained http://www.karaoke-version.com
7. Substantive transformative changes have been made - new lyrics and vocals
8. Where needed/possible (paid or free) copyright for the images and videos used has been obtained},
	editora = {{James McCormack}},
	editoratype = {collaborator},
	urldate = {2014-07-16},
	date = {2014-02-19}
}

@article{smith_correcting_2014,
	title = {Correcting for Optimistic Prediction in Small Data Sets},
	issn = {1476-6256},
	doi = {10.1093/aje/kwu140},
	abstract = {The C statistic is a commonly reported measure of screening test performance. Optimistic estimation of the C statistic is a frequent problem because of overfitting of statistical models in small data sets, and methods exist to correct for this issue. However, many studies do not use such methods, and those that do correct for optimism use diverse methods, some of which are known to be biased. We used clinical data sets (United Kingdom Down syndrome screening data from Glasgow (1991-2003), Edinburgh (1999-2003), and Cambridge (1990-2006), as well as Scottish national pregnancy discharge data (2004-2007)) to evaluate different approaches to adjustment for optimism. We found that sample splitting, cross-validation without replication, and leave-1-out cross-validation produced optimism-adjusted estimates of the C statistic that were biased and/or associated with greater absolute error than other available methods. Cross-validation with replication, bootstrapping, and a new method (leave-pair-out cross-validation) all generated unbiased optimism-adjusted estimates of the C statistic and had similar absolute errors in the clinical data set. Larger simulation studies confirmed that all 3 methods performed similarly with 10 or more events per variable, or when the C statistic was 0.9 or greater. However, with lower events per variable or lower C statistics, bootstrapping tended to be optimistic but with lower absolute and mean squared errors than both methods of cross-validation.},
	journaltitle = {American Journal of Epidemiology},
	shortjournal = {Am. J. Epidemiol.},
	author = {Smith, Gordon C. S. and Seaman, Shaun R. and Wood, Angela M. and Royston, Patrick and White, Ian R.},
	date = {2014-06-24},
	pmid = {24966219}
}

@video{james_mccormack_choosing_2014,
	title = {Choosing Wisely},
	url = {http://www.youtube.com/watch?v=FqQ-JuRDkl8&feature=youtube_gdata_player},
	abstract = {Choosing Wisely - a parody of the infectious Pharrell Williams song "Happy" - choose wisely when it comes to making health care decisions and if you choose wisely it will make you happy.

{LYRICS} {AND} {VIDEO} {PRODUCTION}
James {McCormack}

{GREAT} {VOCALS}
Liam Styles Chang (Lead Vocals)
Shae Scotten (Background Vocals)
Check their band out at http://www.youtube.com/user/weareaivia
Check out their album "The Four Winds" at http://goo.gl/{pBxW}94

{BACKING} {TRACK}
Purchased and downloaded from http://www.karaoke-versio.com 
and written permission was obtained

{VIDEO}
The video is a mash-up of content from http://24hoursofhappy.com

{IMAGES}
Almost all images were purchased from shutterstock.com

{CHOOSING} {WISELY} - for spanish lyrics go to http://goo.gl/{MsDYVw}

It might seem crazy what I'm about to say
Less is more can often be the best way
There are tests, treatments and procedures you don't really need
While some are very useful, some at best mislead

[Chorus:]
Now we're choosing wisely
Body scans annual exams can do more harm than good
We're choosing wisely
Imaging for minor things may not give you the truth
We're choosing wisely
Antibiotics for a cold will do nothing but make you ill
We're choosing wisely
A routine screen for many things is often overkill

[Verse 2:]
You tell me you're worried you've got this and that, yeah,
Well, I truly care so don't hold it back, yeah,
But, from what I can see you'll be just fine, yeah,
You don't need that test, let's not waste our time 
Here's why

[Chorus]

[Bridge:] 
We want to (wisely) keep you - free from harm
That test result (wisely) can - cause alarm
That treatment (wisely) can make you - feel run-down
That drug  (wisely) could spin your head - round and round
I said (I want to keep you) (Wisely, wisely, wisely) - free from harm 
(Wisely, wisely, wisely) 
For that procedure you'll have to wear a - hospital gown
Doing nothing (wisely) can be your - lucky charm
I said

[Chorus 2x]

Hey, come on 

(wisely)
Free from harm ... do the right thing ... (wisely) 
Free from harm ... that dose is too high ... (wisely)
Free from harm ... use common sense ... (wisely) 
Free from harm, I said (let me tell you now)

 [Chorus 2x]

Copyright Issues
1. The original song is Happy by Pharrell Williams - one of the catchiest songs ever
2. In Canada we have a law that states the use of copyrighted material as part of  "Fair dealing for the purpose of research, private study, education, parody or satire does not infringe copyright" 
3. The use is solely for non-commercial purposes
4. The use is solely for the purposes of education - promoting the concepts of evidence-based medicine, shared decision making and common sense to healthcare providers and patients
5. It is parody as it equates the original context of the song - happy - to the concept of choosing wisely when it comes to health care decisions
6. Written permission from the Karaoke (the background music) people to use their content was obtained http://www.karaoke-version.com
7. Substantive transformative changes have been made - new lyrics and vocals
8. Where needed/possible (paid or free) copyright for the images used has been obtained - shutter stock.com
9.This mash-up video used less than 0.3\% of the video content at 24hoursofhappy.com},
	editora = {{James McCormack}},
	editoratype = {collaborator},
	urldate = {2014-07-16},
	date = {2014-06-25}
}

@article{greenhalgh_intuition_2002,
	title = {Intuition and evidence--uneasy bedfellows?},
	volume = {52},
	issn = {0960-1643},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1314297/},
	abstract = {Intuition is a decision-making method that is used unconsciously by experienced practitioners but is inaccessible to the novice. It is rapid, subtle, contextual, and does not follow simple, cause-and-effect logic. Evidence-based medicine offers exciting opportunities\_for improving patient outcomes, but the 'evidence-burdened' approach of the inexperienced, protocol-driven clinician is well documented Intuition is not unscientific. It is a highly creative process, fundamental to hypothesis generation in science. The experienced practitioner should generate and follow clinical hunches as well as (not instead of applying the deductive principles of evidence-based medicine. The educational research literature suggests that we can improve our intuitive powers through systematic critical reflection about intuitive judgements--for example, through creative writing and dialogue with professional colleagues. It is time to revive and celebrate clinical storytelling as a method for professional education and development. The stage is surely set for a new, improved--and, indeed, evidence-based--'Balint'group.},
	pages = {395--400},
	number = {478},
	journaltitle = {The British Journal of General Practice},
	shortjournal = {Br J Gen Pract},
	author = {Greenhalgh, Trisha},
	urldate = {2014-07-16},
	date = {2002-05},
	pmid = {12014539},
	pmcid = {PMC1314297}
}

@online{noauthor_statistic_nodate,
	title = {Statistic Applets for Teaching Topics in Introductory Courses},
	url = {http://sapphire.indstate.edu/~stat-attic/index.php},
	urldate = {2014-06-26}
}

@online{noauthor_jmp_nodate,
	title = {{JMP} Graph Builder for {iPad}},
	url = {http://www.jmp.com/software/jmp/jmp-graph-builder-for-ipad.shtml},
	urldate = {2014-06-26}
}

@online{leek_10_nodate,
	title = {10 things statistics taught us about big data analysis {\textbar} Simply Statistics},
	url = {http://simplystatistics.org/2014/05/22/10-things-statistics-taught-us-about-big-data-analysis/},
	author = {Leek, Jeff},
	urldate = {2014-06-26}
}

@online{noauthor_reproducible_nodate,
	title = {Reproducible research is still a challenge},
	rights = {{CC}0},
	url = {/blog/2014/06/09/reproducibility},
	type = {website},
	urldate = {2014-06-26},
	langid = {english},
	keywords = {Ecology}
}

@online{noauthor_free_nodate,
	title = {Free {DataSet} Directory {\textbar} {AggData}},
	url = {http://aggdata.com/free-data},
	urldate = {2014-06-26}
}

@online{noauthor_fluturas_nodate,
	title = {Fluturas 22 Non Statistical Questions for a Statistician! {\textbar} Flutura in M2M and Big Data Analytics},
	url = {http://blog.fluturasolutions.com/2014/06/fluturas-22-non-statistical-questions.html#.U6MAP7FCxLc},
	urldate = {2014-06-19}
}

@online{noauthor_tom_nodate,
	title = {Tom Jefferson and Peter Doshi: {EMA}’s double U-turn on its peeping tom policy for data release?},
	url = {http://blogs.bmj.com/bmj/2014/06/13/tom-jefferson-and-peter-doshi-emas-double-u-turn-on-its-peeping-tom-policy-for-data-release/},
	shorttitle = {Tom Jefferson and Peter Doshi},
	abstract = {Yesterday’s announcement that the {EMA} Management Board may have adopted a less obstructive policy to releasing clinical trial data comes h},
	titleaddon = {{BMJ}},
	urldate = {2014-06-16}
}

@online{bristol_bristol_nodate,
	title = {Bristol University {\textbar} Centre for Multilevel Modelling {\textbar} {MLPowSim}},
	rights = {http://www.bris.ac.uk/university/web/terms-conditions.html},
	url = {http://www.bristol.ac.uk/cmm/software/mlpowsim/},
	abstract = {{MLPowSim}},
	author = {Bristol, University of},
	urldate = {2014-06-16},
	note = {{MLPowSim}}
}

@article{bland_comparisons_2011,
	title = {Comparisons within randomised groups can be very misleading},
	volume = {342},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/content/342/bmj.d561},
	doi = {10.1136/bmj.d561},
	pages = {d561--d561},
	issue = {may06 2},
	journaltitle = {{BMJ}},
	author = {Bland, J. M. and Altman, D. G.},
	urldate = {2014-06-11},
	date = {2011-05-06},
	langid = {english}
}

@online{noauthor_faq/paranp_nodate,
	title = {{FAQ}/paranp - {CBU} statistics Wiki},
	url = {http://imaging.mrc-cbu.cam.ac.uk/statswiki/FAQ/paranp},
	urldate = {2014-06-10}
}

@article{muhlhausler_whole_2013,
	title = {Whole Animal Experiments Should Be More Like Human Randomized Controlled Trials},
	volume = {11},
	url = {http://dx.doi.org/10.1371/journal.pbio.1001481},
	doi = {10.1371/journal.pbio.1001481},
	abstract = {The quality of reporting of animal studies lags behind that of human randomized controlled trials but a series of additions to the {ARRIVE} guidelines will help ensure that the standards are comparable.},
	pages = {e1001481},
	number = {2},
	journaltitle = {{PLoS} Biol},
	shortjournal = {{PLoS} Biol},
	author = {Muhlhausler, Beverly S. and Bloomfield, Frank H. and Gillman, Matthew W.},
	urldate = {2014-06-10},
	date = {2013-02-12}
}

@article{hirst_need_2014,
	title = {The Need for Randomization in Animal Trials: An Overview of Systematic Reviews},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0098856},
	doi = {10.1371/journal.pone.0098856},
	shorttitle = {The Need for Randomization in Animal Trials},
	abstract = {Background and Objectives Randomization, allocation concealment, and blind outcome assessment have been shown to reduce bias in human studies. Authors from the Collaborative Approach to Meta Analysis and Review of Animal Data from Experimental Studies ({CAMARADES}) collaboration recently found that these features protect against bias in animal stroke studies. We extended the scope the work from {CAMARADES} to include investigations of treatments for any condition. Methods We conducted an overview of systematic reviews. We searched Medline and Embase for systematic reviews of animal studies testing any intervention (against any control) and we included any disease area and outcome. We included reviews comparing randomized versus not randomized (but otherwise controlled), concealed versus unconcealed treatment allocation, or blinded versus unblinded outcome assessment. Results Thirty-one systematic reviews met our inclusion criteria: 20 investigated treatments for experimental stroke, 4 reviews investigated treatments for spinal cord diseases, while 1 review each investigated treatments for bone cancer, intracerebral hemorrhage, glioma, multiple sclerosis, Parkinson's disease, and treatments used in emergency medicine. In our sample 29\% of studies reported randomization, 15\% of studies reported allocation concealment, and 35\% of studies reported blinded outcome assessment. We pooled the results in a meta-analysis, and in our primary analysis found that failure to randomize significantly increased effect sizes, whereas allocation concealment and blinding did not. In our secondary analyses we found that randomization, allocation concealment, and blinding reduced effect sizes, especially where outcomes were subjective. Conclusions Our study demonstrates the need for randomization, allocation concealment, and blind outcome assessment in animal research across a wide range of outcomes and disease areas. Since human studies are often justified based on results from animal studies, our results suggest that unduly biased animal studies should not be allowed to constitute part of the rationale for human trials.},
	pages = {e98856},
	number = {6},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Hirst, Jennifer A. and Howick, Jeremy and Aronson, Jeffrey K. and Roberts, Nia and Perera, Rafael and Koshiaris, Constantinos and Heneghan, Carl},
	urldate = {2014-06-10},
	date = {2014-06-06}
}

@article{austad_ke_association_2014,
	title = {Association of marketing interactions with medical trainees’ knowledge about evidence-based prescribing: Results from a national survey},
	issn = {2168-6106},
	url = {http://dx.doi.org/10.1001/jamainternmed.2014.2202},
	doi = {10.1001/jamainternmed.2014.2202},
	shorttitle = {Association of marketing interactions with medical trainees’ knowledge about evidence-based prescribing},
	abstract = {Importance 
In recent years, numerous {US} medical schools and academic medical centers have enacted policies preventing pharmaceutical sales representatives from interacting directly with students. Little is known about how pharmaceutical sales representatives affect trainees’ knowledge about pharmaceutical prescribing.Objective
To determine whether there is an association between medical trainees’ interactions with pharmaceutical promotion and their preferences in medication use.Design, Setting, and Participants
We surveyed a nationally representative sample of first- and fourth-year medical students and third-year residents by randomly selecting at least 14 trainees at each level per school.Exposures
All trainees were asked how often they used different educational resources to learn about prescription drugs. Among fourth-year students and residents, we posed a series of multiple choice knowledge questions asking about the appropriate initial therapy for clinical scenarios involving patients with diabetes, hyperlipidemia, hypertension, and difficulty sleeping.Main Outcomes and Measures
Evidence-based answers followed widely used clinical guidelines, while marketed-drug answers favored brand-name drugs over generic alternatives. We used survey answers to build an industry relations index assessing each trainee’s level of acceptance of pharmaceutical promotion; we used proportional odds logistic regression models to estimate the association between the index and responses to the knowledge questions.Results
The 1601 student (49.0\% response rate) and 735 resident (42.9\% response rate) respondents reported common use of unfiltered sources of drug information such as Google (74.2\%-88.9\%) and Wikipedia (45.2\%-84.5\%). We found that 48\% to 90\% of fourth-year students and residents accurately identified evidence-based prescribing choices. A 10-point higher industry relations index was associated with 15\% lower odds of selecting an evidence-based prescribing choice (odds ratio [{OR}], 0.85; 95\% {CI}, 0.79-0.92) (P {\textless} .001). There was also a significant association between the industry relations index and greater odds of choosing to prescribe brand-name drugs ({OR}, 1.08; 95\% {CI}, 1.00-1.16) (P = .04).Conclusions and Relevance
Among physician trainees, our survey showed an association between positive attitudes toward industry-physician interactions and less knowledge about evidence-based prescribing and greater inclination to recommend brand-name drugs. Policies intended to insulate trainees from pharmaceutical marketing may promote better educational outcomes.},
	journaltitle = {{JAMA} Internal Medicine},
	shortjournal = {{JAMA} Intern Med},
	author = {{Austad KE} and {Avorn J} and {Franklin JM} and {Campbell EG} and {Kesselheim AS}},
	urldate = {2014-06-10},
	date = {2014-06-09}
}

@online{noauthor_publication_nodate,
	title = {Publication quality Tables in R},
	url = {http://ghostblog-arin.rhcloud.com/2014/06/08/publication-quality-tables-in-r/},
	urldate = {2014-06-09}
}

@online{noauthor_about_nodate,
	title = {About {SAS} {\textbar} {SAS}},
	url = {http://www.sas.com/en_us/company-information.html#history},
	urldate = {2014-05-30}
}

@inreference{noauthor_sas_2014,
	title = {{SAS} (software)},
	rights = {Creative Commons Attribution-{ShareAlike} License},
	url = {http://en.wikipedia.org/w/index.php?title=SAS_(software)&oldid=610196791},
	abstract = {{SAS} (Statistical Analysis System) is a software suite developed by {SAS} Institute for advanced analytics, business intelligence, data management, and predictive analytics. It is the largest market-share holder for advanced analytics.},
	booktitle = {Wikipedia, the free encyclopedia},
	urldate = {2014-05-30},
	date = {2014-05-26},
	langid = {english},
	note = {Page Version {ID}: 610196791}
}

@online{noauthor_sas_nodate,
	title = {{SAS} corporate timeline :},
	url = {http://www.wral.com/business/story/9211429/},
	shorttitle = {{SAS} corporate timeline},
	abstract = {A corporate timeline for {SAS}},
	titleaddon = {{WRAL}.com},
	urldate = {2014-05-30}
}

@online{smith_fda:_nodate,
	title = {{FDA}: R {OK} for drug trials},
	url = {http://www.r-bloggers.com/fda-r-ok-for-drug-trials/},
	shorttitle = {{FDA}},
	abstract = {In a poster ({PDF}) presented at the {UseR} 2012 conference, {FDA} biostatistician Jae Brodsky reiterated the {FDA} policy regarding software used to prepare submissions for drug approvals with clinical trials: Sponsors may use R in their submissions. The {FDA} does not endorse or require any particular software to be used for clinical trial submissions, and there are no regulations that restrict the use of open source software (including R) at the {FDA}. Nonetheless, any software (R included) used to prepare data analysis from clinical trials must comply with the various {FDA} regulations and guidances. The R Foundation helpfully provides a...},
	titleaddon = {R-bloggers},
	author = {Smith, David},
	urldate = {2014-05-30},
	keywords = {Government, life sciences, r, revolution}
}

@online{noauthor_home_nodate,
	title = {Home - Paws to Read - {LibGuides} at Prairie Star Middle School},
	url = {http://prairiestarms.libguides.com/pawstoread},
	urldate = {2014-05-24}
}

@article{gu_risk_2014,
	title = {Risk of acute lung injury/acute respiratory distress syndrome in critically ill adult patients with pre-existing diabetes: a meta-analysis},
	volume = {9},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0090426},
	shorttitle = {Risk of acute lung injury/acute respiratory distress syndrome in critically ill adult patients with pre-existing diabetes},
	abstract = {{BACKGROUND}: The impact of pre-existing diabetes on the development of acute lung injury/acute respiratory distress syndrome ({ALI}/{ARDS}) in critically ill patients remains unclear. We performed a meta-analysis of cohort studies to evaluate the risk of {ALI}/{ARDS} in critically ill patients with and without pre-existing diabetes.
{MATERIALS} {AND} {METHODS}: We searched {PubMed} and Embase from the inception to September 2013 for cohort studies assessing the effect of pre-existing diabetes on {ALI}/{ARDS} occurrence. Pooled odds ratio ({OR}) with 95\% confidence interval ({CI}) was calculated using random- or fixed-effect models when appropriate.
{RESULTS}: Seven cohort studies with a total of 12,794 participants and 2,937 cases of pre-existing diabetes, and 2,457 cases of {ALI}/{ARDS} were included in the meta-analysis. A fixed-effects model meta-analysis showed that pre-existing diabetes was associated with a reduced risk of {ALI}/{ARDS} ({OR} 0.66; 95\% {CI}, 0.55-0.80; p{\textless}0.001), with low heterogeneity among the studies (I(2)=18.9\%; p=0.286). However, the asymmetric funnel plot and Egger's test (p=0.007) suggested publication bias may exist.
{CONCLUSIONS}: Our meta-analysis suggests that pre-existing diabetes was associated with a decreased risk of {ALI}/{ARDS} in critically ill adult patients. However, the result should be interpreted with caution because of the potential bias and confounding in the included studies.},
	pages = {e90426},
	number = {2},
	journaltitle = {{PloS} one},
	shortjournal = {{PLoS} {ONE}},
	author = {Gu, Wan-Jie and Wan, You-Dong and Tie, Hong-Tao and Kan, Quan-Cheng and Sun, Tong-Wen},
	date = {2014},
	pmid = {24587357},
	pmcid = {PMC3937384}
}

@article{lancaster_design_2004,
	title = {Design and analysis of pilot studies: recommendations for good practice},
	volume = {10},
	issn = {1365-2753},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j..2002.384.doc.x/abstract},
	doi = {10.1111/j..2002.384.doc.x},
	shorttitle = {Design and analysis of pilot studies},
	abstract = {Pilot studies play an important role in health research, but they can be misused, mistreated and misrepresented. In this paper we focus on pilot studies that are used specifically to plan a randomized controlled trial ({RCT}). Citing examples from the literature, we provide a methodological framework in which to work, and discuss reasons why a pilot study might be undertaken. A well-conducted pilot study, giving a clear list of aims and objectives within a formal framework will encourage methodological rigour, ensure that the work is scientifically valid and publishable, and will lead to higher quality {RCTs}. It will also safeguard against pilot studies being conducted simply because of small numbers of available patients.},
	pages = {307--312},
	number = {2},
	journaltitle = {Journal of Evaluation in Clinical Practice},
	author = {Lancaster, Gillian A. and Dodd, Susanna and Williamson, Paula R.},
	urldate = {2014-05-07},
	date = {2004-05-01},
	langid = {english}
}

@article{arain_what_2010,
	title = {What is a pilot or feasibility study? A review of current practice and editorial policy},
	volume = {10},
	rights = {2010 Arain et al; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/10/67/abstract},
	doi = {10.1186/1471-2288-10-67},
	shorttitle = {What is a pilot or feasibility study?},
	abstract = {In 2004, a review of pilot studies published in seven major medical journals during 2000-01 recommended that the statistical analysis of such studies should be either mainly descriptive or focus on sample size estimation, while results from hypothesis testing must be interpreted with caution. We revisited these journals to see whether the subsequent recommendations have changed the practice of reporting pilot studies. We also conducted a survey to identify the methodological components in registered research studies which are described as 'pilot' or 'feasibility' studies. We extended this survey to grant-awarding bodies and editors of medical journals to discover their policies regarding the function and reporting of pilot studies.
{PMID}: 20637084},
	pages = {67},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Arain, Mubashir and Campbell, Michael J. and Cooper, Cindy L. and Lancaster, Gillian A.},
	urldate = {2014-05-07},
	date = {2010-07-16},
	langid = {english},
	pmid = {20637084}
}

@article{thabane_tutorial_2010,
	title = {A tutorial on pilot studies: the what, why and how},
	volume = {10},
	rights = {2010 Thabane et al; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/10/1/abstract},
	doi = {10.1186/1471-2288-10-1},
	shorttitle = {A tutorial on pilot studies},
	abstract = {Pilot studies for phase {III} trials - which are comparative randomized trials designed to provide preliminary evidence on the clinical efficacy of a drug or intervention - are routinely performed in many clinical areas. Also commonly know as "feasibility" or "vanguard" studies, they are designed to assess the safety of treatment or interventions; to assess recruitment potential; to assess the feasibility of international collaboration or coordination for multicentre trials; to increase clinical experience with the study medication or intervention for the phase {III} trials. They are the best way to assess feasibility of a large, expensive full-scale study, and in fact are an almost essential pre-requisite. Conducting a pilot prior to the main study can enhance the likelihood of success of the main study and potentially help to avoid doomed main studies. The objective of this paper is to provide a detailed examination of the key aspects of pilot studies for phase {III} trials including: 1) the general reasons for conducting a pilot study; 2) the relationships between pilot studies, proof-of-concept studies, and adaptive designs; 3) the challenges of and misconceptions about pilot studies; 4) the criteria for evaluating the success of a pilot study; 5) frequently asked questions about pilot studies; 7) some ethical aspects related to pilot studies; and 8) some suggestions on how to report the results of pilot investigations using the {CONSORT} format.
{PMID}: 20053272},
	pages = {1},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Thabane, Lehana and Ma, Jinhui and Chu, Rong and Cheng, Ji and Ismaila, Afisi and Rios, Lorena P. and Robson, Reid and Thabane, Marroon and Giangregorio, Lora and Goldsmith, Charles H.},
	urldate = {2014-05-07},
	date = {2010-01-06},
	langid = {english},
	pmid = {20053272}
}

@article{shanyinde_questions_2011,
	title = {Questions asked and answered in pilot and feasibility randomized controlled trials},
	volume = {11},
	rights = {2011 Shanyinde et al; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/11/117/abstract},
	doi = {10.1186/1471-2288-11-117},
	abstract = {In the last decade several authors have reviewed the features of pilot and feasibility studies and advised on the issues that should be addressed within them. We extend this literature by examining published pilot/feasibility trials that incorporate random allocation, examining their stated objectives, results presented and conclusions drawn, and comparing drug and non-drug trials.
{PMID}: 21846349},
	pages = {117},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Shanyinde, Milensu and Pickering, Ruth M. and Weatherall, Mark},
	urldate = {2014-05-07},
	date = {2011-08-16},
	langid = {english},
	pmid = {21846349}
}

@article{koletsi_are_2014,
	title = {Are Sample Sizes Clear and Justified in {RCTs} Published in Dental Journals?},
	volume = {9},
	url = {http://dx.doi.org/10.1371/journal.pone.0085949},
	doi = {10.1371/journal.pone.0085949},
	abstract = {Sample size calculations are advocated by the {CONSORT} group to justify sample sizes in randomized controlled trials ({RCTs}). The aim of this study was primarily to evaluate the reporting of sample size calculations, to establish the accuracy of these calculations in dental {RCTs} and to explore potential predictors associated with adequate reporting. Electronic searching was undertaken in eight leading specific and general dental journals. Replication of sample size calculations was undertaken where possible. Assumed variances or odds for control and intervention groups were also compared against those observed. The relationship between parameters including journal type, number of authors, trial design, involvement of methodologist, single-/multi-center study and region and year of publication, and the accuracy of sample size reporting was assessed using univariable and multivariable logistic regression. Of 413 {RCTs} identified, sufficient information to allow replication of sample size calculations was provided in only 121 studies (29.3\%). Recalculations demonstrated an overall median overestimation of sample size of 15.2\% after provisions for losses to follow-up. There was evidence that journal, methodologist involvement ({OR} = 1.97, {CI}: 1.10, 3.53), multi-center settings ({OR} = 1.86, {CI}: 1.01, 3.43) and time since publication ({OR} = 1.24, {CI}: 1.12, 1.38) were significant predictors of adequate description of sample size assumptions. Among journals {JCP} had the highest odds of adequately reporting sufficient data to permit sample size recalculation, followed by {AJODO} and {JDR}, with 61\% ({OR} = 0.39, {CI}: 0.19, 0.80) and 66\% ({OR} = 0.34, {CI}: 0.15, 0.75) lower odds, respectively. Both assumed variances and odds were found to underestimate the observed values. Presentation of sample size calculations in the dental literature is suboptimal; incorrect assumptions may have a bearing on the power of {RCTs}.},
	pages = {e85949},
	number = {1},
	journaltitle = {{PLoS} {ONE}},
	shortjournal = {{PLoS} {ONE}},
	author = {Koletsi, Despina and Fleming, Padhraig S. and Seehra, Jadbinder and Bagos, Pantelis G. and Pandis, Nikolaos},
	urldate = {2014-05-07},
	date = {2014-01-21}
}

@article{julious_sample_2006,
	title = {Sample size calculations for clinical studies allowing for uncertainty about the variance},
	volume = {5},
	issn = {1539-1604},
	abstract = {One of the most important steps in the design of a pharmaceutical clinical trial is the estimation of the sample size. For a superiority trial the sample size formula (to achieve a stated power) would be based on a given clinically meaningful difference and a value for the population variance. The formula is typically used as though this population variance is known whereas in reality it is unknown and is replaced by an estimate with its associated uncertainty. The variance estimate would be derived from an earlier similarly designed study (or an overall estimate from several previous studies) and its precision would depend on its degrees of freedom. This paper provides a solution for the calculation of sample sizes that allows for the imprecision in the estimate of the sample variance and shows how traditional formulae give sample sizes that are too small since they do not allow for this uncertainty with the deficiency being more acute with fewer degrees of freedom. It is recommended that the methodology described in this paper should be used when the sample variance has less than 200 degrees of freedom.},
	pages = {29--37},
	number = {1},
	journaltitle = {Pharmaceutical statistics},
	shortjournal = {Pharm Stat},
	author = {Julious, Steven A and Owen, Roger J},
	date = {2006-03},
	pmid = {17080926}
}

@article{julious_predicting_2007,
	title = {Predicting where future means will lie based on the results of the current trial},
	volume = {28},
	issn = {1551-7144},
	url = {http://www.sciencedirect.com/science/article/pii/S1551714407000080},
	doi = {10.1016/j.cct.2007.01.010},
	abstract = {We show that the proportion of estimates of mean effects in future trials that would be expected to fall within the 95\% confidence interval of a current trial is 83.4\% for a large sample size — assuming that the sample size for a future trial is the same as for the current trial. Using the results from this paper you can predict the results for future trials from those observed in the current trial.},
	pages = {352--357},
	number = {4},
	journaltitle = {Contemporary Clinical Trials},
	shortjournal = {Contemporary Clinical Trials},
	author = {Julious, Steven A. and Campbell, Michael J. and Walters, Stephen J.},
	urldate = {2014-05-07},
	date = {2007-07}
}

@article{julious_personal_2007,
	title = {A personal perspective on the Royal Statistical Society report of the working party on statistical issues in first-in-man studies},
	volume = {6},
	rights = {Copyright © 2007 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/pst.283/abstract},
	doi = {10.1002/pst.283},
	pages = {75--78},
	number = {2},
	journaltitle = {Pharmaceutical Statistics},
	shortjournal = {Pharmaceut. Statist.},
	author = {Julious, Steven},
	urldate = {2014-05-07},
	date = {2007-04-01},
	langid = {english}
}

@article{senn_measurement_2009,
	title = {Measurement in clinical trials: A neglected issue for statisticians?},
	volume = {28},
	rights = {Copyright © 2009 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.3603/abstract},
	doi = {10.1002/sim.3603},
	shorttitle = {Measurement in clinical trials},
	abstract = {Biostatisticians have frequently uncritically accepted the measurements provided by their medical colleagues engaged in clinical research. Such measures often involve considerable loss of information. Particularly, unfortunate is the widespread use of the so-called ‘responder analysis’, which may involve not only a loss of information through dichotomization, but also extravagant and unjustified causal inference regarding individual treatment effects at the patient level, and, increasingly, the use of the so-called number needed to treat scale of measurement. Other problems involve inefficient use of baseline measurements, the use of covariates measured after the start of treatment, the interpretation of titrations and composite response measures. Many of these bad practices are becoming enshrined in the regulatory guidance to the pharmaceutical industry. We consider the losses involved in inappropriate measures and suggest that statisticians should pay more attention to this aspect of their work. Copyright © 2009 John Wiley \& Sons, Ltd.},
	pages = {3189--3209},
	number = {26},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statist. Med.},
	author = {Senn, Stephen and Julious, Steven},
	urldate = {2014-05-07},
	date = {2009-11-20},
	langid = {english}
}

@article{julious_comparison_2011,
	title = {A comparison of methods for sample size estimation for non-inferiority studies with binary outcomes},
	volume = {20},
	issn = {0962-2802, 1477-0334},
	url = {http://smm.sagepub.com/content/20/6/595},
	doi = {10.1177/0962280210378945},
	pages = {595--612},
	number = {6},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {Julious, Steven A. and Owen, Roger J.},
	urldate = {2014-05-07},
	date = {2011-12-01},
	langid = {english},
	pmid = {20889572}
}

@article{pyke_potential_2011,
	title = {The potential for bias in reporting of industry-sponsored clinical trials},
	volume = {10},
	rights = {Copyright © 2010 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/pst.429/abstract},
	doi = {10.1002/pst.429},
	abstract = {Concerns about potentially misleading reporting of pharmaceutical industry research have surfaced many times. The potential for duality (and thereby conflict) of interest is only too clear when you consider the sums of money required for the discovery, development and commercialization of new medicines. As the ability of major, mid-size and small pharmaceutical companies to innovate has waned, as evidenced by the seemingly relentless decline in the numbers of new medicines approved by Food and Drug Administration and European Medicines Agency year-on-year, not only has the cost per new approved medicine risen: so too has the public and media concern about the extent to which the pharmaceutical industry is open and honest about the efficacy, safety and quality of the drugs we manufacture and sell. In 2005 an Editorial in Journal of the American Medical Association made clear that, so great was their concern about misleading reporting of industry-sponsored studies, henceforth no article would be published that was not also guaranteed by independent statistical analysis. We examine the precursors to this Editorial, as well as its immediate and lasting effects for statisticians, for the manner in which statistical analysis is carried out, and for the industry more generally. Copyright © 2010 John Wiley \& Sons, Ltd.},
	pages = {74--79},
	number = {1},
	journaltitle = {Pharmaceutical Statistics},
	shortjournal = {Pharmaceut. Statist.},
	author = {Pyke, Stephen and Julious, Steven A. and Day, Simon and O'Kelly, Michael and Todd, Susan and Matcham, James and Seldrup, Jorgen},
	urldate = {2014-05-07},
	date = {2011-01-01},
	langid = {english}
}

@article{julious_best_2011,
	title = {Best practice for statisticians in industry sponsored trials},
	volume = {342},
	issn = {0959-8138, 1468-5833},
	url = {http://www.bmj.com/content/342/bmj.d1636?view=long&pmid=21406524},
	doi = {10.1136/bmj.d1636},
	pages = {d1636--d1636},
	issue = {mar15 2},
	journaltitle = {{BMJ}},
	author = {Julious, S. A. and Pyke, S. and Hughes, S.},
	urldate = {2014-05-07},
	date = {2011-03-15},
	langid = {english}
}

@article{julious_seven_2012,
	title = {Seven useful designs},
	volume = {11},
	rights = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/pst.485/abstract},
	doi = {10.1002/pst.485},
	abstract = {When designing a clinical trial there is a need to design a study that achieves the objectives of a trial both efficiently and with minimum resources. This paper describes seven trial designs that could possibly be used in clinical development and highlights how a design although not optimal for an individual study may be optimal for a wider clinical program. Copyright © 2011 John Wiley \& Sons, Ltd.},
	pages = {24--31},
	number = {1},
	journaltitle = {Pharmaceutical Statistics},
	shortjournal = {Pharmaceut. Statist.},
	author = {Julious, Steven A.},
	urldate = {2014-05-07},
	date = {2012-01-01},
	langid = {english}
}

@article{julious_abc_2011,
	title = {The {ABC} of non-inferiority margin setting from indirect comparisons},
	volume = {10},
	rights = {Copyright © 2011 John Wiley \& Sons, Ltd.},
	issn = {1539-1612},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/pst.517/abstract},
	doi = {10.1002/pst.517},
	abstract = {In a non-inferiority trial to assess a new investigative treatment, there may need to be consideration of an indirect comparison with placebo using the active control in the current trial. We can, therefore, use the fact that there is a common active control in the comparisons of the investigative treatment and placebo. In analysing a non-inferiority trial, the {ABC} of: Assay sensitivity, Bias minimisation and Constancy assumption needs to be considered. It is highlighted how the {ABC} assumptions can potentially fail when there is placebo creep or a patient population shift. In this situation, the belief about the placebo response expressed in terms of a prior probability in Bayesian formulation could be used with the observed treatment effects to set the non-inferiority limit. Copyright © 2011 John Wiley \& Sons, Ltd.},
	pages = {448--453},
	number = {5},
	journaltitle = {Pharmaceutical Statistics},
	shortjournal = {Pharmaceut. Statist.},
	author = {Julious, Steven A.},
	urldate = {2014-05-07},
	date = {2011-09-01},
	langid = {english}
}

@article{julious_tutorial_2012,
	title = {Tutorial in biostatistics: sample sizes for parallel group clinical trials with binary data},
	volume = {31},
	rights = {Copyright © 2012 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/sim.5381/abstract},
	doi = {10.1002/sim.5381},
	shorttitle = {Tutorial in biostatistics},
	abstract = {This article gives an overview of sample size calculations for a single response and a comparison of two responses in a parallel group trial where the outcome is binary. Sample size derivation is given for trials where the objective is to demonstrate: superiority, equivalence, non-inferiority and estimation to a given precision. For each type of trial the null and alternative hypotheses are described and how the impact these have on the sample size calculations. For each type of trial the calculations are highlighted through worked examples. Sample size tables for the different types of trials and worked examples are given to assist in future calculations. Copyright © 2012 John Wiley \& Sons, Ltd.},
	pages = {2904--2936},
	number = {24},
	journaltitle = {Statistics in Medicine},
	shortjournal = {Statist. Med.},
	author = {Julious, Steven A. and Campbell, Michael J.},
	urldate = {2014-05-07},
	date = {2012-10-30},
	langid = {english}
}

@article{julious_estimating_2013,
	title = {Estimating effect sizes for health related quality of life outcomes},
	issn = {0962-2802, 1477-0334},
	url = {http://smm.sagepub.com/content/early/2013/02/14/0962280213476379},
	doi = {10.1177/0962280213476379},
	abstract = {To enable an assessment of the costs and benefits of a new health technology one should use a range of outcome measures, including medical, psychosocial and economic. Therefore, unless a patient-reported outcome as well as clinical outcome is assessed in a study, the effect of a health technology on the patient will remain unknown as two therapies may have similar clinical consequences but different impacts upon the quality of the life of the patients. An important issue when designing a study with a new patient-reported outcome is the quantification of an effect size. Through a case study we highlight how simple calculations can enable the estimation of the effect sizes if there is information on established outcomes. This is done by mapping changes on the new scale to clinically relevant and important changes on established scales. We recommend the approaches described in this paper be considered for the quantification of important treatment effects when designing a clinical trial with a new patient-reported outcome measure.},
	pages = {0962280213476379},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {Julious, Steven A. and Walters, Stephen J.},
	urldate = {2014-05-07},
	date = {2013-02-19},
	langid = {english},
	pmid = {23427222}
}

@article{sully_reinvestigation_2013,
	title = {A reinvestigation of recruitment to randomised, controlled, multicenter trials: a review of trials funded by two {UK} funding agencies},
	volume = {14},
	rights = {2013 Sully et al.; licensee {BioMed} Central Ltd.},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/14/1/166/abstract},
	doi = {10.1186/1745-6215-14-166},
	shorttitle = {A reinvestigation of recruitment to randomised, controlled, multicenter trials},
	abstract = {Randomised controlled trials ({RCTs}) are the gold standard assessment for health technologies. A key aspect of the design of any clinical trial is the target sample size. However, many publicly-funded trials fail to reach their target sample size. This study seeks to assess the current state of recruitment success and grant extensions in trials funded by the Health Technology Assessment ({HTA}) program and the {UK} Medical Research Council ({MRC}).
{PMID}: 23758961},
	pages = {166},
	number = {1},
	journaltitle = {Trials},
	author = {Sully, Ben G. O. and Julious, Steven A. and Nicholl, Jon},
	urldate = {2014-05-07},
	date = {2013-06-09},
	langid = {english},
	pmid = {23758961}
}

@article{billingham_audit_2013,
	title = {An audit of sample sizes for pilot and feasibility trials being undertaken in the United Kingdom registered in the United Kingdom Clinical Research Network database},
	volume = {13},
	rights = {2013 Billingham et al.; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/13/104/abstract},
	doi = {10.1186/1471-2288-13-104},
	abstract = {There is little published guidance as to the sample size required for a pilot or feasibility trial despite the fact that a sample size justification is a key element in the design of a trial. A sample size justification should give the minimum number of participants needed in order to meet the objectives of the trial. This paper seeks to describe the target sample sizes set for pilot and feasibility randomised controlled trials, currently running within the United Kingdom.
{PMID}: 23961782},
	pages = {104},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Billingham, Sophie {AM} and Whitehead, Amy L. and Julious, Steven A.},
	urldate = {2014-05-07},
	date = {2013-08-20},
	langid = {english},
	pmid = {23961782}
}

@article{sully_investigation_2014,
	title = {An investigation of the impact of futility analysis in publicly funded},
	volume = {15},
	rights = {2014 Sully et al.; licensee {BioMed} Central Ltd.},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/15/1/61/abstract},
	doi = {10.1186/1745-6215-15-61},
	abstract = {Publicly funded trials regularly fail to recruit their target sample size or find a significant positive result. Adaptive clinical trials which may partly mediate against the problems are not often applied. In this paper we investigate the potential of a form of adaption in a clinical trial - a futility analysis - to see if it has potential to improve publicly funded trials.
{PMID}: 24533447},
	pages = {61},
	number = {1},
	journaltitle = {Trials},
	author = {Sully, Benjamin {GO} and Julious, Steven A. and Nicholl, Jon},
	urldate = {2014-05-07},
	date = {2014-02-17},
	langid = {english},
	pmid = {24533447}
}

@article{lee_statistical_2014,
	title = {The statistical interpretation of pilot trials: should significance thresholds be reconsidered?},
	volume = {14},
	rights = {2014 Lee et al.; licensee {BioMed} Central Ltd.},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/14/41/abstract},
	doi = {10.1186/1471-2288-14-41},
	shorttitle = {The statistical interpretation of pilot trials},
	abstract = {In an evaluation of a new health technology, a pilot trial may be undertaken prior to a trial that makes a definitive assessment of benefit. The objective of pilot studies is to provide sufficient evidence that a larger definitive trial can be undertaken and, at times, to provide a preliminary assessment of benefit.
{PMID}: 24650044},
	pages = {41},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Lee, Ellen C. and Whitehead, Amy L. and Jacques, Richard M. and Julious, Steven A.},
	urldate = {2014-05-07},
	date = {2014-03-20},
	langid = {english},
	pmid = {24650044}
}

@collection{the_cochrane_collaboration_cochrane_1996,
	location = {Chichester, {UK}},
	title = {Cochrane Database of Systematic Reviews: Reviews},
	url = {http://summaries.cochrane.org/MR000034/comparing-effect-estimates-of-randomized-controlled-trials-and-observational-studies},
	shorttitle = {Cochrane Database of Systematic Reviews},
	publisher = {John Wiley \& Sons, Ltd},
	editor = {{The Cochrane Collaboration}},
	urldate = {2014-05-07},
	date = {1996-09-01},
	langid = {english}
}

@article{pigott_review_2001,
	title = {A Review of Methods for Missing Data},
	volume = {7},
	doi = {10.1076/edre.7.4.353.8937},
	abstract = {This paper reviews methods for handling missing data in a research study. Many researchers use ad hoc methods such as complete case analysis, available case analysis (pairwise deletion), or single-value imputation. Though these methods are easily implemented, they require assumptions about the data that rarely hold in practice. Model-based methods such as maximum likelihood using the {EM} algorithm and multiple imputation hold more promise for dealing with difficulties caused by missing data. While model-based methods require specialized computer programs and assumptions about the nature of the missing data, these methods are appropriate for a wider range of situations than the more commonly used ad hoc methods. The paper provides an illustration of the methods using data from an intervention study designed to increase students' ability to control their asthma symptoms.},
	pages = {353--383},
	number = {4},
	journaltitle = {Educational Research and Evaluation},
	shortjournal = {Educational Research and Evaluation},
	author = {Pigott, Therese D.},
	date = {2001-12-01}
}