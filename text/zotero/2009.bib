
@article{moses_academic_2001,
	title = {Academic Relationships With Industry: A New Model for Biomedical Research},
	volume = {285},
	url = {http://jama.ama-assn.org/cgi/content/extract/285/7/933},
	shorttitle = {Academic Relationships With Industry},
	abstract = {Description: This article proposes several general principles for managing the increasingly complex financial ties between academic research institutions and industry.},
	pages = {933--935},
	journaltitle = {Journal of the American Medical Association},
	author = {Moses, Hamilton and Martin, Joseph B.},
	urldate = {2009-03-09},
	date = {2001-02-21},
	keywords = {Conflict of interest}
}

@article{nathan_academic_2002,
	title = {Academic Freedom in Clinical Research},
	volume = {347},
	url = {http://content.nejm.org/cgi/content/extract/347/17/1368},
	abstract = {Description: This article summarizes the Nancy Oliveri case. Dr. Olivieri was a researcher who was determined to present information about safety problems with a drug she was studying, in violation of a confidentiality agreement with the drug company that sponsored the research. This case illustrates the need to avoid agreements with drug companies that allow those companies to completely bar publication of unfavorable results.},
	pages = {1368--1371},
	journaltitle = {New England Journal of Medicine},
	author = {Nathan, {DG} and Weatheral, {DJ}},
	urldate = {2009-03-09},
	date = {2002-10-24},
	keywords = {Conflict of interest}
}

@article{krogsboll_spontaneous_2009,
	title = {Spontaneous improvement in randomised clinical trials: meta-analysis of three-armed trials comparing no treatment, placebo and active intervention},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/1},
	doi = {10.1186/1471-2288-9-1},
	shorttitle = {Spontaneous improvement in randomised clinical trials},
	abstract = {Abstract: "{BACKGROUND}: It can be challenging for patients and clinicians to properly interpret a change in the clinical condition after a treatment has been given. It is not known to which extent spontaneous improvement, effect of placebo and effect of active interventions contribute to the observed change from baseline, and we aimed at quantifying these contributions. {METHODS}: Systematic review and meta-analysis, based on a Cochrane review of the effect of placebo interventions for all clinical conditions. We selected all trials that had randomised the patients to three arms: no treatment, placebo and active intervention, and that had used an outcome that was measured on a continuous scale or on a ranking scale. Clinical conditions that had been studied in less than three trials were excluded. {RESULTS}: We analysed 37 trials (2900 patients) that covered 8 clinical conditions. The active interventions were psychological in 17 trials, physical in 15 trials, and pharmacological in 5 trials. Overall, across all conditions and interventions, there was a statistically significant change from baseline in all three arms. The standardized mean difference ({SMD}) for change from baseline was -0.24 (95\% confidence interval -0.36 to -0.12) for no treatment, -0.44 (-0.61 to -0.28) for placebo, and -1.01 (-1.16 to -0.86) for active treatment. Thus, on average, the relative contributions of spontaneous improvement and of placebo to that of the active interventions were 24\% and 20\%, respectively, but with some uncertainty, as indicated by the confidence intervals for the three {SMDs}. The conditions that had the most pronounced spontaneous improvement were nausea (45\%), smoking (40\%), depression (35\%), phobia (34\%) and acute pain (25\%). {CONCLUSION}: Spontaneous improvement and effect of placebo contributed importantly to the observed treatment effect in actively treated patients, but the relative importance of these factors differed according to clinical condition and intervention.},
	pages = {1},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Krogsboll, Lasse and Hrobjartsson, Asbjorn and Gotzsche, Peter},
	urldate = {2009-02-23},
	date = {2009},
	keywords = {Placebo controlled trials}
}

@article{pogue_should_2009,
	title = {Should You Worry About Data Rot?},
	issn = {0362-4331},
	url = {http://www.nytimes.com/2009/03/26/technology/personaltech/26pogue-email.html},
	abstract = {Excerpt: "Data rot refers mainly to problems with the medium on which information is stored. Over time, things like temperature, humidity, exposure to light, being stored not-very-good locations like moldy basements, make this information very difficult to read. The second aspect of data rot is actually finding the machines to read them. And that is a real problem. If you think of the 8-track tape player, for example, basically the only way you can find 8-track cartridges is in a flea market or a garage sale."},
	journaltitle = {The New York Times},
	author = {Pogue, David},
	urldate = {2009-03-30},
	date = {2009-03-26},
	keywords = {Data management}
}

@article{tang_integrating_2009,
	title = {Integrating post-genomic approaches as a strategy to advance our understanding of health and disease},
	rights = {Copyright 2009 {BioMed} Central Ltd},
	url = {http://genomemedicine.com/content/1/3/35/},
	author = {Tang, Jing and Tan, Chong and Oresic, Matej and Vidal-Puig, Antonio},
	urldate = {2009-04-22},
	date = {2009-03-30},
	note = {Abstract Following the publication of the complete human genomic sequence, the post-genomic era is driven by the need to extract useful information from genomic data. Genomics, transcriptomics, proteomics, metabolomics, epidemiological data and microbial data provide different angles to our understanding of gene-environment interactions and the determinants of disease and health. Our goal and our challenge are to integrate these very different types of data and perspectives of disease into a global model suitable for dissecting the mechanisms of disease and for predicting novel therapeutic strategies. This review aims to highlight the need for and problems with complex data integration, and proposes a framework for data integration. While there are many obstacles to overcome, biological models based upon multiple datasets will probably become the basis that drives future biomedical research.}
}

@online{stephen_jay_gould_cancerguide:_nodate,
	title = {{CancerGuide}: The Median Isn't the Message, Prefatory Note by Steve Dunn},
	url = {http://cancerguide.org/median_not_msg.html},
	abstract = {Prefatory Note by Steve Dunn: "Stephen Jay Gould was an influential evolutionary biologist who taught at Harvard University. He was the author of at least ten popular books on evolution, and science, including, among others, The Flamingo's Smile, The Mismeasure of Man, Wonderful Life, and Full House. As far as I'm concerned, Gould's The Median Isn't the Message is the wisest, most humane thing ever written about cancer and statistics. It is the antidote both to those who say that, "the statistics don't matter," and to those who have the unfortunate habit of pronouncing death sentences on patients who face a difficult prognosis. Anyone who researches the medical literature will confront the statistics for their disease. Anyone who reads this will be armed with reason and with hope. The Median Isn't the Message is reproduced here by permission of the author."},
	author = {{Stephen Jay Gould}},
	urldate = {2009-11-19}
}

@article{wolfe_if_2002,
	title = {If we're so different, why do we keep overlapping? When 1 plus 1 doesn't make 2},
	volume = {166},
	url = {http://www.cmaj.ca/cgi/content/full/166/1/65},
	shorttitle = {If we're so different, why do we keep overlapping?},
	abstract = {Excerpt: "In the last decade, guidelines for the presentation of statistical results in medical journals have emphasized confidence intervals ({CIs}) as an adjunct to, or even a replacement for, statistical tests and p values. Because of the intimate links between the 2 concepts, authors now use statements like "the 95\% {CI} overlaps 0" where they would formerly have stated "the difference is not statistically significant at the 5\% level." Although this interchangeability is technically correct in 1-sample situations, it does not carry over fully to comparisons involving 2 samples. A frequently encountered misconception is that if 2 independent 95\% {CIs} overlap each other, as they do in Fig. 1, then a statistical test of the difference will not be statistically significant at the 5\% level."},
	pages = {65--66},
	number = {1},
	journaltitle = {{CMAJ}},
	author = {Wolfe, Rory and Hanley, James},
	urldate = {2009-01-05},
	date = {2002-01-08},
	keywords = {Confidence intervals}
}

@article{goodyear_unintended_nodate,
	title = {Unintended results of research},
	volume = {6},
	url = {http://www.harmreductionjournal.com/content/6/1/5/comments#336610},
	doi = {10.1186/1477-7517-6-5},
	abstract = {Description: "This letter is a response to an article about the epidemic of {HIV}/{AIDS} in Vancouver, British Columbia. The letter writer is critical of the article and claims that research subjects in the study have been 'harmed, stigmatised and quite probably subjected to increased levels of violence'. He goes on to argue that the subjects did not provide informed consent. 'When women gave permission to be tested for {HIV} they did not anticipate that it would be used to stigmatise them, and presumably when they gave consent for it to be used for research they were not informed of that.'"},
	number = {5},
	journaltitle = {Harm Reduction Journal},
	author = {Goodyear, Michael},
	keywords = {Ethics in research}
}

@article{chan_empirical_2004,
	title = {Empirical Evidence for Selective Reporting of Outcomes in Randomized Trials: Comparison of Protocols to Published Articles},
	volume = {291},
	url = {http://jama.ama-assn.org/cgi/content/abstract/291/20/2457},
	doi = {10.1001/jama.291.20.2457},
	shorttitle = {Empirical Evidence for Selective Reporting of Outcomes in Randomized Trials},
	abstract = {Abstract: "Context: Selective reporting of outcomes within published studies based on the nature or direction of their results has been widely suspected, but direct evidence of such bias is currently limited to case reports. Objective To study empirically the extent and nature of outcome reporting bias in a cohort of randomized trials. Design: Cohort study using protocols and published reports of randomized trials approved by the Scientific-Ethical Committees for Copenhagen and Frederiksberg, Denmark, in 1994-1995. The number and characteristics of reported and unreported trial outcomes were recorded from protocols, journal articles, and a survey of trialists. An outcome was considered incompletely reported if insufficient data were presented in the published articles for meta-analysis. Odds ratios relating the completeness of outcome reporting to statistical significance were calculated for each trial and then pooled to provide an overall estimate of bias. Protocols and published articles were also compared to identify discrepancies in primary outcomes. Main Outcome Measures: Completeness of reporting of efficacy and harm outcomes and of statistically significant vs nonsignificant outcomes; consistency between primary outcomes defined in the most recent protocols and those defined in published articles. Results: One hundred two trials with 122 published journal articles and 3736 outcomes were identified. Overall, 50\% of efficacy and 65\% of harm outcomes per trial were incompletely reported. Statistically significant outcomes had a higher odds of being fully reported compared with nonsignificant outcomes for both efficacy (pooled odds ratio, 2.4; 95\% confidence interval [{CI}], 1.4-4.0) and harm (pooled odds ratio, 4.7; 95\% {CI}, 1.8-12.0) data. In comparing published articles with protocols, 62\% of trials had at least 1 primary outcome that was changed, introduced, or omitted. Eighty-six percent of survey responders (42/49) denied the existence of unreported outcomes despite clear evidence to the contrary. Conclusions: The reporting of trial outcomes is not only frequently incomplete but also biased and inconsistent with protocols. Published articles, as well as reviews that incorporate them, may therefore be unreliable and overestimate the benefits of an intervention. To ensure transparency, planned trials should be registered and protocols should be made publicly available prior to trial completion."},
	pages = {2457--2465},
	number = {20},
	journaltitle = {{JAMA}},
	author = {Chan, An-Wen and Hrobjartsson, Asbjorn and Haahr, Mette T. and Gotzsche, Peter C. and Altman, Douglas G.},
	urldate = {2009-02-25},
	date = {2004-05-26},
	keywords = {{FraudInResearch}}
}

@article{ioannidis_why_2005,
	title = {Why Most Published Research Findings Are False},
	volume = {2},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1182327},
	doi = {10.1371/journal.pmed.0020124},
	abstract = {Abstract: "There is increasing concern that most current published research findings are false. The probability that a research claim is true may depend on study power and bias, the number of other studies on the same question, and, importantly, the ratio of true to no relationships among the relationships probed in each scientific field. In this framework, a research finding is less likely to be true when the studies conducted in a field are smaller; when effect sizes are smaller; when there is a greater number and lesser preselection of tested relationships; where there is greater flexibility in designs, definitions, outcomes, and analytical modes; when there is greater financial and other interest and prejudice; and when more teams are involved in a scientific field in chase of statistical significance. Simulations show that for most study designs and settings, it is more likely for a research claim to be false than true. Moreover, for many current scientific fields, claimed research findings may often be simply accurate measures of the prevailing bias. In this essay, I discuss the implications of these problems for the conduct and interpretation of research."},
	pages = {e124},
	number = {8},
	journaltitle = {{PLoS} Medicine},
	shortjournal = {{PLoS} Med.},
	author = {Ioannidis, John P. A.},
	urldate = {2009-01-06},
	date = {2005-08},
	note = {{PMC}1182327}
}

@article{abrahamowicz_bias_2004,
	title = {Bias due to Aggregation of Individual Covariates in the Cox Regression Model},
	volume = {160},
	url = {http://aje.oxfordjournals.org/cgi/content/abstract/160/7/696},
	doi = {10.1093/aje/kwh266},
	abstract = {Abstract: "The impact of covariate aggregation, well studied in relation to linear regression, is less clear in the Cox model. In this paper, the authors use real-life epidemiologic data to illustrate how aggregating individual covariate values may lead to important underestimation of the exposure effect. The issue is then systematically assessed through simulations, with six alternative covariate representations. It is shown that aggregation of important predictors results in a systematic bias toward the null in the Cox model estimate of the exposure effect, even if exposure and predictors are not correlated. The underestimation bias increases with increasing strength of the covariate effect and decreasing censoring and, for a strong predictor and moderate censoring, may exceed 20\%, with less than 80\% coverage of the 95\% confidence interval. However, covariate aggregation always induces smaller bias than covariate omission does, even if the two phenomena are shown to be related. The impact of covariate aggregation, but not omission, is independent of the covariate-exposure correlation. Simulations involving time-dependent aggregates demonstrate that bias results from failure of the baseline covariate mean to account for nonrandom changes over time in the risk sets and suggest a simple approach that may reduce the bias if individual data are available but have to be aggregated."},
	pages = {696--706},
	number = {7},
	journaltitle = {Am. J. Epidemiol.},
	author = {Abrahamowicz, Michal and du Berger, Roxane and Krewski, Daniel and Burnett, Richard and Bartlett, Gillian and Tamblyn, Robyn M. and Leffondre, Karen},
	urldate = {2009-01-19},
	date = {2004-10-01},
	keywords = {Covariate adjustment, Survival analysis, a02}
}

@online{carson_reliability_nodate,
	title = {Reliability Analysis: Statnotes, from North Carolina State University, Public Administration Program},
	url = {http://faculty.chass.ncsu.edu/garson/PA765/reliab.htm},
	abstract = {Excerpt: "Researchers must demonstrate instruments are reliable since without reliability, research results using the instrument are not replicable, and replicability is fundamental to the scientific method. Reliability is the correlation of an item, scale, or instrument with a hypothetical one which truly measures what it is supposed to. Since the true instrument is not available, reliability is estimated in one of four ways: 1. Internal consistency: Estimation based on the correlation among the variables comprising the set (typically, Cronbach's alpha). 2. Split-half reliability: Estimation based on the correlation of two equivalent forms of the scale (typically, the Spearman-Brown coefficient). 3. Test-retest reliability: Estimation based on the correlation between two (or more) administrations of the same item, scale, or instrument for different times, locations, or populations, when the two administrations do not differ on other relevant variables (typically, the Spearman Brown coefficient). 4. Inter-rater reliability: Estimation based on the correlation of scores between/among two or more raters who rate the same item, scale, or instrument (typically, intraclass correlation, of which there are six types discussed below). These four reliability estimation methods are not necessarily mutually exclusive, nor need they lead to the same results. All reliability coefficients are forms of correlation coefficients, but there are multiple types discussed below, representing different meanings of reliability and more than one might be used in single research setting. "},
	author = {Carson, G. David},
	urldate = {2010-01-01},
	keywords = {Measuring agreement, a02}
}

@article{deeks_issues_2002,
	title = {Issues in the selection of a summary statistic for meta-analysis of clinical trials with binary outcomes},
	volume = {21},
	url = {http://dx.doi.org/10.1002/sim.1188},
	doi = {10.1002/sim.1188},
	abstract = {Abstract: "Meta-analysis of binary data involves the computation of a weighted average of summary statistics calculated for each trial. The selection of the appropriate summary statistic is a subject of debate due to conflicts in the relative importance of mathematical properties and the ability to intuitively interpret results. This paper explores the process of identifying a summary statistic most likely to be consistent across trials when there is variation in control group event rates. Four summary statistics are considered: odds ratios ({OR}); risk differences ({RD}) and risk ratios of beneficial ({RR}(B)); and harmful outcomes ({RR}(H)). Each summary statistic corresponds to a different pattern of predicted absolute benefit of treatment with variation in baseline risk, the greatest difference in patterns of prediction being between {RR}(B) and {RR}(H). Selection of a summary statistic solely based on identification of the best-fitting model by comparing tests of heterogeneity is problematic, principally due to low numbers of trials. It is proposed that choice of a summary statistic should be guided by both empirical evidence and clinically informed debate as to which model is likely to be closest to the expected pattern of treatment benefit across baseline risks. Empirical investigations comparing the four summary statistics on a sample of 551 systematic reviews provide evidence that the {RR} and {OR} models are on average more consistent than {RD}, there being no difference on average between {RR} and {OR}. From a second sample of 114 meta-analyses evidence indicates that for interventions aimed at preventing an undesirable event, greatest absolute benefits are observed in trials with the highest baseline event rates, corresponding to the model of constant {RR}(H). The appropriate selection for a particular meta-analysis may depend on understanding reasons for variation in control group event rates; in some situations uncertainty about the choice of summary statistic will remain. Copyright © 2002 John Wiley \& Sons, Ltd."},
	pages = {1575--1600},
	number = {11},
	journaltitle = {Statistics in Medicine},
	author = {Deeks, Jonathan J.},
	urldate = {2009-12-18},
	date = {2002},
	keywords = {Systematic overviews, a02}
}

@article{ocathain_structural_2009,
	title = {Structural issues affecting mixed methods studies in health research: a qualitative study},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/82},
	doi = {10.1186/1471-2288-9-82},
	shorttitle = {Structural issues affecting mixed methods studies in health research},
	abstract = {Abstract: "{BACKGROUND}: Health researchers undertake studies which combine qualitative and quantitative methods. Little attention has been paid to the structural issues affecting this mixed methods approach. We explored the facilitators and barriers to undertaking mixed methods studies in health research. {METHODS}: Face-to-face semi-structured interviews with 20 researchers experienced in mixed methods research in health in the United Kingdom. {RESULTS}: Structural facilitators for undertaking mixed methods studies included a perception that funding bodies promoted this approach, and the multidisciplinary constituency of some university departments. Structural barriers to exploiting the potential of these studies included a lack of education and training in mixed methods research, and a lack of templates for reporting mixed methods articles in peer-reviewed journals. The 'hierarchy of evidence' relating to effectiveness studies in health care research, with the randomised controlled trial as the gold standard, appeared to pervade the health research infrastructure. Thus integration of data and findings from qualitative and quantitative components of mixed methods studies, and dissemination of integrated outputs, tended to occur through serendipity and effort, further highlighting the presence of structural constraints. Researchers are agents who may also support current structures - journal reviewers and editors, and directors of postgraduate training courses - and thus have the ability to improve the structural support for exploiting the potential of mixed methods research. {CONCLUSIONS}: The environment for health research in the {UK} appears to be conducive to mixed methods research but not to exploiting the potential of this approach. Structural change, as well as change in researcher behaviour, will be necessary if researchers are to fully exploit the potential of using mixed methods research."},
	pages = {82},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {O'Cathain, Alicia and Nicholl, Jon and Murphy, Elizabeth},
	urldate = {2009-12-15},
	date = {2009},
	keywords = {Qualitative data, a02}
}

@article{schluter_multivariate_2009,
	title = {A multivariate hierarchical Bayesian approach to measuring agreement in repeated measurement method comparison studies},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/6},
	doi = {10.1186/1471-2288-9-6},
	abstract = {Description: This paper considers a Bayesian extension to the Bland-Altman chart to incorporate repeatability assessment as well as agreement among three or more methods. Abstract: "{BACKGROUND}: Assessing agreement in method comparison studies depends on two fundamentally important components; validity (the between method agreement) and reproducibility (the within method agreement). The Bland-Altman limits of agreement technique is one of the favoured approaches in medical literature for assessing between method validity. However, few researchers have adopted this approach for the assessment of both validity and reproducibility. This may be partly due to a lack of a flexible, easily implemented and readily available statistical machinery to analyse repeated measurement method comparison data. {METHODS}: Adopting the Bland-Altman framework, but using Bayesian methods, we present this statistical machinery. Two multivariate hierarchical Bayesian models are advocated, one which assumes that the underlying values for subjects remain static (exchangeable replicates) and one which assumes that the underlying values can change between repeated measurements (non-exchangeable replicates). {RESULTS}: We illustrate the salient advantages of these models using two separate datasets that have been previously analysed and presented; (i) assuming static underlying values analysed using both multivariate hierarchical Bayesian models, and (ii) assuming each subject's underlying value is continually changing quantity and analysed using the non-exchangeable replicate multivariate hierarchical Bayesian model. {CONCLUSIONS}: These easily implemented models allow for full parameter uncertainty, simultaneous method comparison, handle unbalanced or missing data, and provide estimates and credible regions for all the parameters of interest. Computer code for the analyses in also presented, provided in the freely available and currently cost free software package {WinBUGS}."},
	pages = {6},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Schluter, Philip},
	urldate = {2009-01-30},
	date = {2009},
	keywords = {Measuring agreement, a02}
}

@article{kraemer_correlation_2006,
	title = {Correlation coefficients in medical research: from product moment correlation to the odds ratio},
	volume = {15},
	issn = {0962-2802},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17260922},
	shorttitle = {Correlation coefficients in medical research},
	abstract = {Description: There are several measures of agreement (such as the phi coefficient, the point biserial correlation, and the tetrachoric correlation) that are used to show relationships when one or both variables are binary. This paper shows the interrelationships and the interpretation of these correlations and relates them to other measures not traditionally thought of as measures of correlation, such as the odds ratio. Abstract: "{OBJECTIVE}: Presentation of effect sizes that can be interpreted in terms of clinical or practical significance is currently urged whenever statistical significance (a 'p-value') is reported in research journals. However, which effect size and how to interpret it are not yet clearly delineated. The present focus is on effect sizes indicating strength of correlation, that is, effect sizes that describe the strength of monotonic association between two random variables X and Y in a population. {METHODS}: A logical structure of measures of association is traced, showing the interrelationships among the many measures of association. Advantages and disadvantages of each are discussed. {CONCLUSIONS}: Suggestions are made for the future use of measures of association in research to facilitate considerations of clinical significance, emphasizing distribution-free effect sizes such as the Spearman correlation coefficient and Kendall's coefficient of concordance for ordinal versus ordinal associations, weighted and intraclass kappa for binary versus binary associations and risk difference ({RD}) for binary versus ordinal association."},
	pages = {525--45},
	number = {6},
	journaltitle = {Statistical Methods in Medical Research},
	shortjournal = {Stat Methods Med Res},
	author = {Kraemer, Helena Chmura},
	urldate = {2009-01-30},
	date = {2006-12},
	pmid = {17260922},
	keywords = {Measuring agreement, a02}
}

@article{griffin_knee-heel_1999,
	title = {Knee-heel length measurement in healthy preterm infants},
	volume = {81},
	url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1720961&amp;rendertype=abstract},
	abstract = {Description: This article provides an illustrative example of how to use the coefficient of variation to measure agreement on a continuous trait among several raters. Abstract: "{AIM}: To examine the reproducibility of crown-heel length measurement; the precision and reproducibility of knee-heel length measurement; and the association between the two in healthy preterm infants. {METHODS}: Paired crown-heel and knee-heel lengths were measured on 172 occasions by three observers in 43 preterm infants between 205and 458 days of postconceptional age. {RESULTS}: Crown-heel length ({CHL}) measurement was highly reproducible, with a coefficient of variation ({CV}) of 0.41\%. Knee-heel length ({KHL}) measurement was relatively precise ({CV} 0.78\%), but less reproducible (intra-observer {CV} 1.77\%, intra-observer {CV} 2.11\%), especially in larger infants. The association between {KHL} and {CHL} was not consistent and varied with age. {KHL} was a poor predictor of {CHL}, with a 95\% predictive interval of ± 27.5mm. {CONCLUSIONS}: {KHL} was less reproducible than {CHL}, especially in larger infants, and a poor predictor of {CHL}."},
	pages = {F50--F55},
	number = {1},
	journaltitle = {Archives of Disease in Childhood. Fetal and Neonatal Edition},
	shortjournal = {Arch Dis Child Fetal Neonatal Ed.},
	author = {Griffin, I. and Pang, N. and Perring, J. and Cooke, R.},
	urldate = {2009-01-30},
	date = {1999-07},
	note = {{PMC}1720961},
	keywords = {Measuring agreement, a02}
}

@article{pokrzywinski_development_2009,
	title = {Development and psychometric assessment of the {COPD} and Asthma Sleep Impact Scale ({CASIS})},
	volume = {7},
	issn = {1477-7525},
	url = {http://www.hqlo.com/content/7/1/98},
	doi = {10.1186/1477-7525-7-98},
	abstract = {Abstract: "{BACKGROUND}: Patients with respiratory disease experience disturbed sleep, but there is no widely accepted measure of sleep impairment due to respiratory disease. We developed and evaluated the psychometric performance of a patient-reported measure to assess the impact on sleep due to respiratory disease, the {COPD} and Asthma Sleep Impact Scale ({CASIS}). {METHODS}: Identification of the items forming the {CASIS} was guided by patient interviews and focus groups. An observational study involving patients from the {US} and {UK} was then conducted to assess the psychometric characteristics of the measure. {RESULTS}: Qualitative data from 162 patients were used to develop the {CASIS} (n=78 {COPD}; n=84 asthma). The observational study included 311 patients with {COPD} and 324 patients with asthma. The final seven items used in the {CASIS} were identified based on factor and item response theory analyses. Internal consistency was 0.90 ({COPD}) and 0.92 (asthma), and test-retest reliability was 0.84 (both groups). In the {COPD} sample, {CASIS} scores were significantly correlated with {SGRQ} scores (all p{\textless}0.0001) and differed significantly by patient-reported disease severity, exacerbation status, and overall health status (all p[less than or equal to]0.005). In the asthma sample, {CASIS} scores were significantly correlated with {AQLQ} scores (all p{\textless}0.0001) and differed significantly by clinician and patient-reported disease severity, exacerbation status, and overall health status (all p[less than or equal to]0.0005). {CONCLUSIONS}: The {CASIS} shows good internal consistency, test-retest reliability, and construct validity and may be useful in helping to understand the impact that {COPD} and asthma have on sleep outcomes."},
	pages = {98},
	number = {1},
	journaltitle = {Health and Quality of Life Outcomes},
	author = {Pokrzywinski, Robin and Meads, David and {McKenna}, Stephen and Glendenning, G and Revicki, Dennis},
	urldate = {2009-12-15},
	date = {2009},
	keywords = {Measuring agreement, a02}
}

@article{newman_power_2003,
	title = {The power of stories over statistics},
	volume = {327},
	url = {http://www.bmj.com},
	doi = {10.1136/bmj.327.7429.1424},
	abstract = {Excerpt: "Neonatal jaundice and infant safety on aeroplanes provide two lessons on the power of narrative, rather than statistical evidence, in determining practice."},
	pages = {1424--1427},
	number = {7429},
	journaltitle = {{BMJ}},
	author = {Newman, Thomas B},
	urldate = {2009-12-10},
	date = {2003-12-20},
	keywords = {Critical appraisal, a02}
}

@article{bhopal_seven_2009,
	title = {Seven mistakes and potential solutions in epidemiology, including a call for a World Council of Epidemiology and Causality},
	volume = {6},
	issn = {1742-7622},
	url = {http://www.ete-online.com/content/6/1/6},
	doi = {10.1186/1742-7622-6-6},
	abstract = {Abstract: "All sciences make mistakes, and epidemiology is no exception. I have chosen 7 illustrative mistakes and derived 7 solutions to avoid them. The mistakes (Roman numerals denoting solutions) are: 1. Failing to provide the context and definitions of study populations. (I Describe the study population in detail.). 2. Insufficient attention to evaluation of error. ({II} Don't pretend error does not exist.). 3. Not demonstrating comparisons are like-for-like. ({III} Start with detailed comparisons of groups.). 4. Either overstatement or understatement of the case for causality. ({IV} Never say this design cannot contribute to causality or imply causality is ensured by your design.). 5. Not providing both absolute and relative summary measures. (V Give numbers, rates and comparative measures, and adjust summary measures such as odds ratios appropriately.). 6. In intervention studies not demonstrating general health benefits. ({VI} Ensure general benefits (mortality/morbidity) before recommending application of cause-specific findings.). 7. Failure to utilise study data to benefit populations. ({VII} Establish a World Council on Epidemiology to help infer causality from associations and apply the work internationally.). Analysis of these and other common mistakes is needed to benefit from the increasing discovery of associations that will be multiplying as data mining, linkage, and large-scale scale epidemiology become commonplace."},
	pages = {6},
	number = {1},
	journaltitle = {Emerging Themes in Epidemiology},
	author = {Bhopal, Raj},
	urldate = {2009-12-10},
	date = {2009}
}

@article{mercaldo_confidence_2007,
	title = {Confidence intervals for predictive values with an emphasis to case-control studies},
	volume = {26},
	url = {http://dx.doi.org/10.1002/sim.2677},
	doi = {10.1002/sim.2677},
	abstract = {Abstract: "The accuracy of a binary-scale diagnostic test can be represented by sensitivity (Se), specificity (Sp) and positive and negative predictive values ({PPV} and {NPV}). Although Se and Sp measure the intrinsic accuracy of a diagnostic test that does not depend on the prevalence rate, they do not provide information on the diagnostic accuracy of a particular patient. To obtain this information we need to use {PPV} and {NPV}. Since {PPV} and {NPV} are functions of both the accuracy of the test and the prevalence of the disease, constructing their confidence intervals for a particular patient is not straightforward. In this paper, a novel method for the estimation of {PPV} and {NPV}, as well as their confidence intervals, is developed. For both predictive values, standard, adjusted and their logit transformed-based confidence intervals are compared using coverage probabilities and interval lengths in a simulation study. These methods are then applied to two case-control studies: a diagnostic test assessing the ability of the e4 allele of the apolipoprotein E gene ({ApoE}.e4) on distinguishing patients with late-onset Alzheimer's disease ({AD}) and a prognostic test assessing the predictive ability of a 70-gene signature on breast cancer metastasis. Copyright © 2006 John Wiley \& Sons, Ltd."},
	pages = {2170--2183},
	number = {10},
	journaltitle = {Statistics in Medicine},
	author = {Mercaldo, Nathaniel D. and Lau, Kit F. and Zhou, Xiao H.},
	urldate = {2009-12-10},
	date = {2007},
	keywords = {Diagnostic testing, a02}
}

@online{chronic_disease_prevention_and_control_research_center_at_baylor_college_of_medicine_major_nodate,
	title = {Major Deficiencies in the Design and Funding of Clinical Trials: A Report to the Nation Improving on How Human Studies Are Conducted},
	url = {http://www.bcm.edu/edict/PDF/EDICT_Project_White_Paper.pdf},
	abstract = {Excerpt: "Clinical trials are a critical resource for the discovery of new, life-saving drugs and for developing better prevention and diagnostic screening methods. Today’s most effective prevention and treatment modalities are based on previous clinical trial results. But while the need for clinical research is undisputed, how clinical trials are now conducted remains problematic. Increasing research finds major deficiencies in the way clinical trials are designed, carried out and funded in the U.S. with serious implications for the outcomes of medical research studies. Of key significance for the future of scientific innovation is the exclusion or underrepresentation of women, older people, minorities, disabled persons, and rural populations in the vast majority of the research studies conducted in the U.S. Without adequate representation of all patient populations, researchers cannot learn about potential differences among groups and cannot ensure the generalization of results."},
	author = {Chronic Disease Prevention \{and\} Control Research Center at Baylor College of Medicine},
	urldate = {2009-12-09},
	keywords = {Exclusions in research, a02}
}

@article{huang_record_2007,
	title = {Record linkage research and informed consent: who consents?},
	volume = {7},
	issn = {1472-6963},
	url = {http://www.biomedcentral.com/1472-6963/7/18},
	doi = {10.1186/1472-6963-7-18},
	shorttitle = {Record linkage research and informed consent},
	abstract = {Abstract: "{BACKGROUND}: Linking computerized health insurance records with routinely collected survey data is becoming increasingly popular in health services research. However, if consent is not universal, the requirement of written informed consent may introduce a number of research biases. The participants of a national health survey in Taiwan were asked to have their questionnaire results linked to their national health insurance records. This study compares those who consented with those who refused. {METHODS}: A national representative sample (n = 14,611 adults) of the general adult population aged 20 years or older who participated in the Taiwan National Health Interview Survey ({NHIS}) and who provided complete survey information were used in this study. At the end of the survey, the respondents were asked if they would give permission to access their National Health Insurance records. Information given by the interviewees in the survey was used to analyze who was more likely to consent to linkage and who wasn't. {RESULTS}: Of the 14,611 {NHIS} participants, 12,911 (88\%) gave consent, and 1,700 (12\%) denied consent. The elderly, the illiterate, those with a lower income, and the suburban area residents were significantly more likely to deny consent. The aborigines were significantly less likely to refuse. No discrepancy in gender and self-reported health was found between individuals who consented and those who refused. {CONCLUSION}: This study is the first population-based study in assessing the consent pattern in a general Asian population. Consistent with people in Western societies, in Taiwan, a typical Asian society, a high percentage of adults gave consent for their health insurance records and questionnaire results to be linked. Consenters differed significantly from non-consenters in important aspects such as age, ethnicity, and educational background. Consequently, having a high consent rate (88\%) may not fully eliminate the possibility of selection bias. Researchers should take this source of bias into consideration in their study design and investigate any potential impact of this source of bias on their results.},
	pages = {18},
	number = {1},
	journaltitle = {{BMC} Health Services Research},
	author = {Huang, Nicole and Shih, Shu-Fang and Chang, Hsing-Yi and Chou, Yiing-Jenq},
	urldate = {2009-12-09},
	date = {2007},
	keywords = {Exclusions in research, a02}
}

@online{kuo_randomization_nodate,
	title = {Randomization May Not be Valid in Tests of Psychotherapy vs Medications},
	url = {http://www.medscape.com/viewarticle/564001},
	abstract = {Description: In a study comparing various combinations of medication and/or cognitive behavioral therapy for treating depression, only 1\% of all patients surveyed found all seven arms of the study acceptable. This leads to serious problems with volunteer bias.},
	author = {Kuo, Irving},
	urldate = {2009-12-09},
	keywords = {Exclusions in research, a02}
}

@online{parker-poe_older_nodate,
	title = {Older Cancer Patients Often Excluded From Research},
	url = {http://well.blogs.nytimes.com/2008/11/17/older-cancer-patients-excluded-from-research/},
	abstract = {Excerpt: "The majority of people diagnosed with cancer are over 65, but most major cancer studies exclude them, leaving a wide gap in knowledge about how best to treat older patients. The knowledge gap about older cancer patients was highlighted recently by researchers from Barcelona who studied the role that age played in the prognosis of 224 cancer patients."},
	author = {Parker-Poe, Tara},
	urldate = {2009-12-09},
	keywords = {Exclusions in research, a02}
}

@article{craig_non-response_2009,
	title = {Non-response bias in physical activity trend estimates},
	volume = {9},
	issn = {1471-2458},
	url = {http://www.biomedcentral.com/1471-2458/9/425},
	doi = {10.1186/1471-2458-9-425},
	abstract = {Abstract: "{BACKGROUND}: Increases in reported leisure time physical activity ({PA}) and obesity have been observed in several countries. One hypothesis for these apparently contradictory trends is differential bias in estimates over time. The purpose of this short report is to examine the potential impact of changes in response rates over time on the prevalence of adequate {PA} in Canadian adults. {METHODS}: Participants were recruited in representative national telephone surveys of {PA} from 1995-2007. Differences in {PA} prevalence estimates between participants and those hard to reach were assessed using Student's t tests adjusted for multiple comparisons. {RESULTS}: The number of telephone calls required to reach and speak with someone in the household increased over time, as did the percentage of selected participants who initially refused during the first interview attempt. A higher prevalence of adequate {PA} was observed with 5-9 attempts to reach anyone in the household in 1999-2002, but this was not significant after adjustment for multiple comparisons. {CONCLUSIONS}: No significant impact on {PA} trend estimates was observed due to differential non response rates. It is important for health policy makers to understand potential biases and how these may affect secular trends in all aspects of the energy balance equation."},
	pages = {425},
	number = {1},
	journaltitle = {{BMC} Public Health},
	author = {Craig, Cora and Cameron, Christine and Griffiths, Joe and Bauman, Adrian and Tudor-Locke, Catrine and Andersen, Ross},
	urldate = {2009-11-29},
	date = {2009},
	keywords = {Exclusions in research, a02}
}

@article{feinman_intention--treat._2009,
	title = {Intention-to-treat. What is the question?},
	volume = {6},
	issn = {1743-7075},
	url = {http://www.nutritionandmetabolism.com/content/6/1/1},
	doi = {10.1186/1743-7075-6-1},
	abstract = {Abstract: "It has become commonplace for Randomized Controlled Trials ({RCTs}) to be analyzed according to Intention-to-Treat ({ITT}) principles in which data from all subjects are used regardless of the subjects' adherence to protocol. While {ITT} analyses can provide useful information in some cases, they do not answer the question that motivates many {RCTs}, namely, whether the treatments differ in efficacy. {ITT} tends to reduce information by combining two questions, whether the intervention is effective and whether, as implemented, it has good compliance. Because these questions may be separate there is a risk of misuse. Two examples are presented that demonstrate this potential for abuse: a study on the effectiveness of vitamin E in reducing cardiovascular risk and comparisons of low fat and low carbohydrate diets. In the first case, a treatment that is demonstrably effective is described as without merit. In the second, {ITT} describes as the same, two diets that actually have different outcomes. These misuses of {ITT} are not atypical and are not technical problems in statistics but have real consequences for scientific principles and health recommendations. {ITT} analyses may answer the question of what happens when treatments are recommended but are inappropriate where separate information on adherence and performance is available. It is proposed that results of {RCTs}, or any experimental study, be reported, not in terms of the analyses that were performed, but rather in terms of the questions that the analyses can answer properly."},
	pages = {1},
	number = {1},
	journaltitle = {Nutrition \& Metabolism},
	author = {Feinman, Richard},
	urldate = {2009-02-24},
	date = {2009},
	keywords = {Exclusions in research, a02}
}

@article{zimmerman_are_2002,
	title = {Are Subjects in Pharmacological Treatment Trials of Depression Representative of Patients in Routine Clinical Practice?},
	volume = {159},
	url = {http://ajp.psychiatryonline.org/cgi/content/abstract/159/3/469},
	doi = {10.1176/appi.ajp.159.3.469},
	abstract = {Abstract: "{OBJECTIVE}: The methods used to evaluate the efficacy of antidepressants differ from treatment for depression in routine clinical practice. The rigorous inclusion/exclusion criteria used to select subjects for participation in efficacy studies potentially limit the generalizability of these trials' results. It is unknown how much impact these criteria have on the representativeness of subjects in efficacy trials. This study estimated the proportion of depressed patients treated in routine clinical practice who would meet standard inclusion/exclusion criteria for an efficacy trial. {METHOD}: A total of 803 individuals, aged 16-65 years, who were seen at intake at an outpatient practice underwent a thorough diagnostic evaluation, including the administration of semistructured diagnostic interviews; 346 patients had current major depression. Common inclusion/exclusion criteria used in efficacy studies of antidepressants were applied to the depressed patients to determine how many would have qualified for an efficacy trial. {RESULTS}: Approximately one-sixth of the 346 depressed patients would have been excluded from an efficacy trial because they had a bipolar or psychotic subtype of depression. The presence of a comorbid anxiety or substance use disorder, insufficient severity of depressive symptoms, or current suicidal ideation would have excluded 86.0\% (N=252) of the remaining 293 outpatients with nonpsychotic unipolar major depressive disorder from an antidepressant efficacy trial. {CONCLUSIONS}: Subjects treated in antidepressant trials represent a minority of patients treated for major depression in routine clinical practice. These results show that antidepressant efficacy trials tend to evaluate a subset of depressed individuals with a specific clinical profile."},
	pages = {469--473},
	number = {3},
	journaltitle = {Am J Psychiatry},
	author = {Zimmerman, Mark and Mattia, Jill I. and Posternak, Michael A.},
	urldate = {2009-12-09},
	date = {2002-03-01},
	keywords = {Exclusions in research, a02}
}

@article{may_randomized_1981,
	title = {The randomized clinical trial: bias in analysis},
	volume = {64},
	issn = {0009-7322},
	url = {http://circ.ahajournals.org/cgi/content/abstract/64/4/669},
	shorttitle = {The randomized clinical trial},
	abstract = {Abstract: "The realization that bias in patient selection may influence the results of clinical studies has helped to establish the randomized controlled clinical trial in medical research. However, bias can be equally important at other stages of a trial, especially at the time of analysis. Withdrawing patients from consideration in the analysis because of ineligibility on account of study entry criteria, lack of compliance to the protocol, or data of poor quality may be a source of systematic error. Examples to illustrate the possible consequences are taken from trials in the cardiovascular field. We recommended that reported study results should include outcome data from all subjects randomized in the group to which they were originally assigned."},
	pages = {669--673},
	number = {4},
	journaltitle = {Circulation},
	shortjournal = {Circulation},
	author = {May, G S and {DeMets}, D L and Friedman, L M and Furberg, C and Passamani, E},
	urldate = {2009-12-09},
	date = {1981-10},
	pmid = {7023743},
	keywords = {Exclusions in research, a02}
}

@article{rucker_undue_2008,
	title = {Undue reliance on I{\textasciicircum}2 in assessing heterogeneity may mislead},
	volume = {8},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/8/79/abstract},
	doi = {10.1186/1471-2288-8-79},
	abstract = {Abstract: "{BACKGROUND}: The heterogeneity statistic I{\textasciicircum}2, interpreted as the percentage of variability due to heterogeneity between studies rather than sampling error, depends on precision, that is, the size of the studies included. {METHODS}: Based on a real meta-analysis, we simulate artificially `inflating' the sample size under the random effects model. For a given inflation factor M = 1, 2, 3, ... and for each trial i, we create a M-inflated trial by drawing a treatment effect estimate from the random effects model, using s\_i{\textasciicircum}2/M as within-trial sampling variance. {RESULTS}: As precision increases, while estimates of the heterogeneity variance tau{\textasciicircum}2 remain unchanged on average, estimates of I{\textasciicircum}2 increase rapidly to nearly 100\%. A similar phenomenon is apparent in a sample of 157 meta-analyses. {CONCLUSION}: When deciding whether or not to pool treatment estimates in a meta-analysis, the yard-stick should be the clinical relevance of any heterogeneity present. tau{\textasciicircum}2, rather than I{\textasciicircum}2, is the appropriate measure for this purpose."},
	pages = {79},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Rucker, Gerta and Schwarzer, Guido and Carpenter, James and Schumacher, Martin},
	urldate = {2009-01-02},
	date = {2008},
	keywords = {Systematic overviews, a02}
}

@article{tierney_practical_2007,
	title = {Practical methods for incorporating summary time-to-event data into meta-analysis},
	volume = {8},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/8/1/16},
	doi = {10.1186/1745-6215-8-16},
	abstract = {Abstract: "{BACKGROUND}: In systematic reviews and meta-analyses, time-to-event outcomes are most appropriately analysed using hazard ratios ({HRs}). In the absence of individual patient data ({IPD}), methods are available to obtain {HRs} and/or associated statistics by carefully manipulating published or other summary data. Awareness and adoption of these methods is somewhat limited, perhaps because they are published in the statistical literature using statistical notation. {METHODS}: This paper aims to 'translate' the methods for estimating a {HR} and associated statistics from published time-to-event-analyses into less statistical and more practical guidance and provide a corresponding, easy-to-use calculations spreadsheet, to facilitate the computational aspects. {RESULTS}: A wider audience should be able to understand published time-to-event data in individual trial reports and use it more appropriately in meta-analysis. When faced with particular circumstances, readers can refer to the relevant sections of the paper. The spreadsheet can be used to assist them in carrying out the calculations. {CONCLUSION}: The methods cannot circumvent the potential biases associated with relying on published data for systematic reviews and meta-analysis. However, this practical guide should improve the quality of the analysis and subsequent interpretation of systematic reviews and meta-analyses that include time-to-event outcomes.},
	pages = {16},
	number = {1},
	journaltitle = {Trials},
	author = {Tierney, Jayne and Stewart, Lesley and Ghersi, Davina and Burdett, Sarah and Sydes, Matthew},
	urldate = {2009-03-03},
	date = {2007},
	keywords = {Systematic overviews, a02}
}

@article{eisenberg_pharmacotherapies_2008,
	title = {Pharmacotherapies for smoking cessation: a meta-analysis of randomized controlled trials},
	volume = {179},
	url = {http://www.cmaj.ca/cgi/content/abstract/179/2/135},
	doi = {10.1503/cmaj.070256},
	shorttitle = {Pharmacotherapies for smoking cessation},
	abstract = {Description: "This paper is an illustrative example of the use of Bayesian methods for meta-analysis." Abstract: "Background: Many placebo-controlled trials have demonstrated the efficacy of individual pharmacotherapies approved for smoking cessation. However, few direct or indirect comparisons of such interventions have been conducted. We performed a meta-analysis to compare the treatment effects of 7 approved pharmacologic interventions for smoking cessation. Methods: We searched the {US} Centers for Disease Control and Prevention's Tobacco Information and Prevention database as well as {MEDLINE}, {EMBASE} and the Cochrane Library for published reports of placebo-controlled, double-blind randomized controlled trials of pharmacotherapies for smoking cessation. We included studies that reported biochemically validated measures of abstinence at 6 and 12 months. We used a hierarchical Bayesian random-effects model to summarize the results for each intervention. Results: We identified 70 published reports of 69 trials involving a total of 32 908 patients. Six of the 7 pharmacotherapies studied were found to be more efficacious than placebo: varenicline (odds ratio [{OR}] 2.41, 95\% credible interval [{CrI}] 1.91-3.12), nicotine nasal spray ({OR} 2.37, 95\% {CrI} 1.12-5.13), bupropion ({OR} 2.07, 95\% {CrI} 1.73-2.55), transdermal nicotine ({OR} 2.07, 95\% {CrI} 1.69-2.62), nicotine tablet ({OR} 2.06, 95\% {CrI} 1.12-5.13) and nicotine gum ({OR} 1.71, 95\% {CrI} 1.35-2.21). Similar results were obtained regardless of which measure of abstinence was used. Although the point estimate favoured nicotine inhaler over placebo ({OR} 2.17), these results were not conclusive because the credible interval included unity (95\% {CrI} 0.95-5.43). When all 7 interventions were included in the same model, all were more efficacious than placebo. In our analysis of data from the varenicline trials that included bupropion control arms, we found that varenicline was superior to bupropion ({OR} 2.18, 95\% {CrI} 1.09-4.08). Interpretation: Varenicline, bupropion and the 5 nicotine replacement therapies were all more efficacious than placebo at promoting smoking abstinence at 6 and 12 months."},
	pages = {135--144},
	number = {2},
	journaltitle = {{CMAJ}},
	author = {Eisenberg, Mark J. and Filion, Kristian B. and Yavin, Daniel and Belisle, Patrick and Mottillo, Salvatore and Joseph, Lawrence and Gervais, Andre and O'Loughlin, Jennifer and Paradis, Gilles and Rinfret, Stephane and Pilote, Louise},
	urldate = {2009-01-30},
	date = {2008-07-15},
	keywords = {Systematic overviews, a02}
}

@article{hirji_no_2009,
	title = {No short-cut in assessing trial quality: a case study},
	volume = {10},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/10/1/1},
	doi = {10.1186/1745-6215-10-1},
	shorttitle = {No short-cut in assessing trial quality},
	abstract = {Abstract: "{BACKGROUND}: Assessing the quality of included trials is a central part of a systematic review. Many check-list type of instruments for doing this exist. Using a trial of antibiotic treatment for acute otitis media, Burke et al., {BMJ}, 1991, as the case study, this paper illustrates some limitations of the check-list approach to trial quality assessment. {RESULTS}: The general verdict from the check list type evaluations in nine relevant systematic reviews was that Burke et al. (1991) is a good quality trial. All relevant meta-analyses extensively used its data to formulate therapeutic evidence. My comprehensive evaluation, on the other hand, brought to the surface a series of serious problems in the design, conduct, analysis and report of this trial that were missed by the earlier evaluations. {CONCLUSION}: A check-list or instrument based approach, if used as a short-cut, may at times rate deeply flawed trials as good quality trials. Check lists are crucial but they need to be augmented with an in-depth review, and where possible, a scrutiny of the protocol, trial records, and original data. The extent and severity of the problems I uncovered for this particular trial warrant an independent audit before it is included in a systematic review."},
	pages = {1},
	number = {1},
	journaltitle = {Trials},
	author = {Hirji, Karim},
	urldate = {2009-02-23},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{moreno_novel_2009,
	title = {Novel methods to deal with publication biases: secondary analysis of antidepressant trials in the {FDA} trial registry database and related journal publications},
	volume = {339},
	url = {http://www.bmj.com/cgi/content/abstract/339/aug07_1/b2981},
	doi = {10.1136/bmj.b2981},
	shorttitle = {Novel methods to deal with publication biases},
	abstract = {Abstract: "Objective: To assess the performance of novel contour enhanced funnel plots and a regression based adjustment method to detect and adjust for publication biases. Design Secondary analysis of a published systematic literature review. Data sources: Placebo controlled trials of antidepressants previously submitted to the {US} Food and Drug Administration ({FDA}) and matching journal publications. Methods: Publication biases were identified using novel contour enhanced funnel plots, a regression based adjustment method, Egger's test, and the trim and fill method. Results were compared with a meta-analysis of the gold standard data submitted to the {FDA}. Results: Severe asymmetry was observed in the contour enhanced funnel plot that appeared to be heavily influenced by the statistical significance of results, suggesting publication biases as the cause of the asymmetry. Applying the regression based adjustment method to the journal data produced a similar pooled effect to that observed by a meta-analysis of the {FDA} data. Contrasting journal and {FDA} results suggested that, in addition to other deviations from study protocol, switching from an intention to treat analysis to a per protocol one would contribute to the observed discrepancies between the journal and {FDA} results. Conclusion: Novel contour enhanced funnel plots and a regression based adjustment method worked convincingly and might have an important part to play in combating publication biases."},
	pages = {b2981},
	issue = {aug07\_1},
	journaltitle = {{BMJ}},
	author = {Moreno, Santiago G and Sutton, Alex J and Turner, Erick H and Abrams, Keith R and Cooper, Nicola J and Palmer, Tom M and Ades, A E},
	urldate = {2009-08-09},
	date = {2009-08-07},
	keywords = {Systematic overviews, a02}
}

@article{mills_metastatic_2009,
	title = {Metastatic renal cell cancer treatments: An indirect comparison meta-analysis},
	volume = {9},
	issn = {1471-2407},
	url = {http://www.biomedcentral.com/1471-2407/9/34},
	doi = {10.1186/1471-2407-9-34},
	shorttitle = {Metastatic renal cell cancer treatments},
	abstract = {Abstract: "{BACKGROUND}: Treatment for metastatic renal cell cancer ({mRCC}) has advanced dramatically with understanding of the pathogenesis of the disease. New treatment options may provide improved progression-free survival ({PFS}). We aimed to determine the relative effectiveness of new therapies in this field. {METHODS}: We conducted comprehensive searches of 11 electronic databases from inception to April 2008. We included randomized trials ({RCTs}) that evaluated bevacizumab, sorafenib, and sunitinib. Two reviewers independently extracted data, in duplicate. Our primary outcome was investigator-assessed {PFS}. We performed random-effects meta-analysis with a mixed treatment comparison analysis. {RESULTS}: We included 3 bevacizumab (2 of bevacizumab plus interferon-a [{IFN}-a]), 2 sorafenib, 1 sunitinib, and 1 temsirolimus trials (total n=3,957). All interventions offer advantages for {PFS}. Using indirect comparisons with interferon-alpha as the common comparator, we found that sunitinib was superior to both sorafenib ({HR} 0.58, 95\% {CI}, 0.38-0.86, P={\textless}0.001) and bevacizumab + {IFN}-a ({HR} 0.75, 95\% {CI}, 0.60-0.93, P=0.001). Sorafenib was not statistically different from bevacizumab +{IFN}-a in this same indirect comparison analysis ({HR} 0.77, 95\% {CI}, 0.52-1.13, P=0.23). Using placebo as the similar comparator, we were unable to display a significant difference between sorafenib and bevacizumab alone ({HR} 0.81, 95\% {CI}, 0.58-1.12, P=0.23). Temsirolimus provided significant {PFS} in patients with poor prognosis ({HR} 0.69, 95\% {CI}, 0.57-0.85). {CONCLUSIONS}: New interventions for {mRCC} offer a favourable {PFS} for {mRCC} compared to interferon-alpha and placebo."},
	pages = {34},
	number = {1},
	journaltitle = {{BMC} Cancer},
	author = {Mills, Edward and Rachlis, Beth and O'Regan, Chris and Thabane, Lehana and Perri, Dan},
	urldate = {2009-01-30},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{broeze_individual_2009,
	title = {Individual patient data meta-analysis of diagnostic and prognostic studies in obstetrics, gynaecology and reproductive medicine.},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/22},
	doi = {10.1186/1471-2288-9-22},
	abstract = {Abstract: "{BACKGROUND}: In clinical practice a diagnosis is based on a combination of clinical history, physical examination and additional diagnostic tests. At present, studies on diagnostic research often report the accuracy of tests without taking into account the information already known from history and examination. Due to this lack of information, together with variations in design and quality of studies, conventional meta-analyses based on these studies will not show the accuracy of the tests in real practice. By using individual patient data ({IPD}) to perform meta-analyses, the accuracy of tests can be assessed in relation to other patient characteristics and allows the development or evaluation of diagnostic algorithms for individual patients. In this study we will examine these potential benefits in four clinical diagnostic problems in the field of gynaecology, obstetrics and reproductive medicine. {METHODS}: Based on earlier systematic reviews for each of the four clinical problems, studies are considered for inclusion. The first authors of the included studies will be invited to participate and share their original data. After assessment of validity and completeness, the acquired datasets are merged. Based on these data, a series of analyses will be performed, including a systematic comparison of the results of the {IPD} meta-analysis with those of a conventional meta-analysis, development of multivariable models for clinical history alone and for the combination of history, physical examination and relevant diagnostic tests, and development of clinical prediction rules for individual patients. These will be made accessible for clinicians. {DISCUSSION}: The use of {IPD} meta-analysis will allow evaluating accuracy of diagnostic tests in relation to other relevant information. Ultimately, this could increase the efficiency of the diagnostic work-up, e.g. by reducing the need for invasive tests and/or improving the accuracy of the diagnostic workup. This study will assess whether these benefits of {IPD} meta-analysis over conventional meta-analysis can be exploited and will provide a framework for future {IPD} meta-analyses in diagnostic research."},
	pages = {22},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Broeze, Kimiko and Opmeer, Brent and Bachmann, Lucas and Broekmans, Frank and Bossuyt, Patrick and Coppus, Sjors and Johnson, Neil and Khan, Khalid and ter Riet, Gerben and van der Veen, Fulco and van Wely, Madelon and Mol, Ben},
	urldate = {2009-03-30},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{shojania_how_2007,
	title = {How Quickly Do Systematic Reviews Go Out of Date? A Survival Analysis},
	volume = {147},
	url = {http://www.annals.org/cgi/content/abstract/147/4/224},
	shorttitle = {How Quickly Do Systematic Reviews Go Out of Date?},
	abstract = {Abstract: "Background: Systematic reviews are often advocated as the best source of evidence to guide clinical decisions and health care policy, yet we know little about the extent to which they require updating. Objective: To estimate the average time to changes in evidence that are sufficiently important to warrant updating systematic reviews. Design: Survival analysis of 100 quantitative systematic reviews. Sample: Systematic reviews published from 1995 to 2005 and indexed in {ACP} Journal Club. Eligible reviews evaluated a specific drug or class of drug, device, or procedure and included only randomized or quasi-randomized, controlled trials. Measurements: Quantitative signals for updating were changes in statistical significance or relative changes in effect magnitude of at least 50\% involving 1 of the primary outcomes of the original systematic review or any mortality outcome. Qualitative signals included substantial differences in characterizations of effectiveness, new information about harm, and caveats about the previously reported findings that would affect clinical decision making. Results: The cohort of 100 systematic reviews included a median of 13 studies and 2663 participants per review. A qualitative or quantitative signal for updating occurred for 57\% of reviews (95\% {CI}, 47\% to 67\%). Median duration of survival free of a signal for updating was 5.5 years ({CI}, 4.6 to 7.6 years). However, a signal occurred within 2 years for 23\% of reviews and within 1 year for 15\%. In 7\%, a signal had already occurred at the time of publication. Only 4\% of reviews had a signal within 1 year of the end of the reported search period; 11\% had a signal within 2 years of the search. Shorter survival was associated with cardiovascular topics (hazard ratio, 2.70 [{CI}, 1.36 to 5.34]) and heterogeneity in the original review (hazard ratio, 2.15 [{CI}, 1.12 to 4.11]). Limitation: Judgments of the need for updating were made without involving content experts. Conclusion: In a cohort of high-quality systematic reviews directly relevant to clinical practice, signals for updating occurred frequently and within a relatively short time."},
	pages = {224--233},
	number = {4},
	journaltitle = {Ann Intern Med},
	author = {Shojania, Kaveh G. and Sampson, Margaret and Ansari, Mohammed T. and Ji, Jun and Doucette, Steve and Moher, David},
	urldate = {2009-03-10},
	date = {2007-08-21},
	keywords = {Systematic overviews, a02}
}

@online{grade_working_group_grading_nodate,
	title = {The Grading of Recommendations Assessment, Development and Evaluation (short {GRADE}) Working Group},
	url = {http://www.gradeworkinggroup.org/index.htm},
	abstract = {Excerpt: "The Grading of Recommendations Assessment, Development and Evaluation (short {GRADE}) Working Group began in the year 2000 as an informal collaboration of people with an interest in addressing the shortcomings of present grading systems in health care. The working group has developed a common, sensible and transparent approach to grading quality of evidence and strength of recommendations. Many international organizations have provided input into the development of the approach and have started using it."},
	author = {Grade working group},
	urldate = {2009-03-10},
	keywords = {Systematic overviews, a02}
}

@article{guyatt_grade:_2008,
	title = {{GRADE}: an emerging consensus on rating quality of evidence and strength of recommendations},
	volume = {336},
	url = {http://www.bmj.com/cgi/content/full/336/7650/924},
	doi = {10.1136/bmj.39489.470347.AD},
	shorttitle = {{GRADE}},
	abstract = {Excerpt: "Guideline developers around the world are inconsistent in how they rate quality of evidence and grade strength of recommendations. As a result, guideline users face challenges in understanding the messages that grading systems try to communicate. Since 2006 the {BMJ} has requested in its "Instructions to Authors" on bmj.com that authors should preferably use the Grading of Recommendations Assessment, Development and Evaluation ({GRADE}) system for grading evidence when submitting a clinical guidelines article. What was behind this decision? In this first in a series of five articles we will explain why many organisations use formal systems to grade evidence and recommendations and why this is important for clinicians; we will focus on the {GRADE} approach to recommendations. In the next two articles we will examine how the {GRADE} system categorises quality of evidence and strength of recommendations. The final two articles will focus on recommendations for diagnostic tests and {GRADE}’s framework for tackling the impact of interventions on use of resources."},
	pages = {924--926},
	number = {7650},
	journaltitle = {{BMJ}},
	author = {Guyatt, Gordon H and Oxman, Andrew D and Vist, Gunn E and Kunz, Regina and Falck-Ytter, Yngve and Alonso-Coello, Pablo and Schunemann, Holger J and for the {GRADE} Working Group},
	urldate = {2009-01-03},
	date = {2008-04-26},
	keywords = {Systematic overviews, a02}
}

@article{vecchi_does_2009,
	title = {Does direction of results of abstracts submitted to scientific conferences on drug addiction predict full publication?},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/23},
	doi = {10.1186/1471-2288-9-23},
	abstract = {Abstract: "{BACKGROUND}: Data from scientific literature show that about 63\% of abstracts presented at biomedical conferences will be published in full. Some studies have indicated that full publication is associated with the direction of results (publication bias). No study has looked into the occurrence of publication bias in the field of addiction. Objectives: To investigate whether the significance or direction of results of abstracts presented at the major international scientific conference on addiction is associated with full  publication. {METHODS}: The conference proceedings of the {US} Annual Meeting of the College on Problems of Drug Dependence ({CPDD}), were handsearched for abstracts of randomized controlled trials and controlled clinical trials that evaluated interventions for prevention, rehabilitation and treatment of drug addiction in humans (years searched 1993-2002). Data regarding the study designs and outcomes reported were extracted. Subsequent publication in peer reviewed journals was searched in {MEDLINE} and {EMBASE} databases, as of March 2006. {RESULTS}: Out of 5919 abstracts presented, 581 met the inclusion criteria; 359 (62\%) conference abstracts had been published in a broad variety of peer reviewed journals (average time of publication 2.6 years, {SD} +/- 1.78). The proportion of published studies was almost the same for randomized controlled trials (62.4 \%) and controlled clinical trials (59.5 \%) while studies that reported positive results were significantly more likely to be published (74.5\%) than those that did not report statistical results (60.9\%), negative or null results (47.1\%) and no results (38.6\%). Abstracts reporting positive results had a significantly higher probability of being published in full, while abstracts reporting null or negative results were half as likely to be published compared with positive ones ({HR}=0.48; 95\%{CI} 0.30-0.74). {CONCLUSIONS}: Clinical trials were the minority of abstracts presented at the {CPDD}; we found evidence of possible publication bias in the field of addiction, with negative or null results having half the likelihood of being published than positive ones."},
	pages = {23},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Vecchi, Simona and Belleudi, Valeria and Amato, Laura and Davoli, Marina and Perucci, Carlo},
	urldate = {2009-04-23},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{lelorier_discrepancies_1997,
	title = {Discrepancies between Meta-Analyses and Subsequent Large Randomized, Controlled Trials},
	volume = {337},
	url = {http://content.nejm.org/cgi/content/abstract/337/8/536},
	doi = {10.1056/NEJM199708213370806},
	abstract = {Abstract: "Background: Meta-analyses are now widely used to provide evidence to support clinical strategies. However, large randomized, controlled trials are considered the gold standard in evaluating the efficacy of clinical interventions. Methods: We compared the results of large randomized, controlled trials (involving 1000 patients or more) that were published in four journals (the New England Journal of Medicine, the Lancet, the Annals of Internal Medicine, and the Journal of the American Medical Association) with the results of meta-analyses published earlier on the same topics. Regarding the principal and secondary outcomes, we judged whether the findings of the randomized trials agreed with those of the corresponding meta-analyses, and we determined whether the study results were positive (indicating that treatment improved the outcome) or negative (indicating that the outcome with treatment was the same or worse than without it) at the conventional level of statistical significance (P{\textless}0.05). Results: We identified 12 large randomized, controlled trials and 19 meta-analyses addressing the same questions. For a total of 40 primary and secondary outcomes, agreement between the meta-analyses and the large clinical trials was only fair (kappa = 0.35; 95 percent confidence interval, 0.06 to 0.64). The positive predictive value of the meta-analyses was 68 percent, and the negative predictive value 67 percent. However, the difference in point estimates between the randomized trials and the meta-analyses was statistically significant for only 5 of the 40 comparisons (12 percent). Furthermore, in each case of disagreement a statistically significant effect of treatment was found by one method, whereas no statistically significant effect was found by the other. Conclusions: The outcomes of the 12 large randomized, controlled trials that we studied were not predicted accurately 35 percent of the time by the meta-analyses published previously on the same topics."},
	pages = {536--542},
	number = {8},
	journaltitle = {N Engl J Med},
	author = {{LeLorier}, Jacques and Gregoire, Genevieve and Benhaddad, Abdeltif and Lapierre, Julie and Derderian, Francois},
	urldate = {2009-03-07},
	date = {1997-08-21},
	keywords = {Systematic overviews, a02}
}

@article{moreno_assessment_2009,
	title = {Assessment of regression-based methods to adjust for publication bias through a comprehensive simulation study},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/2},
	doi = {10.1186/1471-2288-9-2},
	abstract = {Abstract: "{BACKGROUND}: In meta-analysis, the presence of funnel plot asymmetry is attributed to publication or other small-study effects, which causes larger effects to be observed in the smaller studies. This issue potentially mean inappropriate conclusions are drawn from a meta-analysis. If meta-analysis is to be used to inform decision-making, a reliable way to adjust pooled estimates for potential funnel plot asymmetry is required. {METHODS}: A comprehensive simulation study is presented to assess the performance of different adjustment methods including the novel application of several regression-based methods (which are commonly applied to detect publication bias rather than adjust for it) and the popular Trim \& Fill algorithm. Meta-analyses with binary outcomes, analysed on the log odds ratio scale, were simulated by considering scenarios with and without i) publication bias and; ii) heterogeneity. Publication bias was induced through two underlying mechanisms assuming the probability of publication depends on i) the study effect size; or ii) the p-value. {RESULTS}: The performance of all methods tended to worsen as unexplained heterogeneity increased and the number of studies in the meta-analysis decreased. Applying the methods conditional on an initial test for the presence of funnel plot asymmetry generally provided poorer performance than the unconditional use of the adjustment method. Several of the regression based methods consistently outperformed the Trim \& Fill estimators. {CONCLUSIONS}: Regression-based adjustments for publication bias and other small study effects are easy to conduct and outperformed more established methods over a wide range of simulation scenarios."},
	pages = {2},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Moreno, Santiago and Sutton, Alex and Ades, A and Stanley, Tom and Abrams, Keith and Peters, Jaime and Cooper, Nicola},
	urldate = {2009-02-24},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@article{wallace_meta-analyst:_2009,
	title = {Meta-Analyst: software for meta-analysis of binary, continuous and diagnostic data},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/80},
	doi = {10.1186/1471-2288-9-80},
	shorttitle = {Meta-Analyst},
	abstract = {Abstract: "{BACKGROUND}: Meta-analysis is increasingly used as a key source of evidence synthesis to inform clinical practice. The theory and statistical foundations of meta-analysis continually evolve, providing solutions to many new and challenging problems. In practice, most meta-analyses are performed in general statistical packages or dedicated meta-analysis programs. {RESULTS}: Herein, we introduce Meta-Analyst, a novel, powerful, intuitive, and free meta-analysis program for the meta-analysis of a variety of problems. Meta-Analyst is implemented in C\# atop of the Microsoft .{NET} framework, and features a graphical user interface. The software performs several meta-analysis and meta-regression models for binary and continuous outcomes, as well as analyses for diagnostic and prognostic test studies in the frequentist and Bayesian frameworks. Moreover, Meta-Analyst includes a flexible tool to edit and customize generated meta-analysis graphs (e.g., forest plots) and provides output in many formats (images, Adobe {PDF}, Microsoft Word-ready {RTF}). The software architecture employed allows for rapid changes to be made to either the Graphical User Interface ({GUI}) or to the analytic modules.
We verified the numerical precision of Meta-Analyst by comparing its output with that from standard meta-analysis routines in Stata over a large database of 11,803 meta-analyses of binary outcome data, and 6,881 meta-analyses of continuous outcome data from the Cochrane Library of Systematic Reviews. Results from analyses of diagnostic and prognostic test studies have been verified in a limited number of meta-analyses versus {MetaDisc} and {MetaTest}. Bayesian statistical analyses use the {OpenBUGS} calculation engine (and are thus as accurate as the standalone {OpenBUGS} software). {CONCLUSION}: We have developed and validated a new program for conducting meta-analyses that combines the advantages of existing software for this task."},
	pages = {80},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Wallace, Byron and Schmid, Christopher and Lau, Joseph and Trikalinos, Thomas},
	urldate = {2009-12-09},
	date = {2009},
	keywords = {Systematic overviews, a02}
}

@book{bonate_analysis_nodate,
	location = {Boca Raton},
	title = {Analysis of pretest-posttest designs},
	isbn = {1584881739 (acid-free paper)},
	url = {http://lccn.loc.gov/00027509},
	abstract = {Excerpt: "How do you analyze pretest-posttest data? Difference scores? Percent change scores? {ANOVA}? In medical, psychological, sociological, and educational studies, researchers often design experiments in which they collect baseline (pretest) data prior to randomization. However, they often find it difficult to decide which method of statistical analysis is most appropriate to use. Until now, consulting the available literature would prove a long and arduous task, with papers sparsely scattered throughout journals and textbook references few and far between.Analysis of Pretest-Posttest Designs brings welcome relief from this conundrum. This one-stop reference - written specifically for researchers - answers the questions and helps clear the confusion about analyzing pretest-posttest data. Keeping derivations to a minimum and offering real life examples from a range of disciplines, the author gathers and elucidates the concepts and techniques most useful for studies incorporating baseline data.Understand the pros and cons of different methods - {ANOVA}, {ANCOVA}, percent change, difference scores, and {moreLearn} to choose the most appropriate statistical test - Numerous Monte Carlo simulations compare the various tests and help you select the one best suited to your {dataTackle} more difficult analyses - The extensive {SAS} code included saves you programming time and {effortRequiring} just a basic background in statistics and experimental design, this book incorporates most, if not all of the reference material that deals with pretest-posttest data. If you use baseline data in your studies, Analysis of Pretest-Posttest Designs will save you time, increase your understanding, and ultimately improve the interpretation and analysis of your data."},
	publisher = {Chapman \& Hall/{CRC}},
	author = {Bonate, Peter L},
	keywords = {Experimental design}
}

@book{silverman_wheres_nodate,
	location = {New York},
	title = {Where's the evidence? Controversies in modern medicine},
	isbn = {0192629344 (hardbound)},
	url = {http://lccn.loc.gov/97052058},
	series = {Oxford medical publications},
	shorttitle = {Where's the evidence?},
	abstract = {Excerpt: "Medicine is moving away from reliance on the proclamations of authorities to the use of numerical methods to estimate the size of effects of its interventions. But a rumbling note of uneasiness underlines present-day medical progress: the more we know, The more questions we encounter about what to do with the hard-won information. The essays in Where's the Evidence examine the dilemmas that have arisen as the result of medicine's unprecedented increase in technical powers. How do doctors draw the line between "knowing" (the acquisition of new medical information) and doing" (the application of that new knowledge)? What are the long-term consequences of responding to the demand that physicians always do everything that can be done? Is medicine's primary aim to increase the length of life? Or is it to reduce the amount of pain and suffering? And who is empowered to choose when these ends are mutually exclusive? This engaging collection of essays will be of interest to professionals interested in the evidence-based medicine debate, including epidemiologists, neonatologists, those involved in clinical trials and health policy, medical ethicists, medical students, and trainees. "},
	publisher = {Oxford University Press},
	author = {Silverman, William A},
	keywords = {Critical appraisal, a02}
}

@online{george_mason_university_zotero:_nodate,
	title = {Zotero: The Next-Generation Research Tool},
	url = {http://www.zotero.org/},
	abstract = {Excerpt: "Zotero [zoh-{TAIR}-oh] is a free, easy-to-use Firefox extension to help you collect, manage, and cite your research sources. It lives right where you do your work—in the web browser itself. "},
	author = {George Mason University},
	urldate = {2009-03-04},
	keywords = {Writing research papers, a02}
}

@article{osborne_what_2009,
	title = {What is authorship, and what should it be? A survey of prominent
guidelines for determining authorship in scientific publications.},
	volume = {14},
	abstract = {Abstract: "Before the mid 20th century most scientific writing was solely authored (Claxton, 2005; Greene, 2007) and thus it is only relatively recently, as science has grown more complex, that the ethical and procedural issues around authorship have arisen. Fields as diverse as medicine (International Committee of Medical Journal Editors, 2008), mathematics (e.g., American Statistical Association, 1999), the physical sciences (e.g., American Chemical Society, 2006), and the social sciences (e.g., American Psychological Association, 2002) have, in recent years, wrestled with what constitutes authorship and how to eliminate problematic practices such as honorary authorship and ghost authorship (e.g., Anonymous, 2004; Claxton, 2005; Manton \& English, 2008). As authorship is the coin of the realm in academia (Louis, Holdsworth, Anderson, \& Campbell, 2008), it is an ethical issue of singular importance. The goal of this paper is to review prominent and diverse guidelines concerning scientific authorship and to attempt to synthesize existing guidelines into recommendations that represent ethical practices for ensuring credit where (and only where) credit is due.},
	number = {15},
	journaltitle = {Practical Assessment, Research \& Evaluation},
	author = {Osborne, Jason W. and Holland, Abigail},
	date = {2009-07},
	keywords = {Writing research papers, a02}
}

@book{davis_scientific_nodate,
	location = {San Diego},
	edition = {2nd ed.},
	title = {Scientific papers and presentations},
	isbn = {0120884240},
	url = {http://lccn.loc.gov/2004050500},
	series = {Effective communication skills in science},
	abstract = {Excerpt: "Scientific communication is essential for helping us use and take care of this earth. Researchers who discover the wonders of science must tell someone about their findings in clear, complete, and concise terms. To add to the pool of scientific knowledge, scientists must synthesize available information with what they discover. If a scientist garbles words or leaves out important points, messages become unclear and the progress of science suffers."},
	publisher = {Academic Press},
	author = {Davis, Martha},
	keywords = {Writing research papers, a02}
}

@online{kornbrot_reporting_nodate,
	title = {Reporting Non-significant Results: Summary},
	url = {http://web.me.com/kornbrot/KornbrotNonSignificantSummary.htm},
	abstract = {Excerpt: "Aims: The purpose of the survey was to determine expert views on how to report the results of studies that produced non-significant results. The ultimate aim is to produce agreed guidelines for non-significant results. Method: Respondents were presented with 2 scenarios of studies comparing 2 groups on the proportion of people with high blood pressure. Results were given as proportion in each group with high blood pressure, size of the study and the (non-significant) chi-square. Respondents were first asked for free form text on how the results should be reported to lay and professional audiences. They were then presented with multiple choices for the proportions of respondents with high bop in each group that would be expected in a replication study. Discipline, role, experience and further comments were also elicited."},
	author = {Kornbrot, Dianaa},
	urldate = {2009-03-04},
	keywords = {Writing research papers, a02}
}

@article{ethgen_quality_2009,
	title = {Quality of reporting internal and external validity data from randomized controlled trials evaluating stents for percutaneous coronary intervention},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/24},
	doi = {10.1186/1471-2288-9-24},
	abstract = {Abstract: "{BACKGROUND}: Stents are commonly used to treat patients with coronary artery disease. However, the quality of reporting internal and external validity data in published reports of randomised controlled trials ({RCTs}) of stents has never been assessed.
The objective of our study was to evaluate the quality of reporting internal and external validity data in published reports of {RCTs} assessing the stents for percutaneous coronary interventions. {METHODS}: A systematic literature review was conducted. Reports of {RCTs} assessing stents for percutaneous coronary interventions indexed in {MEDLINE} and the Cochrane Central Register of Controlled Trials and published between January 2003 and September 2008 were selected. A standardized abstraction form was used to extract data. All analyses were adjusted for the effect of clustering articles by journal. {RESULTS}: 132 articles were analyzed. The generation of the allocation sequence was adequate in 58.3\% of the reports; treatment allocation was concealed in 34.8\%. Adequate blinding was reported in one-fifth of the reports. An intention-to-treat analysis was described in 79.5\%. The main outcome was a surrogate angiographic endpoint in 47.0\%. The volume of interventions per center was described in two reports. Operator expertise was described in five (3.8\%) reports. The quality of reporting was better in journals with high impact factors and in journals endorsing the {CONSORT} statement. {CONCLUSIONS}: The current reporting of results of {RCTs} testing stents needs to be improved to allow readers to appraise the risk of bias and the applicability of the results."},
	pages = {24},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Ethgen, Morgane and Boutron, Isabelle and Steg, Philippe Gabriel and Roy, Carine and Ravaud, Philippe},
	urldate = {2009-04-16},
	date = {2009},
	keywords = {Writing research papers, a02}
}

@online{the_plain_language_action_and_information_network_plain_nodate,
	title = {Plain Language: Improving Communications from the Federal Government to the Public},
	url = {http://www.plainlanguage.gov/},
	abstract = {Excerpt: " Plain language (also called Plain English) is communication your audience can understand the first time they read or hear it. Language that is plain to one set of readers may not be plain to others. Written material is in plain language if your audience can:  Find what they need; Understand what they find; and Use what they find to meet their needs."},
	author = {The Plain Language Action \{and\} Information Network},
	urldate = {2009-03-04},
	keywords = {Writing research papers, a02}
}

@article{mills_design_2009,
	title = {Design, analysis, and presentation of crossover trials},
	volume = {10},
	issn = {1745-6215},
	url = {http://www.trialsjournal.com/content/10/1/27},
	doi = {10.1186/1745-6215-10-27},
	abstract = {Abstract: "{OBJECTIVE}: Although crossover trials enjoy wide use, standards for analysis and reporting have not been established. We reviewed methodological aspects and quality of reporting in a representative sample of published crossover trials. {METHODS}: We searched {MEDLINE} for December 2000 and identified all randomized crossover trials. We abstracted data independently, in duplicate, on 14 design criteria, 13 analysis criteria, and 14 criteria assessing the data presentation. {RESULTS}: We identified 526 randomized controlled trials, of which 116 were crossover trials. Trials were drug efficacy (48\%), pharmacokinetic (28\%), and nonpharmacologic (30\%). The median sample size was 15 (interquartile range 8-38). Most (72\%) trials used 2 treatments and had 2 periods (64\%). Few trials reported allocation concealment (17\%) or sequence generation (7\%). Only 20\% of trials reported a sample size calculation and only 31\% of these considered pairing of data in the calculation. Carry-over issues were addressed in 29\% of trial's methods. Most trials reported and defended a washout period (70\%). Almost all trials (93\%) tested for treatment effects using paired data and also presented details on by-group results (95\%). Only 29\% presented {CIs} or {SE} so that data could be entered into a meta-analysis. {CONCLUSION}: Reports of crossover trials frequently omit important methodological issues in design, analysis, and presentation. Guidelines for the conduct and reporting of crossover trials might improve the conduct and reporting of studies using this important trial design."},
	pages = {27},
	number = {1},
	journaltitle = {Trials},
	author = {Mills, Edward and Chan, An-Wen and Wu, Ping and Vail, Andy and Guyatt, Gordon and Altman, Douglas},
	urldate = {2009-05-20},
	date = {2009},
	keywords = {Writing research papers, a02}
}

@article{siebers_data_2001,
	title = {Data Inconsistencies in Abstracts of Articles in Clinical Chemistry},
	volume = {47},
	url = {http://www.clinchem.org/cgi/content/full/47/1/149},
	abstract = {Excerpt: "The abstract of a research article is considered to be the most important part of the article. It should contain, in a brief but concise form, the critical components of the scientific study being reported. It often is the only part of the article that is read and is widely available through biomedical databases. Thus, it is of fundamental importance that data reported in abstracts are consistent with those reported in the body of the article. Recent studies have reported that data in abstracts sometimes are inconsistent with those reported in the article. The aim of this study was to determine the abstract data inconsistency rate of Clinical Chemistry."},
	pages = {149},
	number = {1},
	journaltitle = {Clin Chem},
	author = {Siebers, Robert},
	urldate = {2009-03-04},
	date = {2001-01-01},
	keywords = {Writing research papers, a02}
}

@article{hopewell_consort_2008,
	title = {{CONSORT} for Reporting Randomized Controlled Trials in Journal and Conference Abstracts: Explanation and Elaboration},
	volume = {5},
	url = {http://dx.doi.org/10.1371%2Fjournal.pmed.0050020},
	doi = {10.1371/journal.pmed.0050020},
	shorttitle = {{CONSORT} for Reporting Randomized Controlled Trials in Journal and Conference Abstracts},
	abstract = {Abstract: "Background: Clear, transparent, and sufficiently detailed abstracts of conferences and journal articles related to randomized controlled trials ({RCTs}) are important, because readers often base their assessment of a trial solely on information in the abstract. Here, we extend the {CONSORT} (Consolidated Standards of Reporting Trials) Statement to develop a minimum list of essential items, which authors should consider when reporting the results of a {RCT} in any journal or conference abstract. Methods and Findings: We generated a list of items from existing quality assessment tools and empirical evidence. A three-round, modified-Delphi process was used to select items. In all, 109 participants were invited to participate in an electronic survey; the response rate was 61\%. Survey results were presented at a meeting of the {CONSORT} Group in Montebello, Canada, January 2007, involving 26 participants, including clinical trialists, statisticians, epidemiologists, and biomedical editors. Checklist items were discussed for eligibility into the final checklist. The checklist was then revised to ensure that it reflected discussions held during and subsequent to the meeting. {CONSORT} for Abstracts recommends that abstracts relating to {RCTs} have a structured format. Items should include details of trial objectives; trial design (e.g., method of allocation, blinding/masking); trial participants (i.e., description, numbers randomized, and number analyzed); interventions intended for each randomized group and their impact on primary efficacy outcomes and harms; trial conclusions; trial registration name and number; and source of funding. We recommend the checklist be used in conjunction with this explanatory document, which includes examples of good reporting, rationale, and evidence, when available, for the inclusion of each item. Conclusions: {CONSORT} for Abstracts aims to improve reporting of abstracts of {RCTs} published in journal articles and conference proceedings. It will help authors of abstracts of these trials provide the detail and clarity needed by readers wishing to assess a trial's validity and the applicability of its results."},
	pages = {e20 EP --},
	number = {1},
	journaltitle = {{PLoS} Medicine},
	author = {Hopewell, Sally and Clarke, Mike and Moher, David and Wager, Elizabeth and Middleton, Philippa and Altman, Douglas G. and Schulz, Kenneth F.},
	urldate = {2009-03-04},
	date = {2008-01-01},
	keywords = {Writing research papers, a02}
}

@online{templeton_10_nodate,
	title = {10 Big Myths about copyright explained},
	url = {http://www.templetons.com/brad/copymyths.html},
	abstract = {Excerpt: "Note that this is an essay about copyright myths. It assumes you know at least what copyright is -- basically the legal exclusive right of the author of a creative work to control the copying of that work."},
	author = {Templeton, Brad},
	urldate = {2009-04-22},
	keywords = {Writing research papers, a02}
}

@online{roig_avoiding_nodate,
	title = {Avoiding plagiarism, self-plagiarism, and other questionable writing practices: A guide to ethical writing},
	url = {http://facpub.stjohns.edu/~roigm/plagiarism/Index.html},
	abstract = {Excerpt: "In recognizing the importance of educating aspiring scientists in the responsible conduct of research ({RCR}), the Office of Research Integrity ({ORI}), began sponsoring in 2002 the creation of instructional resources to address this pressing need.  The present guide on avoiding plagiarism and other inappropriate writing practices was created, in part, to meet this need.  Its purpose is to help students, as well as professionals, identify and prevent such practices and to develop an awareness of ethical writing.  This guide is one of the many products stemming from {ORI}’s educational initiatives in the {RCR}."},
	author = {Roig, Miguel},
	urldate = {2009-12-07},
	keywords = {Writing research papers, a02}
}

@online{ucla_academic_technology_services_spss_nodate,
	title = {{SPSS} Paper Examples: Using {SAS} Proc Mixed to Fit Multilevel Models, Hierarchical Models, and Individual Growth Models},
	url = {http://www.ats.ucla.edu/stat/spss/paperexamples/singer/default.htm},
	abstract = {Description: "This website shows how to use {SPSS} to match analysis in {SAS} in the paper 'Using {SAS} Proc Mixed to Fit Multilevel Models, Hierarchical Models, and Individual Growth Models' by Judith Singer"},
	author = {{UCLA} Academic Technology Services},
	urldate = {2009-12-05},
	keywords = {Mixed models, a02}
}

@article{singer_using_1998,
	title = {Using {SAS} {PROC} {MIXED} to Fit Multilevel Models, Hierarchical Models, and Individual Growth Models},
	volume = {23},
	url = {http://gseweb.harvard.edu/%7Efaculty/singer/Papers/Using%20Proc%20Mixed.pdf},
	doi = {10.3102/10769986023004323},
	abstract = {Abstract: "{SAS} {PROC} {MIXED} is a flexible program suitable for fitting multilevel models, hierarchical linear models, and individual growth models. Its position as an integrated program within the {SAS} statistical package makes it an ideal choice for empirical researchers and applied statisticians seeking to do data reduction, management, and analysis within a single statistical package. Because the program was developed from the perspective of a "mixed" statistical model with both random and fixed effects, its syntax and programming logic may appear unfamiliar to users in education and the social and behavioral sciences who tend to express these models as multilevel or hierarchical models. The purpose of this paper is to help users familiar with fitting multilevel models using other statistical packages (e.g., {HLM}, {MLwiN}, {MIXREG}) add {SAS} {PROC} {MIXED} to their array of analytic options. The paper is written as a step-by-step tutorial that shows how to fit the two most common multilevel models: (a) school effects models, designed for data on individuals nested within naturally occurring hierarchies (e.g., students within classes); and (b) individual growth models, designed for exploring longitudinal data (on individuals) over time. The conclusion discusses how these ideas can be extended straighforwardly to the case of three level models. An appendix presents general strategies for working with multilevel data in {SAS} and for creating data sets at several levels."},
	pages = {323--355},
	number = {4},
	journaltitle = {Journal of Educational and Behavioral Statistics},
	author = {Singer, Judith D.},
	urldate = {2009-12-05},
	date = {1998-01-01},
	keywords = {Mixed models, a02}
}

@book{diggle_analysis_2002,
	location = {New York},
	edition = {2nd ed. /},
	title = {Analysis of longitudinal data},
	isbn = {0198524846 (acid-free paper)},
	url = {http://lccn.loc.gov/2002030767},
	series = {Oxford statistical science series ; 25},
	abstract = {Description: "Diggle, Liang, and Zeger's book provides an excellent overview of methods for longitudinal models which are the source of some of the greatest complexity in Statistics today. These authors, who have pioneered some of the most important work in this area, lay out both theoretical and practical information about analysis of longitudinal data. This book is for students who want more mathematical details."},
	publisher = {Oxford University Press},
	author = {Diggle, Peter},
	date = {2002},
	keywords = {Mixed models, a02}
}

@online{browne_centre_nodate,
	title = {Centre for Multilevel Modelling ({CMM})},
	url = {http://www.cmm.bristol.ac.uk/},
	abstract = {Excerpt: "The Centre for Multilevel Modelling ({CMM}) is a research centre based at the University of Bristol within the Graduate School of Education, the School of Geographical Sciences and the Department of Clinical Veterinary Science and forming part of the The Bristol Institute of Public Affairs ({BIPA})"},
	author = {Browne, Hilary},
	urldate = {2009-12-05},
	keywords = {Mixed models, a02}
}

@article{proschan_cluster_2008,
	title = {Cluster without fluster: The effect of correlated outcomes on inference in randomized clinical trials},
	volume = {27},
	url = {http://dx.doi.org/10.1002/sim.2977},
	doi = {10.1002/sim.2977},
	shorttitle = {Cluster without fluster},
	abstract = {Abstract: "Inference for randomized clinical trials is generally based on the assumption that outcomes are independently and identically distributed under the null hypothesis. In some trials, particularly in infectious disease, outcomes may be correlated. This may be known in advance (e.g. allowing randomization of family members) or completely unplanned (e.g. sexual sharing among randomized participants). There is particular concern when the form of the correlation is essentially unknowable, in which case we cannot take advantage of the correlation to construct a more efficient test. Instead, we can only investigate the impact of potential correlation on the independent-samples test statistic. Randomization tends to balance out treatment and control assignments within clusters, so it is logical that performance of tests averaged over all possible randomization assignments would be essentially unaffected by arbitrary correlation. We confirm this intuition by showing that a permutation test controls the type 1 error rate in a certain averagesense whenever the clustering is independent of treatment assignment. It is nonetheless possible to obtain a lsquobadrsquo randomization such that members of a cluster tend to be assigned to the same treatment. Conditioned on such a bad randomization, the type 1 error rate is increased. Published in 2007 by John Wiley \& Sons, Ltd."},
	pages = {795--809},
	number = {6},
	journaltitle = {Statistics in Medicine},
	author = {Proschan, Michael and Follmann, Dean},
	urldate = {2009-12-05},
	date = {2008},
	keywords = {Mixed models, a02}
}

@online{office_for_human_research_protections_guidance_2008,
	title = {Guidance on Research Involving Coded Private Information or Biological Specimens},
	url = {http://www.hhs.gov/ohrp/humansubjects/guidance/cdebiol.htm},
	abstract = {Excerpt: "This document applies to research involving coded private information or human biological specimens (hereafter referred to as �specimens�) that is conducted or supported by {HHS}. This document does the following: (1) Provides guidance as to when research involving coded private information or specimens is or is not research involving human subjects, as defined under {HHS} regulations for the protection of human research subjects (45 {CFR} part 46). (2) Reaffirms {OHRP} policy (see {OHRP} guidance on repository activities http://www.hhs.gov/ohrp/humansubjects/guidance/reposit.htm and research on human embryonic stem cells http://www.hhs.gov/ohrp/humansubjects/guidance/stemcell.pdf) that, under certain limited conditions, research involving only coded private information or specimens is not human subjects research. (3) Clarifies the distinction between (a) research involving coded private information or specimens that does not involve human subjects and (b) human subjects research that is exempt from the requirements of the {HHS} regulations. (4) References pertinent requirements of the {HIPAA} Privacy Rule that may be applicable to research involving coded private information or specimens."},
	author = {Office for Human Research Protections},
	urldate = {2009-12-05},
	date = {2008-10-16},
	keywords = {Privacy in research, a02}
}

@online{psca_international_privacy_nodate,
	title = {Privacy needed for patients' data},
	url = {http://www.publicservice.co.uk/news_story.asp?id=11486},
	abstract = {Excerpt: "Approximately one half of patients and the general public believe that identifiable patient data should never be used for research without consent.

Whilst only 11 per cent of researchers believed this should never happen, 53 per cent of the general public and 46 per cent of patients thought it was unacceptable without prior consent.

But more than half of researchers thought patient identifiable data should be used without patient consent if it had first been reviewed by the Patient Information Advisory Group ({PIAG}). Only 30 per cent of both patients and the general public agreed."},
	author = {{PSCA International}},
	urldate = {2009-12-05},
	keywords = {Privacy in research, a02}
}

@article{goldstein_dont_2008,
	title = {Don't Ask, Don't Tell? Transfer and Sale of De-Identified Patient Data},
	volume = {4},
	url = {http://firstclinical.com/journal/2008/0804_HIPAA.pdf},
	abstract = {Excerpt: "Advances in bioinformatics and automated laboratory equipment have made it possible to “mine” very large medical databases and repositories of biological samples for information that will ultimately produce novel therapies. Some physicians and hospitals have noted the
commercial implications of this research and have begun to charge pharmaceutical
companies and biomedical researchers fees for medical data and biological specimens
obtained during patient care. Commercialization of this emerging area of research has made it more important to address issues that relate to the original source of the information and samples: patients and clinical research subjects (in this article, collectively referred to as
“patients”)."},
	number = {4},
	journaltitle = {Journal of Clinical Research Best Practices},
	author = {Goldstein, Gabrielle B. and Gordon, Jill H.},
	urldate = {2009-12-05},
	date = {2008-04},
	keywords = {Privacy in research, a02}
}

@online{trevor_sheldon_managing_2008,
	title = {Managing uncertainty in healthcare. 
Report of a meeting organised by {NICE} and {AHRQ}},
	url = {http://www.nice.org.uk/media/A1A/E6/NICEAHRQWorkshopReportFINAL.pdf},
	abstract = {Abstract: "There are certain challenges that confront virtually all health systems, irrespective of the means by which they are funded and administered. One such is the management of uncertainty: specifically, knowing what to do when data on the effectiveness or the cost-effectiveness of new medicines or procedures is incomplete or inadequate, but decisions have nonetheless to be taken on whether to purchase and supply them to patients. A similar issue arises when there is suspicion that a procedure or medicine already in use may be relatively ineffective or represent poor value for money."},
	author = {{Trevor Sheldon}},
	urldate = {2009-01-15},
	date = {2008-08-08},
	keywords = {Critical appraisal, a02}
}

@article{greenhalgh_narrative_1999,
	title = {Narrative based medicine: Narrative based medicine in an evidence based world},
	volume = {318},
	url = {http://www.bmj.com/cgi/content/full/318/7179/323},
	shorttitle = {Narrative based medicine},
	abstract = {Excerpt: "In a widely quoted riposte to critics who accused them of naive empiricism, Sackett and colleagues claimed that "the practice of evidence based medicine means integrating individual clinical expertise with the best available external clinical evidence .... By individual clinical expertise we mean the proficiency and judgment that individual clinicians acquire through clinical experience and clinical practice." Sackett and colleagues were anxious to acknowledge that there is an art to medicine as well as an objective empirical science but they did not attempt to define or categorise the elusive quality of clinical competence. This article explores the dissonance between the "science" of objective measurement and the "art" of clinical proficiency and judgment, and attempts to integrate these different perspectives on clinical method."},
	pages = {323--325},
	number = {7179},
	journaltitle = {{BMJ}},
	author = {Greenhalgh, Trisha},
	urldate = {2009-12-05},
	date = {1999-01-30},
	keywords = {Critical appraisal, a02}
}

@article{kalitzkus_narrative-based_nodate,
	title = {Narrative-Based Medicine: Potential, Pitfalls, and Practice},
	volume = {13},
	url = {http://xnet.kp.org/permanentejournal/winter09/narrativemedicine.html},
	abstract = {Excerpt: "Narratives have always been a vital part of medicine. Stories about patients, the experience of caring for them, and their recovery from illness have always been shared—among physicians as well as among patients and their relatives. With the evolution of “modern” medicine, narratives were increasingly neglected in favor of “facts and findings,” which were regarded as more scientific and objective. Now, in recent years medical narrative is changing—from the stories about patients and their illnesses, patient narratives and the unfolding and interwoven story between health care professionals and patients are both gaining momentum, leading to the creation or defining of narrative-based medicine ({NBM}). The term was coined deliberately to mark its distinction from evidence-based medicine ({EBM}); in fact, {NBM} was propagated to counteract the shortcomings of {EBM}.1,2 But what is {NBM}? Is it a specific therapeutic tool, a special form of physician-patient communication, a qualitative research tool, or does it simply signify a particular attitude towards patients and doctoring? It can be all of the above with different forms or genres of narrative or practical approach called for depending on the field of application. In this article we will give a systematic overview of {NBM}: a short historic background; the various narrative genres; and an analysis of how the genres can be effectively applied in theory, research, and practice in the medical field, with a focus on possibilities and limitations of a narrative approach. "},
	pages = {80--86},
	number = {1},
	journaltitle = {The Permanente Journal},
	author = {Kalitzkus, Vera and Matthiessen, Peter F.},
	urldate = {2009-12-05},
	keywords = {Critical appraisal, a02}
}

@online{kelly_overview_nodate,
	title = {Overview of Computer-Intensive Statistics},
	url = {http://www.hsrd.houston.med.va.gov/AdamKelly/resampling.html},
	abstract = {Excerpt: "Resampling procedures, also commonly referred to as computer intensive statistical inference procedures, may be used to assess the significance of a statistic in a hypothesis test or to determine the lower and upper bounds for a confidence interval when the usual assumptions of parametric statistical procedures are not met (Manly, 1991). Computer intensive procedures require the recomputation of hundreds or thousands of artificially constructed data sets. Like other nonparametric statistical procedures, these procedures existed as theory on paper long before they were brought into the practical mainstream. The Monte Carlo method of resampling, for example, was introduced by Barnard in 1963 (Noreen, 1989), but at that time could only be illustrated and implemented operationally on very small sample sizes.

However, with the advent of fast, inexpensive computing, essentially since around 1990, the use of computer intensive procedures has grown dramatically, particularly in the area of basic academic research. Actually, with the widespread availability of powerful personal computers and statistical software that even brings resampling-type methods right into the home, the name computer intensive seems today to be as anachronistic as it was descriptive just a few years ago."},
	author = {Kelly, P. Adam},
	urldate = {2009-03-04},
	keywords = {Resampling methods, a02}
}

@online{simon_resampling:_nodate,
	title = {Resampling: The New Statistics},
	url = {http://www.resample.com/content/text/index.shtml},
	abstract = {Excerpt: "This text grew out of chapters in the 1969 edition of Basic Research Methods in Social Science by the same author, and contains the first published example of what was later called the bootstrap. Simon is best known for his research in demography, population and the economics of natural resources, and gained fame when the noted biologist Paul Ehrlich selected five commodities and bet Simon that scarcity would drive their prices up over the period of the bet (in fact, their prices all dropped). Resampling: The New Statistics contains a number of examples in Resampling Stats, a computer program originated by Simon, but can be read on its own without the program."},
	author = {Simon, Julian L.},
	urldate = {2009-12-05},
	keywords = {Resampling methods, a02}
}

@online{howell_resampling_nodate,
	title = {Resampling Statistics: Randomization and the Bootstrap},
	url = {http://www.uvm.edu/~dhowell/StatPages/Resampling/Resampling.html},
	abstract = {Excerpt: "This set of pages is intended to serve two purposes. On the one hand, it was written to accompany a set of Windows© programs that I have written. The main program is named Resampling.exe, and is available on disk and can be downloaded from www.uvm.edu/{\textasciitilde}dhowell/{StatPages}/Resampling/{ResamplingPackage} .zip. The second purpose of these pages is to elaborate on resampling techniques and the theory behind them. "},
	author = {Howell, David C.},
	urldate = {2009-12-05},
	keywords = {Resampling methods, a02}
}

@article{gaffikin_visual_2007,
	title = {Visual inspection with acetic acid as a cervical cancer test: accuracy validated using latent class analysis},
	volume = {7},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/7/36},
	doi = {10.1186/1471-2288-7-36},
	shorttitle = {Visual inspection with acetic acid as a cervical cancer test},
	abstract = {Abstract: "{BACKGROUND}: The purpose of this study was to validate the accuracy of an alternative cervical cancer test - visual inspection with acetic acid ({VIA}) - by addressing possible imperfections in the gold standard through latent class analysis ({LCA}). The data were originally collected at peri-urban health clinics in Zimbabwe. {METHODS}: Conventional accuracy (sensitivity/specificity) estimates for {VIA} and two other screening tests using colposcopy/biopsy as the reference standard were compared to {LCA} estimates based on results from all four tests. For conventional analysis, negative colposcopy was accepted as a negative outcome when biopsy was not available as the reference standard. With {LCA}, local dependencies between tests were handled through adding direct effect parameters or additional latent classes to the model. {RESULTS}: Two models yielded good fit to the data, a 2-class model with two adjustments and a 3-class model with one adjustment. The definition of latent disease associated with the latter was more stringent, backed by three of the four tests. Under that model, sensitivity for {VIA} (abnormal+) was 0.74 compared to 0.78 with conventional analyses. Specificity was 0.639 versus 0.568, respectively. By contrast, the {LCA}-derived sensitivity for colposcopy/biopsy was 0.63. {CONCLUSION}: {VIA} sensitivity and specificity with the 3-class {LCA} model were within the range of published data and relatively consistent with conventional analyses, thus validating the original assessment of test accuracy. {LCA} probably yielded more likely estimates of the true accuracy than did conventional analysis with in-country colposcopy/biopsy as the reference standard. Colpscopy with biopsy can be problematic as a study reference standard and {LCA} offers the possibility of obtaining estimates adjusted for referent imperfections."},
	pages = {36},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Gaffikin, Lynne and {McGrath}, John and Arbyn, Marc and Blumenthal, Paul},
	urldate = {2009-12-04},
	date = {2007},
	keywords = {Diagnostic testing, a02}
}

@article{kerr_role_2008,
	title = {The Role of Clinical Uncertainty in Treatment Decisions for Diabetic Patients with Uncontrolled Blood Pressure},
	volume = {148},
	url = {http://www.annals.org/content/148/10/717.abstract},
	doi = {VL  - 148},
	abstract = {Abstract: "Factors underlying failure to intensify therapy in response to elevated blood pressure have not been systematically studied.
To examine the process of care for diabetic patients with elevated triage blood pressure (≥140/90 mm Hg) during routine primary care visits to assess whether a treatment change occurred and to what degree specific patient and provider factors correlated with the likelihood of treatment change.
Prospective cohort study.
9 Veterans Affairs facilities in 3 midwestern states. 1169 diabetic patients with scheduled visits to 92 primary care providers from February 2005 to March 2006.
Proportion of patients who had a change in a blood pressure treatment (medication intensification or planned follow-up within 4 weeks). Predicted probability of treatment change was calculated from a multilevel logistic model that included variables assessing clinical uncertainty, competing demands and prioritization, and medication-related factors (controlling for blood pressure).
Overall, 573 (49\%) patients had a blood pressure treatment change at the visit. The following factors made treatment change less likely: repeated blood pressure by provider recorded as less than 140/90 mm Hg versus 140/90 mm Hg or greater or no recorded repeated blood pressure (13\% vs. 61\%;  {\textless} 0.001); home blood pressure reported by patients as less than 140/90 mm Hg versus 140/90 mm Hg or greater or no recorded home blood pressure (18\% vs. 52\%;  {\textless} 0.001); provider systolic blood pressure goal greater than 130 mm Hg versus 130 mm Hg or less (33\% vs. 52\%;  = 0.002); discussion of conditions unrelated to hypertension and diabetes versus no discussion (44\% vs. 55\%;  = 0.008); and discussion of medication issues versus no discussion (23\% vs. 52\%;  {\textless} 0.001). Providers knew that the study pertained to diabetes and hypertension, and treatment change was assessed for 1 visit per patient. Approximately 50\% of diabetic patients presenting with a substantially elevated triage blood pressure received treatment change at the visit. Clinical uncertainty about the true blood pressure value was a prominent reason that providers did not intensify therapy."},
	pages = {717--727},
	number = {10},
	journaltitle = {Annals of Internal Medicine},
	author = {Kerr, Eve A. and Zikmund-Fisher, Brian J. and Klamerus, Mandi L. and Subramanian, Usha and Hogan, Mary M. and Hofer, Timothy P.},
	urldate = {2009-12-04},
	date = {2008-05-20},
	keywords = {Diagnostic testing, a02}
}

@article{berner_overconfidence_2008,
	title = {Overconfidence as a Cause of Diagnostic Error in Medicine},
	volume = {121},
	issn = {00029343},
	url = {http://www.amjmed.com/article/S0002-9343(08)00040-5/fulltext},
	doi = {10.1016/j.amjmed.2008.01.001},
	abstract = {Abstract: "The great majority of medical diagnoses are made using automatic, efficient cognitive processes, and these diagnoses are correct most of the time. This analytic review concerns the exceptions: the times when these cognitive processes fail and the final diagnosis is missed or wrong. We argue that physicians in general underappreciate the likelihood that their diagnoses are wrong and that this tendency to overconfidence is related to both intrinsic and systemically reinforced factors. We present a comprehensive review of the available literature and current thinking related to these issues. The review covers the incidence and impact of diagnostic error, data on physician overconfidence as a contributing cause of errors, strategies to improve the accuracy of diagnostic decision making, and recommendations for future research."},
	pages = {S2--S23},
	number = {5},
	journaltitle = {The American Journal of Medicine},
	shortjournal = {The American Journal of Medicine},
	author = {Berner, E and Graber, M},
	urldate = {2009-12-04},
	date = {2008-05},
	keywords = {Diagnostic testing, a02}
}

@article{berner_missed_2007,
	title = {Missed and Delayed Diagnoses in the Ambulatory Setting},
	volume = {146},
	url = {http://www.annals.org/content/146/6/470.1.extract},
	doi = {VL  - 146},
	abstract = {Excerpt: "We applaud Gandhi and colleagues for highlighting the problem of outpatient diagnostic errors. However, malpractice claims are a biased data source. Primary identification of diagnostic errors in ambulatory settings remains problematic."},
	pages = {470},
	number = {6},
	journaltitle = {Annals of Internal Medicine},
	author = {Berner, Eta S. and Miller, Randolph A. and Graber, Mark L.},
	urldate = {2009-12-04},
	date = {2007-03-20},
	keywords = {Diagnostic testing, a02}
}

@article{sach_men_2009,
	title = {Men and women: beliefs about cancer and about screening},
	volume = {9},
	issn = {1471-2458},
	url = {http://www.biomedcentral.com/1471-2458/9/431},
	doi = {10.1186/1471-2458-9-431},
	shorttitle = {Men and women},
	abstract = {Abstract: "{BACKGROUND}: Cancer screening programmes in England are publicly-funded. Professionals' beliefs in the public health benefits of screening can conflict with individuals' entitlements to exercise informed judgement over whether or not to participate. The recognition of the importance of individual autonomy in decision making requires greater understanding of the knowledge, attitudes and beliefs upon which people's screening choices are founded. Until recently, the technology available required that cancer screening be confined to women. This study aimed to discover whether male and female perceptions of cancer and of screening differ. {METHODS}: Data on the public's cancer beliefs were collected by means of a postal survey (anonymous questionnaire). Two general practices based in Nottingham and in Mansfield, in east-central England, sent questionnaires to registered patients aged 30 to 70 years. 1,808 completed questionnaires were returned for analysis, 56.5 per cent from women. {RESULTS}: Women were less likely to underestimate overall cancer incidence, although each sex was more likely to cite a sex-specific cancer as being amongst the most common cancer site. In terms of risk factors, men were most uncertain about the role of stress and sexually-transmitted diseases, whereas women were more likely to rate excessive alcohol and family history as major risk factors. The majority of respondents believed the public health care system should provide cancer screening, but significantly more women than men reported having benefiting from the nationally-provided screening services. Those who were older, in better health or had longer periods of formal education were less worried about cancer than those who had illness experiences, lower incomes, or who were smokers. Actual or potential participation in bowel screening was higher amongst those who believed bowel cancer to be common and amongst men, despite women having more substantial worries about cancer than men. {CONCLUSIONS}: Our results suggest that men's and women's differential knowledge of cancer correlates with women's closer involvement with screening. Even so, men were neither less positive about screening nor less likely to express a willingness to participate in relevant screening in the future. It is important to understand gender-related differences in knowledge and perceptions of cancer, if health promotion resources are to be allocated efficiently."},
	pages = {431},
	number = {1},
	journaltitle = {{BMC} Public Health},
	author = {Sach, Tracey and Whynes, David},
	urldate = {2009-11-30},
	date = {2009},
	keywords = {Diagnostic testing, a02}
}

@article{bowen_educational_2006,
	title = {Educational strategies to promote clinical diagnostic reasoning},
	volume = {355},
	issn = {1533-4406},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/17124019},
	doi = {10.1056/NEJMra054782},
	abstract = {Excerpt: "Clinical teachers differ from clinicians in a fundamental way. They must simultaneously foster high-quality patient care and assess the clinical skills and reasoning of learners in order to promote their progress toward independence in the clinical setting. Clinical teachers must diagnose both the patient's clinical problem and the learner's ability and skill. To assess a learner's diagnostic reasoning strategies effectively, the teacher needs to consider how doctors learn to reason in the clinical environment."},
	pages = {2217--2225},
	number = {21},
	journaltitle = {The New England Journal of Medicine},
	shortjournal = {N. Engl. J. Med},
	author = {Bowen, Judith L},
	urldate = {2009-12-04},
	date = {2006-11-23},
	pmid = {17124019},
	keywords = {Diagnostic testing, a02}
}

@article{redelmeier_cognitive_2005,
	title = {The Cognitive Psychology of Missed Diagnoses},
	volume = {142},
	url = {http://www.annals.org/cgi/content/abstract/142/2/115},
	abstract = {Abstract: "Cognitive psychology is the science that examines how people reason, formulate judgments, and make decisions. This case involves a patient given a diagnosis of pharyngitis, whose ultimate diagnosis of osteomyelitis was missed through a series of cognitive shortcuts. These errors include the availability heuristic (in which people judge likelihood by how easily examples spring to mind), the anchoring heuristic (in which people stick with initial impressions), framing effects (in which people make different decisions depending on how information is presented), blind obedience (in which people stop thinking when confronted with authority), and premature closure (in which several alternatives are not pursued). Rather than trying to completely eliminate cognitive shortcuts (which often serve clinicians well), becoming aware of common errors might lead to sustained improvement in patient care."},
	pages = {115--120},
	number = {2},
	journaltitle = {Ann Intern Med},
	author = {Redelmeier, Donald A.},
	urldate = {2009-07-08},
	date = {2005-01-18},
	keywords = {Diagnostic testing, a02}
}

@article{hozo_calculating_1998,
	title = {Calculating confidence intervals for threshold and post-test probabilities},
	volume = {15},
	issn = {0724-6811},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/9540324},
	abstract = {Abstract: "We describe a method and a computer program, written in {JavaScript}, for calculating confidence intervals. The method uses Taylor's series to approximate the standard errors of a post-test probability and threshold probabilities and, from them, to obtain the associated confidence intervals. This method is valid if the variables of interest are stochastically independent."},
	pages = {110--115},
	number = {2},
	journaltitle = {M.D. Computing: Computers in Medical Practice},
	shortjournal = {{MD} Comput},
	author = {Hozo, I and Djulbegovic, B},
	urldate = {2009-12-04},
	date = {1998-04},
	pmid = {9540324},
	keywords = {Diagnostic testing, a02}
}

@article{hamberg_accuracy_1996,
	title = {Accuracy of clinical diagnosis of cirrhosis among alcohol-abusing men},
	volume = {49},
	issn = {0895-4356},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/8892498},
	abstract = {Abstract: "There is a considerable variation among specialists in the use of liver biopsy for the diagnosis of alcoholic cirrhosis, which is often based solely on clinical findings, sometimes supplemented with blood tests. To assess the diagnostic accuracy that may be achieved by this approach, we related items of the history, symptoms and signs, and routine blood tests to the presence/absence of cirrhosis in a unique, previously established, consecutive series of 303 alcohol-abusing men, in whom liver biopsy was performed irrespective of the clinical and biochemical findings. Using logistic regression analyses, we created a clinical, a combined clinical and biochemical, and a pure biochemical diagnostic model. The probability of cirrhosis in patients with the specified characteristics was estimated, the diagnostic accuracy was assessed as functions of diagnostic thresholds for cirrhosis defined by the probability of cirrhosis varying between 0 and 1,and confidence intervals were estimated by bootstrap sampling. The clinical model, including facial teleangiectasia, vascular spiders, white nails, abdominal veins, fatness, and peripheral edema, could be used with high diagnostic accuracy and it was clearly superior to the biochemical model. Adding biochemical findings to the clinical model improved the accuracy of the clinical model only slightly. We conclude that cirrhosis may be diagnosed in alcohol-abusing men with a high accuracy using selected, properly weighted clinical observations only."},
	pages = {1295--1301},
	number = {11},
	journaltitle = {Journal of Clinical Epidemiology},
	shortjournal = {J Clin Epidemiol},
	author = {Hamberg, K J and Carstensen, B and Sørensen, T I and Eghøje, K},
	urldate = {2009-12-04},
	date = {1996-11},
	pmid = {8892498},
	keywords = {Diagnostic testing, a02}
}

@article{klotsche_novel_2009,
	title = {A novel nonparametric approach for estimating cut-offs in continuous risk indicators with application to diabetes epidemiology},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/63},
	doi = {10.1186/1471-2288-9-63},
	abstract = {Abstract: "{BACKGROUND}: Epidemiological and clinical studies, often including anthropometric measures, have established obesity as a major risk factor for the development of type 2 diabetes. Appropriate cut-off values for anthropometric parameters are necessary for prediction or decision purposes. The cut-off corresponding to the Youden-Index is often applied in epidemiology and biomedical literature for dichotomizing a continuous risk indicator. {METHODS}: Using data from a representative large multistage longitudinal epidemiological study in a primary care setting in Germany, this paper explores a novel approach for estimating optimal cut-offs of anthropomorphic parameters for predicting type 2 diabetes based on a discontinuity of a regression function in a nonparametric regression framework. {RESULTS}: The resulting cut-off corresponded to values obtained by the Youden Index (maximum of the sum of sensitivity and specificity, minus one), often considered the optimal cut-off in epidemiological and biomedical research. The nonparametric regression based estimator was compared to results obtained by the established methods of the Receiver Operating Characteristic plot in various simulation scenarios and based on bias and root mean square error, yielded excellent finite sample properties. {CONCLUSION}: It is thus recommended that this nonparametric regression approach be considered as valuable alternative when a continuous indicator has to be dichotomized at the Youden Index for prediction or decision purposes."},
	pages = {63},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Klotsche, Jens and Ferger, Dietmar and Pieper, Lars and Rehm, Jurgen and Wittchen, Hans-Ulrich},
	urldate = {2009-10-11},
	date = {2009},
	keywords = {Diagnostic testing, a02}
}

@article{greenland_empirical-bayes_1991,
	title = {Empirical-Bayes Adjustments for Multiple Comparisons Are Sometimes Useful},
	volume = {2},
	issn = {10443983},
	url = {http://www.jstor.org/stable/20065674},
	abstract = {Abstract: "Rothman recommends against adjustments for multiple comparisons. Implicit in his recommendation, however, is an assumption that the sole objective of the data analysis is to report and scientifically interpret the data. We concur with his recommendation when this assumption is correct and one is willing to abandon frequentist interpretations of the summary statistics. Nevertheless, there are situations in which an additional or even primary goal of analysis is to reach a set of decisions based on the data. In such situations, Bayes and empirical-Bayes adjustments can provide a better basis for the decisions than conventional procedures."},
	pages = {244--251},
	number = {4},
	journaltitle = {Epidemiology},
	author = {Greenland, Sander and Robins, James M.},
	urldate = {2009-12-02},
	date = {1991-07},
	note = {{ArticleType}: primary\_article / Full publication date: Jul., 1991 / Copyright © 1991 Lippincott Williams \& Wilkins},
	keywords = {Bayesian Statistics, a02}
}

@online{u.s._food_and_drug_administration_guidance_nodate,
	title = {Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials},
	url = {http://www.fda.gov/MedicalDevices/DeviceRegulationandGuidance/GuidanceDocuments/ucm071072.htm},
	abstract = {Excerpt: "This document provides guidance on statistical aspects of the design and analysis of clinical trials for medical devices that use Bayesian statistical methods. The purpose of this guidance is to discuss important statistical issues in Bayesian clinical trials for medical devices and not to describe the content of a medical device submission. Further, while this document provides guidance on many of the statistical issues that arise in Bayesian clinical trials, it is not intended to be all-inclusive. The statistical literature is rich with books and papers on Bayesian theory and methods; a selected bibliography has been included for further discussion of specific topics. {FDA}’s guidance documents, including this guidance, do not establish legally enforceable responsibilities. Instead, guidances describe the Agency’s current thinking on a topic and should be viewed only as recommendations, unless specific regulatory or statutory requirements are cited. The use of the word should in Agency guidances means that something is suggested or recommended, but not required."},
	titleaddon = {Guidance for the Use of Bayesian Statistics in Medical Device Clinical Trials},
	author = {U.S. Food \{and\} Drug Administration},
	urldate = {2009-10-13},
	keywords = {Bayesian Statistics, a02}
}

@article{stallard_decision_1999,
	title = {Decision theoretic designs for phase {II} clinical trials with multiple outcomes},
	volume = {55},
	issn = {0006-341X},
	url = {http://www.ncbi.nlm.nih.gov/pubmed/11315037},
	abstract = {Abstract: "In many phase {II} clinical trials, it is essential to assess both efficacy and safety. Although several phase {II} designs that accommodate multiple outcomes have been proposed recently, none are derived using decision theory. This paper describes a Bayesian decision theoretic strategy for constructing phase {II} designs based on both efficacy and adverse events. The gain function includes utilities assigned to patient outcomes, a reward for declaring the new treatment promising, and costs associated with the conduct of the phase {II} trial and future phase {III} testing. A method for eliciting gain function parameters from medical collaborators and for evaluating the design's frequentist operating characteristics is described. The strategy is illustrated by application to a clinical trial of peripheral blood stem cell transplantation for multiple myeloma."},
	pages = {971--977},
	number = {3},
	journaltitle = {Biometrics},
	shortjournal = {Biometrics},
	author = {Stallard, N and Thall, P F and Whitehead, J},
	urldate = {2009-12-02},
	date = {1999-09},
	pmid = {11315037},
	keywords = {Bayesian Statistics, a02}
}

@article{gajewski_improving_2008,
	title = {Improving quality indicator report cards through Bayesian modeling},
	volume = {8},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/8/77},
	doi = {10.1186/1471-2288-8-77},
	abstract = {Abstract: "{BACKGROUND}: The National Database for Nursing Quality Indicators(R) ({NDNQI}(R)) was established in 1998 to assist hospitals in monitoring indicators of nursing quality (eg, falls and pressure ulcers). Hospitals participating in {NDNQI} transmit data from nursing units to an {NDNQI} data repository. Data are summarized and published in reports that allow participating facilities to compare the results for their units with those from other units across the nation. A disadvantage of this reporting scheme is that the sampling variability is not explicit. For example, suppose a small nursing unit that has 2 out of 10 (rate of 20\%) patients with pressure ulcers. Should the nursing unit immediately undertake a quality improvement plan because of the rate difference from the national average (7\%). {METHODS}: In this paper, we propose approximating 95\% credible intervals ({CrIs}) for unit-level data using statistical models that account for the variability in unit rates for report cards. {RESULTS}: Bayesian {CrIs} communicate the level of uncertainty of estimates more clearly to decision makers than other significance tests. {CONCLUSION}: A benefit of this approach is that nursing units would be better able to distinguish problematic or beneficial trends from fluctuations likely due to chance."},
	pages = {77},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Gajewski, Byron and Mahnken, Jonathan and Dunton, Nancy},
	urldate = {2009-01-03},
	date = {2008},
	keywords = {Bayesian Statistics, a02}
}

@article{cao_bayesian_2009,
	title = {Bayesian optimal discovery procedure for simultaneous significance testing},
	volume = {10},
	issn = {1471-2105},
	url = {http://www.biomedcentral.com/1471-2105/10/5},
	doi = {10.1186/1471-2105-10-5},
	abstract = {Abstract: "{BACKGROUND}: In high throughput screening, such as differential gene expression screening, drug sensitivity screening, and genome-wide {RNAi} screening, tens of thousands of tests need to be conducted simultaneously. However, the number of replicate measurements per test is extremely small, rarely exceeding 3. Several current approaches demonstrate that test statistics with shrinking variance estimates have more power over the traditional t statistic. {RESULTS}: We propose a Bayesian hierarchical model to incorporate the shrinkage concept by introducing a mixture structure on variance components. The estimates from the Bayesian model are utilized in the optimal discovery procedure ({ODP}) proposed by Storey in 2007, which was shown to have optimal performance in multiple significance tests. We compared the performance of the Bayesian {ODP} with several competing test statistics. {CONCLUSION}: We have conducted simulation studies with 2 to 6 replicates per gene. We have also included test results from two real datasets. The Bayesian {ODP} outperforms the other methods in our study, including the original {ODP}. The advantage of the Bayesian {ODP} becomes more significant when there are few replicates per test. The improvement over the original {ODP} is based on the fact that Bayesian model borrows strength across genes in estimating unknown parameters. The proposed approach is efficient in computation due to the conjugate structure of the Bayesian model. The R code (see Additional file 1) to calculate the Bayesian {ODP} is provided."},
	pages = {5},
	number = {1},
	journaltitle = {{BMC} Bioinformatics},
	author = {Cao, Jing and Xie, Xian-Jin and Zhang, Song and Whitehurst, Angelique and White, Michael},
	urldate = {2009-02-23},
	date = {2009},
	keywords = {Bayesian Statistics, a02}
}

@article{coory_bayesian_2009,
	title = {Bayesian versus frequentist statistical inference for investigating a one-off cancer cluster reported to a health department},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/30},
	doi = {10.1186/1471-2288-9-30},
	abstract = {Abstract: {BACKGROUND}: The problem of silent multiple comparisons is one of the most difficult statistical problems faced by scientists. It is a particular problem for investigating a one-off cancer cluster reported to a health department because any one of hundreds, or possibly thousands, of neighbourhoods, schools, or workplaces could have reported a cluster, which could have been for any one of several types of cancer or any one of several time
periods. {METHODS}: This paper contrasts the frequentist approach with a Bayesian approach for dealing with silent multiple comparisons in the context of a one-off cluster reported to a health department. Two published cluster investigations were re-analysed using the Dunn-Sidak method to adjust frequentist p-values and confidence intervals for silent multiple comparisons. Bayesian methods were based on the Gamma distribution. {RESULTS}: Bayesian analysis with non-informative priors produced results similar to the frequentist analysis, and suggested that both clusters represented a statistical excess. In the frequentist framework, the statistical significance of both clusters was extremely sensitive to the number of silent multiple comparisons, which can only ever be a subjective "guesstimate". The Bayesian approach is also subjective: whether there is an apparent statistical excess depends on the specified prior. {CONCLUSIONS}: In cluster investigations, the frequentist approach is just as subjective as the Bayesian approach, but the Bayesian approach is less ambitious in that it treats the analysis as a synthesis of data and personal judgements (possibly poor ones), rather than objective reality. Bayesian analysis is (arguably) a useful tool to support complicated decision-making, because it makes the uncertainty associated with silent multiple comparisons explicit."},
	pages = {30},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Coory, Michael and Wills, Rachael and Barnett, Adrian},
	urldate = {2009-05-19},
	date = {2009},
	keywords = {Bayesian Statistics, a02}
}

@article{ibrahim_bayesian_2002,
	title = {Bayesian Models for Gene Expression with {DNA} Microarray Data},
	volume = {97},
	issn = {01621459},
	url = {http://www.jstor.org/stable/3085761},
	abstract = {Abstract: "Two of the critical issues that arise when examining {DNA} microarray data are (I) determination of which genes best discriminate among the different types of tissue, and (2) characterization of expression patterns in tumor tissues. For (1), there are many genes that characterize {DNA} expression, and it is of critical importance to try and identify a small set of genes that best discriminate between normal and tumor tissues. For (2), it is critical to be able to characterize the {DNA} expression of the normal and tumor tissue samples and develop suitable models that explain patterns of {DNA} expression for these types of tissues. Toward this goal,. we propose a novel Bayesian model for analyzing {DNA} microarray data and propose a model selection methodology for identifying subsets of genes that show different expression levels between normal and cancer tissues. In addition, we propose a novel class of hierarchical priors for the parameters that allow us to borrow strength across genes for making inference. The properties of the priors are examined in detail. We introduce a Bayesian model selection criterion for assessing the various models, and develop Markov chain Monte Carlo algorithms for sampling from the posterior distributions of the parameters and for computing the criterion. We present a detailed case study in endometrial cancer to demonstrate our proposed methodology."},
	pages = {88--99},
	number = {457},
	journaltitle = {Journal of the American Statistical Association},
	author = {Ibrahim, Joseph G. and Chen, Ming-Hui and Gray, Robert J.},
	urldate = {2009-12-02},
	date = {2002-03},
	note = {{ArticleType}: primary\_article / Full publication date: Mar., 2002 / Copyright © 2002 American Statistical Association},
	keywords = {Bayesian Statistics, a02}
}

@online{data_analysis_and_story_library_nambeware_nodate,
	title = {Nambeware Polishing Times},
	url = {http://lib.stat.cmu.edu/DASL/Datafiles/nambedat.html},
	abstract = {Excerpt: "Nambe Mills manufactures a line of tableware made from sand casting a special alloy of several metals. After casting, the pieces go through a series of shaping, grinding, buffing, and polishing steps. In 1989 the company began a program to rationalize its production schedule of some 100 items in its tableware line. The total grinding and polishing times listed here were a major output of this program. Number of cases: 59. Variable Names: 1. {BOWL}: Bowl (1) or not (0); 2. {CASS}: Casserole (1) or not (0); 3. {DISH}: Dish (1) or not (0); 4. {TRAY}: Tray (1) or not (0); 5. {DIAM}: Diameter of item, or equivalent (inches); 6. {TIME}: Grinding and polishing time (minutes); 7. {PRICE}: Retail price (\$). Note: Items not classed as bowl, casserole, dish, or tray are plates."},
	author = {{Data Analysis and Story Library}},
	urldate = {2009-12-01},
	keywords = {Analysis of Variance, a02}
}

@online{ucla_academic_technology_services_coding_nodate,
	title = {Coding systems for categorical variables in regression analysis},
	url = {http://www.ats.ucla.edu/stat/sas/webbooks/reg/chapter5/sasreg5.htm},
	abstract = {Excerpt: "Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. For example, if you have a variable called race that is coded 1 = Hispanic, 2 = Asian 3 = Black 4 = White, then entering race in your regression will look at the linear effect of race, which is probably not what you intended. Instead, categorical variables like this need to be recoded into a series of variables which can then be entered into the regression model. There are a variety of coding systems that can be used when coding categorical variables. Ideally, you would choose a coding system that reflects the comparisons that you want to make. In Chapter 3 of the Regression with {SAS} Web Book we covered the use of categorical variables in regression analysis focusing on the use of dummy variables, but that is not the only coding scheme that you can use. For example, you may want to compare each level to the next higher level, in which case you would want to use "forward difference" coding, or you might want to compare each level to the mean of the subsequent levels of the variable, in which case you would want to use "Helmert" coding. By deliberately choosing a coding system, you can obtain comparisons that are most meaningful for testing your hypotheses."},
	author = {{UCLA} Academic Technology Services},
	urldate = {2009-12-01},
	keywords = {Analysis of Variance, a02}
}

@online{howell_multiple_nodate,
	title = {Multiple Comparisons with Repeated Measures},
	url = {http://www.uvm.edu/~dhowell/StatPages/More_Stuff/RepMeasMultComp/RepMeasMultComp.html},
	abstract = {Excerpt: "One of the commonly asked questions on listservs dealing with statistical issue is 'How do I use {SPSS} (or whatever software is at hand) to run multiple comparisons among a set of repeated measures?' This page is a (longwinded) attempt to address that question. I will restrict myself to the case of one repeated measure (with or without a between subjects variable), but the generalization to more complex cases should be apparent."},
	author = {Howell, David C.},
	urldate = {2009-12-01},
	keywords = {Analysis of Variance, a02}
}

@online{kermit_assessing_nodate,
	title = {Assessing the Performance of a Sensory Panel - Panelist monitoring and tracking},
	url = {http://www.camo.com/resources/casestudies/PMT.pdf},
	abstract = {Abstract: "Sensory science uses the human senses as instruments of measures. This study presents univariate and multivariate data analysis methods to assess individual and group performances in a sensory panel. Green peas were evaluated by a trained panel of 10 assessors for six attributes over two replicates. A consonance analysis with Principal Component Analysis ({PCA}) is run to get an overview of the panel agreement and detect major individual errors. The origin of the panelist errors is identi⬚ed by a series of tests based on {ANOVA}: sensitivity, reproducibility, crossover and panel agreement, complemented with an eggshell-correlation test. One assessor is identi⬚ed with further need for training in attributes pea fl⬚avour, sweetness, fruity and off-fl⬚avour, showing errors in sensitivity, reproducibility and crossover. Another assessor shows poor performance for attribute mealiness and to some extent also fruity ⬚avour. Only one panelist performs well to very well in all attributes. The speci⬚city and complementarity of the series of univariate tests are explored and veri⬚ed with the use of a {PCA} model. Keywords: Sensory panel performance; {ANOVA}; Agreement error; Sensitivity; Reproducibility; Crossover; Eggshell plot."},
	author = {Kermit, Martin and Lengard, Valerie},
	urldate = {2009-12-01},
	keywords = {Analysis of Variance, a02}
}

@article{homa_analysis_2007,
	title = {Analysis of Means Used to Compare Providers' Referral Patterns},
	volume = {16},
	url = {http://journals.lww.com/qmhcjournal/pages/articleviewer.aspx?year=2007&issue=07000&article=00009&type=abstract},
	doi = {10.1097/01.QMH.0000281062.22732.ac},
	abstract = {Abstract: "Objectives: The objective of this study was to demonstrate through a case study how an analysis of means ({ANOM}) chart can be used to compare groups and to advocate the usefulness of this method in improvement work. Methods: The {ANOM} technique was used to compare referral rates among providers at the Dartmouth-Hitchcock Medical Center's Spine Center. The purpose was to see whether there were any differences across providers in referral rates to Behavioral Medicine services for patients who scored low on their mental health score and whether referral rates were any different among the patient characteristics. {ANOM} charts were also used to determine whether patient characteristics were different among the providers. Results: Six of the 17 providers had significantly different referral rates compared to the overall referral rate of 38\%. Seven patients' characteristics had a significantly different referral rate compared to the system's rate. The additional {ANOM} charts used to compare providers relative to specific patient characteristics demonstrated several special causes and revealed characteristic referral patterns for some of the providers analyzed. Conclusion: The {ANOM} chart may be underutilized in health care improvement work. The {ANOM} procedure of analyzing patient characteristics to determine differences among providers could be explored in other patient populations and settings."},
	pages = {256--264},
	number = {3},
	journaltitle = {Quality Management in Health Care},
	author = {Homa, Karen},
	urldate = {2009-12-01},
	date = {2007},
	keywords = {Analysis of Means, a02}
}

@article{argaud_changeovers_2007,
	title = {Changeovers of vasoactive drug infusion pumps: impact of a quality improvement program},
	volume = {11},
	issn = {1364-8535},
	url = {http://ccforum.com/content/11/6/R133},
	doi = {10.1186/cc6209},
	shorttitle = {Changeovers of vasoactive drug infusion pumps},
	abstract = {Abstract: "{BACKGROUND}: Hemodynamic instability following the changeover of vasoactive infusion pump ({CVIP}) is a common problem in the intensive care unit. Several empiric methods are used to achieve {CVIP}. We hypothesized that the variation in these procedures could generate some morbidity. We sought to assess the effects of the standardization of practice, as a quality improvement program, on the {CVIP}-induced incidents. {MATERIALS} {AND} {METHODS}: We performed a prospective before-and-after intervention study including all adult patients with a diagnosis of cardiovascular failure who received a continuous infusion of vasoactive drugs or inotropic drugs. After a baseline preimplementation period (phase 1), a standardized 'quick change method' of {CVIP} using two syringe drivers was implemented in our intensive care unit (phase 2). Endpoints (rate and distribution of incidents: variations of systolic blood pressure {\textgreater}20 {mmHg} or heart rate {\textgreater}20 beats/min, and arrhythmias) were registered in both 3-month phases. {RESULTS}: We studied a total of 913 {CVIP} events (phase 1, 435 events; phase 2, 478 events) from 43 patients. Patient characteristics were not significantly different among phases, with a majority of the patients having septic shock. The frequency of incidents was significantly (P {\textless} 0.0001) reduced in phase 2 (5.9\%, n = 28) versus phase 1 (17.8\%, n = 78). This effect was observed whichever catecholamine was used. More than 98\% of incidents were blood pressure variations, with a similar distribution of the nature of incidents in both phases. {CONCLUSION}: The present study illustrates that adverse events are common following {CVIP}, and illustrates the positive impact of a quality improvement program to enhance inpatient safety related to this current process of care."},
	pages = {R133},
	number = {6},
	journaltitle = {Critical Care},
	author = {Argaud, Laurent and Cour, Martin and Martin, Olivier and Saint-Denis, Marc and Ferry, Tristan and Goyatton, Agnes and Robert, Dominique},
	urldate = {2009-11-30},
	date = {2007},
	keywords = {Adverse events, a02}
}

@article{ravnskov_cholesterol_1992,
	title = {Cholesterol lowering trials in coronary heart disease: frequency of citation and outcome},
	volume = {305},
	issn = {0959-8138},
	url = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1882525/},
	shorttitle = {Cholesterol lowering trials in coronary heart disease},
	abstract = {Abstract: "{OBJECTIVE}--To see if the claim that lowering cholesterol values prevents coronary heart disease is true or if it is based on citation of supportive trials only. {DESIGN}--Comparison of frequency of citation with outcome of all controlled cholesterol lowering trials using coronary heart disease or death, or both, as end point. {SUBJECTS}--22 controlled cholesterol lowering trials. {RESULTS}--Trials considered by their directors as supportive of the contention were cited almost six times more often than others, according to Science Citation Index. Apart from trials discontinued because of alleged side effects of treatment, unsupportive trials were not cited after 1970, although their number almost equalled the number considered supportive. In three supportive reviews the outcome of the selected trials was more favourable than the outcome of the excluded and ignored trials. In the 22 controlled cholesterol lowering trials studied total and coronary heart disease mortality was not changed significantly either overall or in any subgroup. A statistically significant 0.32\% reduction in non-fatal coronary heart disease seemed to be due to bias as event frequencies were unrelated to trial length and to mean net reduction in cholesterol value; individual changes in cholesterol values were unsystematically or not related to outcome; and after correction for a small but significant increase in non-medical deaths in the intervention groups total mortality remained unchanged (odds ratio 1.02). {CONCLUSIONS}--Lowering serum cholesterol concentrations does not reduce mortality and is unlikely to prevent coronary heart disease. Claims of the opposite are based on preferential citation of supportive trials."},
	pages = {15--19},
	number = {6844},
	journaltitle = {{BMJ} (Clinical Research Ed.)},
	shortjournal = {{BMJ}},
	author = {Ravnskov, U},
	urldate = {2009-11-30},
	date = {1992-07-04},
	pmid = {1638188}
}

@article{mckay_review_2009,
	title = {A review of significant events analysed in general practice: implications for the quality and safety of patient care},
	volume = {10},
	issn = {1471-2296},
	url = {http://www.biomedcentral.com/1471-2296/10/61},
	doi = {10.1186/1471-2296-10-61},
	shorttitle = {A review of significant events analysed in general practice},
	abstract = {Abstract: "{BACKGROUND}: Significant event analysis ({SEA}) is promoted as a team-based approach to enhancing patient safety through reflective learning. Evidence of {SEA} participation is required for appraisal and contractual purposes in {UK} general practice. A voluntary educational model in the west of Scotland enables general practitioners ({GPs}) and doctors-in-training to submit {SEA} reports for feedback from trained peers. We reviewed reports to identify the range of safety issues analysed, learning needs raised and actions taken by {GP} teams. {METHOD}: Content analysis of {SEA} reports submitted in an 18 month period between 2005 and 2007.{RESULTS}:191 {SEA} reports were reviewed. 48 described patient harm (25.1\%). A further 109 reports (57.1\%) outlined circumstances that had the potential to cause patient harm. Individual 'error' was cited as the most common reason for event occurrence (32.5\%). Learning opportunities were identified in 182 reports (95.3\%) but were often non-specific professional issues not shared with the wider practice team. 154 {SEA} reports (80.1\%) described actions taken to improve practice systems or professional behaviour. However, non-medical staff were less likely to be involved in the changes resulting from event analyses describing patient harm (p {\textless} 0.05). {CONCLUSION}: The study provides some evidence of the potential of {SEA} to improve healthcare quality and safety. If applied rigorously, {GP} teams and doctors in training can use the technique to investigate and learn from a wide variety of quality issues including those resulting in patient harm. This leads to reported change but it is unclear if such improvement is sustained."},
	pages = {61},
	number = {1},
	journaltitle = {{BMC} Family Practice},
	author = {{McKay}, John and Bradley, Nick and Lough, Murray and Bowie, Paul},
	urldate = {2009-10-11},
	date = {2009},
	keywords = {Adverse events, a02}
}

@article{dyas_strategies_2009,
	title = {Strategies for improving patient recruitment to focus groups in primary care: a case study reflective paper using an analytical framework},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/65},
	doi = {10.1186/1471-2288-9-65},
	shorttitle = {Strategies for improving patient recruitment to focus groups in primary care},
	abstract = {Abstract: "{BACKGROUND}: Recruiting to primary care studies is complex. With the current drive to increase numbers of patients involved in primary care studies, we need to know more about successful recruitment approaches. There is limited evidence on recruitment to focus group studies, particularly when no natural grouping exists and where participants do not regularly meet. The aim of this paper is to reflect on recruitment to a focus group study comparing the methods used with existing evidence using a resource for research recruitment, {PROSPeR} (Planning Recruitment Options: Strategies for Primary Care). {METHODS}: The focus group formed part of modelling a complex intervention in primary care in the Resources for Effective Sleep Treatment ({REST}) study. Despite a considered approach at the design stage, there were a number of difficulties with recruitment. The recruitment strategy and subsequent revisions are detailed. {RESULTS}: The researchers' modifications to recruitment, justifications and evidence from the literature in support of them are presented. Contrary evidence is used to analyse why some aspects were unsuccessful and evidence is used to suggest improvements. Recruitment to focus group studies should be considered in two distinct phases; getting potential participants to contact the researcher, and converting those contacts into attendance. The difficulty of recruitment in primary care is underemphasised in the literature especially where people do not regularly come together, typified by this case study of patients with sleep problems. {CONCLUSIONS}: We recommend training {GPs} and nurses to recruit patients during consultations. Multiple recruitment methods should be employed from the outset and the need to build topic related non-financial incentives into the group meeting should be considered. Recruitment should be monitored regularly with barriers addressed iteratively as a study progresses."},
	pages = {65},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Dyas, Jane and Apeky, Tanefa and Tilling, Michelle and Siriwardena, A},
	urldate = {2009-09-29},
	date = {2009},
	keywords = {Accrual problems, a02}
}

@article{sanders_trials_2009,
	title = {Trials and tribulations of recruiting 2,000 older women onto a clinical trial investigating falls and fractures: Vital D study},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/78},
	doi = {10.1186/1471-2288-9-78},
	shorttitle = {Trials and tribulations of recruiting 2,000 older women onto a clinical trial investigating falls and fractures},
	abstract = {Abstract: "{BACKGROUND}: Randomised, placebo-controlled trials are needed to provide evidence demonstrating safe, effective interventions that reduce falls and fractures in the elderly. The quality of a clinical trial is dependent on successful recruitment of the target participant group. This paper documents the successes and failures of recruiting over 2,000 women aged at least 70 years and at higher risk of falls or fractures onto a placebo-controlled trial of six years duration. The characteristics of study participants at baseline are also described for this study. {METHODS}: The Vital D Study recruited older women identified at high risk of fracture through the use of an eligibility algorithm, adapted from identified risk factors for hip fracture. Participants were randomised to orally receive either 500,000 {IU} vitamin D3 (cholecalciferol) or placebo every autumn for three to five consecutive years. A variety of recruitment strategies were employed to attract potential participants. {RESULTS}: Of the 2,317 participants randomised onto the study, 74\% (n= 1716/ 2317) were consented onto the study in the last five months of recruiting. This was largely due to the success of a targeted mail-out. Prior to this only 541 women were consented in the 18 months of recruiting. A total of 70\% of all participants were recruited as a result of targeted mail-out. The response rate from the letters increased from 2 to 7\% following revision of the material by a public relations company. Participant demographic or risk factor profile did not differ between those recruited by targeted mail-outs compared with other methods. {CONCLUSIONS}: The most successful recruitment strategy was the targeted mail-out and the response rate was no higher in the local region where the study had extensive exposure through other recruiting strategies. The strategies that were labour-intensive and did not result in successful recruitment include the activities directed towards the {GP} medical centres. Comprehensive recruitment programs employ overlapping strategies simultaneously with ongoing assessment of recruitment rates. In our experience, and others direct mail-outs work best although rights to privacy must be respected." Trial registration: {ISRCTN}83409867 and {ACTR}12605000658617.},
	pages = {78},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Sanders, Kerrie and Stuart, Amanda and Merriman, Elizabeth and Read, Meaghan and Kotowicz, Mark and Young, Doris and Taylor, Roderick and Blair-Holt, Ian and Mander, Alistair and Nicholson, Geoffrey},
	urldate = {2009-11-30},
	date = {2009},
	keywords = {Accrual problems, a02}
}

@article{chang_participant_2009,
	title = {Participant recruitment and retention in a pilot program to prevent weight gain in low-income overweight and obese mothers},
	volume = {9},
	issn = {1471-2458},
	url = {http://www.biomedcentral.com/1471-2458/9/424},
	doi = {10.1186/1471-2458-9-424},
	abstract = {Abstract: "Background: Recruitment and retention are key functions for programs promoting nutrition and other lifestyle behavioral changes in low-income populations. This paper describes strategies for recruitment and retention and presents predictors of early (two-month post intervention) and late (eight-month post intervention) dropout (non retention) and overall retention among young, low-income overweight and obese mothers participating in a community-based randomized pilot trial called Mothers In Motion. Methods: Low-income overweight and obese African American and white mothers ages 18 to 34 were recruited from the Special Supplemental Nutrition Program for Women, Infants, and Children in southern Michigan. Participants (n = 129) were randomly assigned to an intervention (n = 64) or control (n = 65) group according to a stratification procedure to equalize representation in two racial groups (African American and white) and three body mass index categories (25.0-29.9 kg/m2, 30.0-34.9 kg/m2, and 35.0-39.9 kg/m2). The 10-week theory-based culturally sensitive intervention focused on healthy eating, physical activity, and stress management messages that were delivered via an interactive {DVD} and reinforced by five peer-support group teleconferences. Forward stepwise multiple logistic regression was performed to examine whether dietary fat, fruit and vegetable intake behaviors, physical activity, perceived stress, positive and negative affect, depression, and race predicted dropout as data were collected two- month and eight-month after the active intervention phase. Results: Trained personnel were successful in recruiting subjects. Increased level of depression was a predictor of early dropout (odds ratio = 1.04; 95\% {CI} = 1.00, 1.08; p = 0.03). Greater stress predicted late dropout (odds ratio = 0.20; 95\% {CI} = 0.00, 0.37; p = 0.01). Dietary fat, fruit, and vegetable intake behaviors, physical activity, positive and negative affect, and race were not associated with either early or late dropout. Less negative affect was a marginal predictor of participant retention (odds ratio = 0.57; 95\% {CI} = 0.31, 1.03; p = 0.06). {CONCLUSIONS}: Dropout rates in this study were higher for participants who reported higher levels of depression and stress." Trial registration: Current Controlled Trials {NCT}00944060},
	pages = {424},
	number = {1},
	journaltitle = {{BMC} Public Health},
	author = {Chang, Mei-Wei and Brown, Roger and Nitzke, Susan},
	urldate = {2009-11-30},
	date = {2009},
	keywords = {Accrual problems, a02}
}

@article{sherman_comparing_2009,
	title = {Comparing recruitment strategies in a study of acupuncture for chronic back pain},
	volume = {9},
	issn = {1471-2288},
	url = {http://www.biomedcentral.com/1471-2288/9/69},
	doi = {10.1186/1471-2288-9-69},
	abstract = {Abstract: "{BACKGROUND}: Meeting recruitment goals is challenging for many clinical trials conducted in primary care populations. Little is known about how the use of different recruitment strategies affects the types of individuals choosing to participate or the conclusions of the study. {METHODS}: A secondary analysis was performed using data from participants recruited to a clinical trial evaluating acupuncture for chronic back pain among primary care patients in a large integrated health care organization. We used two recruitment methods: mailed letters of invitation and an advertisement in the health plan's magazine. For these two recruitment methods, we compared recruitment success (\% randomized, treatment completers, drop outs and losses to follow-up), participant characteristics, and primary clinical outcomes. A linear regression model was used to test for interaction between treatment group and recruitment method. {RESULTS}: Participants recruited via mailed letters closely resembled those responding to the advertisement in terms of demographic characteristics, most aspects of their back pain history and current episode and beliefs and expectations about acupuncture. No interaction between method of recruitment and treatment group was seen, suggesting that study outcomes were not affected by recruitment strategy. {CONCLUSION}: In this trial, the two recruitment strategies yielded similar estimates of treatment effectiveness. However, because this finding may not apply to other recruitment strategies or trial circumstances, trials employing multiple recruitment strategies should evaluate the effect of recruitment strategy on outcome." Trial registration: Clinical Trials.gov {NCT} 00065585.},
	pages = {69},
	number = {1},
	journaltitle = {{BMC} Medical Research Methodology},
	author = {Sherman, Karen and Hawkes, Rene and Ichikawa, Laura and Cherkin, Daniel and Deyo, Richard and Avins, Andrew and Khalsa, Partap},
	urldate = {2009-10-28},
	date = {2009},
	keywords = {Accrual problems, a02}
}

@book{chang_classical_nodate,
	location = {Hoboken, {NJ}},
	title = {Classical and adaptive clinical trial designs with {ExpDesign} Studio},
	isbn = {9780470276129 (cloth/cd)},
	url = {http://lccn.loc.gov/2008001358},
	shorttitle = {Classical and adaptive clinical trial designs with {ExpDesign} Studio?},
	abstract = {Excerpt: "This book introduces pharmaceutical statisticians, scientists, researchers, and others to {ExpDesign} Studio software for classical and adaptive designs of clinical trials. It includes the Professional Version 5.0 of {ExpDesign} Studio software that frees pharmaceutical professionals to focus on drug development and related challenges while the software handles the essential calculations and computations. After a hands-on introduction to the software and an overview of clinical trial designs encompassing numerous variations, Classical and Adaptive Clinical Trial Designs Using {ExpDesign} Studio: * Covers both classical and adaptive clinical trial designs, monitoring, and analyses * Explains various classical and adaptive designs including groupsequential, sample-size reestimation, dropping-loser, biomarker-adaptive, and response-adaptive randomization designs * Includes instructions for over 100 design methods that have been implemented in {ExpDesign} Studio and step-by-step demos as well as real-world examples * Emphasizes applications, yet covers key mathematical formulations * Introduces readers to additional toolkits in {ExpDesign} Studio that help in designing, monitoring, and analyzing trials, such as the adaptive monitor, graphical calculator, the probability calculator, the confidence interval calculator, and more * Presents comprehensive technique notes for sample-size calculation methods, grouped by the number of arms, the trial endpoint, and the analysis basis Written with practitioners in mind, this is an ideal self-study guide for not only statisticians, but also scientists, researchers, and professionals in the pharmaceutical industry, contract research organizations ({CROs}), and regulatory bodies. It's also a go-to reference for biostatisticians, pharmacokinetic specialists, and principal investigators involved in clinical trials."},
	publisher = {John Wiley},
	author = {Chang, Mark},
	keywords = {Accrual problems, a02}
}