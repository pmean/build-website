---
title: Illustrating the lasso model using the Worcester Heart Attack Study
source: "New"
author: Steve Simon
date: 2024-05-07
categories:
- Blog post
tags:
- Survival analysis
- R programming
- Rmarkdown graphs
output: html_document
page_update: complete
---

The lasso model allows you to effectively identify important features in a large regression model. Here is an illustration of how to use a the lasso model with survival data from the Worcester Heart Attack Study.

I've followed the [glmnet vignette for Cox regression][tay1] closely. You might find that explanation to be clearer than what I present below.

[tay1]: https://glmnet.stanford.edu/articles/Coxnet.html

<!---more--->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(glmnet)
library(glue)
library(readr)
library(survival)
library(tidyverse)
library(yaml)
g0 <- "https://raw.githubusercontent.com/pmean/"
r0 <- "datasets/master/"
s1 <- "yaml-functions.R"
f1 <- "whas500.yaml"

source(paste0(g0, r0, s1))
dd <- read_yaml(paste0(g0, r0, f1))
vnames <- extract_names(dd)
vlabels <- extract_labels(dd)
vscales <- extract_scales(dd)
```

## Description of the whas500 dataset

Here is a brief description of the whas500 dataset, taken from the [data dictionary][sim3] on my github site.

`r dd$description`

[sim3]: https://raw.githubusercontent.com/pmean/datasets/master/whas100-data-dictionary.yaml

Here are the first few rows of data and the last few rows of data. Row 101 needs to be removed.

```{r, echo=FALSE}
vnames <- names(dd$vars)
# for some reason, R wants to read in 23 
# columns rather than 22. The last column 
# is all blanks and will be tossed out.
#
# Dates are always a bit messy and I do
# not need them for this analysis, so I
# will keep things simple and read them 
# in as strings.
f2 <- "whas500.dat"
w0 <- read_table(
  file=paste0(g0, r0, f2), 
  col_names=vnames,
  col_types="nnnnnnnnnnnnnnncccnnnn")
w1 <- data.frame(w0)
head(w1)
```

Here are a few descriptive statistics

```{r, echo=FALSE}
categorical <- 
  (vscales=="binary") |
  (vscales=="nominal") |
  (vscales=="ordinal")
for (i_vars in which(categorical)) {
  cat(glue("{vnames[i_vars]}: {vlabels[i_vars]}"), "\n")
  w1 %>%
    count(.data[[vnames[i_vars]]]) %>%
    data.frame %>%
    print
  cat("\n")
}
```

```{r, echo=FALSE}
continuous <- 
  (vscales=="interval") |
  (vscales=="ratio")
for (i_vars in which(continuous)) {
  cat(glue("{vnames[i_vars]}: {vlabels[i_vars]}"), "\n")
  print(summary(w1[vnames[i_vars]]))
  cat("\n")
}
```

## Simple analysis

I am going to show the R code from this point onward so you can see the actual programming required to fit a lasso model.

Here is an overall survival curve.

```{r}
plot(Surv(w1$lenfol, w1$fstat))
```

Here is the traditional Cox model

```{r}
simple_fit <- coxph(
  Surv(w1$lenfol, w1$fstat) ~
      cvd +
      afb + 
      sho + 
      chf + 
      av3 +
      miord +
      mitype,
    data=w1)
coef(simple_fit)
```

## The lasso model

You can find the lasso regression model in the glmnet package. One important difference with glmnet is that it does not use a formula to define your model. Instead you provide matrices for your dependent variable and your independent variables. In a Cox regression model, the dependent matrix has a column for time and a second column with an indicator variable (1 means the event occurred and 0 means censoring).

```{r}
ymat <- cbind(w1$lenfol, w1$fstat)
dimnames(ymat)[2] <- list(c("time", "status"))
xmat <- as.matrix(w1[ , 8:14])
dimnames(xmat)[2] <- list(vnames[8:14])
lasso_fit <- glmnet(xmat, ymat, family="cox", standardize=FALSE)
plot(lasso_fit, xvar="lambda", label=TRUE)
```

Let's spend a bit of time looking at this graph. The left hand side of the graph represents a very small penalty.

```{r}
fit_left <- as.matrix(coef(lasso_fit, min(lasso_fit$lambda)))
fit_left
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(min(lasso_fit$lambda)), 
  fit_left, 
  unlist(dimnames(fit_left)[1]))
```

Notice how the coefficients are very close to the Cox regression model. There is very little shrinkage and only one variable, av3, are zeroed out, meaning that it is totally removed from the model.

```{r}
fit_median <- as.matrix(coef(lasso_fit, median(lasso_fit$lambda)))
fit_median
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(median(lasso_fit$lambda)), 
  fit_median, 
  unlist(dimnames(fit_median)[1]))
```

At the middle (median) value of lambda, several of the variables are zeroed out. Most other variables are shrunk towards zero, relative to the fit with the smallest value of lambda.

What happens at the largest value of lambda?

```{r}
fit_right <- as.matrix(coef(lasso_fit, max(lasso_fit$lambda)))
fit_right
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(max(lasso_fit$lambda)), 
  fit_right, 
  unlist(dimnames(fit_right)[1]))
```

Once lambda gets large enough, the penalty is so great that the algorithm zeroes out every single variable.

## Cross-validation to select the optimal value of lambda

Now you need to decide what lambda works best. You can do this through cross-validation.

```{r}
cv_fit <- cv.glmnet(xmat, ymat, family = "cox", standardize=FALSE)
plot(cv_fit)
```

There are two choices that are commonly selected. 

You could set the value of lambda to the value that minimizes the criteria. The default criteria is the partial likelihood deviance, but you can use other criteria to pick out the "best" value of lambda.

```{r}
cv_fit$lambda.min
plot(lasso_fit, xvar="lambda")
abline(v=log(cv_fit$lambda.min))
fit_min <- as.matrix(coef(lasso_fit, cv_fit$lambda.min))
fit_min
text(log(cv_fit$lambda.min), fit_min, unlist(dimnames(fit_min)[1]))
```

The developers of glmnet felt that the value of lambda that minimized a fit criteria was not aggressive enough in zeroing out and shrinking coefficients. They suggested that instead of choosing the "best" value of lambda, choose the largest value of lambda that is still within one standard error of the criteria produced by the "best" lambda.

```{r}
plot(lasso_fit, xvar="lambda", label=TRUE)
abline(v=log(cv_fit$lambda.1se))
cv_fit$lambda.1se
fit_1se <- as.matrix(coef(lasso_fit, cv_fit$lambda.1se))
fit_1se
text(log(cv_fit$lambda.1se), fit_1se, unlist(dimnames(fit_1se)[1]))
```

Using this criteria, there are only two variables which help in predicting risk of death, and one of them is very weak because its coefficient is very close to zero.
