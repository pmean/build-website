---
title: "A second alternative to the pairwise testing approach"
author: "Steve Simon"
date: '2022-07-04'
output:
  word_document: default
  html_document: default
category: Blog post
tags: Analysis of means
source: new
---

In an earlier blog post, I mentioned Dunnett's test, a comparison of multiple treatments to a control group as an alternative to the pairwise approach to testing that is commonly used in analysis of variance. Here's a second interesting alternative.

You are in a setting where you are expecting to see a degree of uniformity among several groups. For example, you have a group of clinics who all treat the same type of patients. The patients themselves may differ markedly from one to another, but they are assigned in a more or less random way to each clinic. So, although one patient may differ markedly from another, the average result should be the same for across all the clinics. 

The time spent with each patient, the total cost of the medications prescribed, the length of the notes written in the medical record--these can differ from patient to patient. But because of how the patients are assigned to a particular clinic, these should balance out on average. If there is one clinic that has different average results, it has to be because that the physicians at that clinic have a different approach to treatment. If you can identify a clinic (it could be more than one clinic, of course) that differs from the norm, then investigate. Perhaps that clinic is doing something "wrong" that causes the discrepancy, or maybe that clinic is the only one doing things right. In any case understanding why you see unexpected deviations in a process that should be uniform across clinics provides you with an opportunity to learn something.

### The traditional hypothesis

You've seen this hypothesis for the standard ANOVA setting in your introductory Statistics class

```{r, fig.width=5, fig.height=0.33, echo=FALSE}
h0 <- bquote(H[0] ~ ": " ~ mu[i] == mu[j] ~ " for all i,j")
par(mar=rep(0.1, 4))
plot(0:1, 0:1, axes=FALSE, type="n")
text(0.1, 0.5, h0, adj=0)
```

This is a very common approach in a comparison of k independent groups. Test for any deviation from the null hypothesis using an F test. If this is statistically significant, then use a Tukey follow-up test to see which pairs of means differ from one another.

### The comparison to the overall mean hypothesis

Suppose you are in a setting like the one described above, where there is an expectation of similarity between each group mean and the overall mean. Then you would write your hypothesis as 
```{r, fig.width=5, fig.height=0.33, echo=FALSE}
h0 <- bquote(H[0] ~ ": " ~ mu[i] == mu ~ " for all i=1,...,k")
par(mar=rep(0.1, 4))
plot(0:1, 0:1, axes=FALSE, type="n")
text(0.1, 0.5, h0, adj=0)
```

This approach is not that common, but you can find it in most statistical software programs under the name "Analysis of Means." It fits in well with a setting where you hope that all the groups produce consistent results. If any do not, then you want to identify the group or groups that deviate from the norm and study factors that may account for the deviation.

The analysis of means approach that compares each group mean to the overall mean is easy to implement and it lends itself to a simple graphical display.

You don't need to compute the traditional F statistic for Analysis of Variance first, because the Analysis of Means approach controls the overall Type I error rate. This protects you from the accusation of p-hacking, even if the number of groups is very large.

It's important to editorialize a bit here. Deviating from the norm could be a good thing or a bad thing or it could be an indifferent thing. Your goal is not to use statistics to hunt out different groups to reward or punish them. You are using statistics to help in understanding why deviations from the norm occur.

### A simple example

Five genetically modified strains of eggplant were tested for presence of a natural insecticide, CryA1c, in the leaves of the plant. Four plants were assessed for each strain. Here are the descriptive statistics. The research question is whether all strains have a comparable amount of CryA1c in their leaves.

![](http://www.pmean.com/new-images/22/analysis-of-means-00.png) 

(Image taken from Wikipedia)

Desiree M. Hautea DM et al. Field Performance of Bt Eggplants (Solanum melongena L.) in the Philippines: Cry1Ac Expression and Control of the Eggplant Fruit and Shoot Borer (Leucinodes orbonalis GuenÃ©e). PLoS One. 2016; 11(6): e0157498.

https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0157498


Here are the summary statistics.

```{r, echo=FALSE}
mn <- c(
  21.83,
  20.40,
  21.62,
  21.06,
  20.57)

sd <- c(
  1.17,
  1.09,
  1.32,
  1.11,
  0.99)

mse <- mean(sd^2)
ybarbar <- mean(mn)
t <- 5
N <- 20
h <- 2.88
hi <- ybarbar+h*sqrt((t-1)*mse/N)
lo <- ybarbar-h*sqrt((t-1)*mse/N)
```  

![](http://www.pmean.com/new-images/22/analysis-of-means-01.png)  

The raw data was not available, but you can calculate the analysis of means limits using the group means and standard deviations. You can plot the individual means versus the limits provided by analysis of means calculations.

![](http://www.pmean.com/new-images/22/analysis-of-means-02.png)  

Notice that all five means lie inside the limits. None of the five strains shows a  statisticially significant difference from the overall mean.

### Caveats

The formulas get a bit more complex and the graph gets a bit messier if the sample size differs from one group to another.

More importantly, the only comparisons that you can make with the analysis of means approach is a comparison to the overall mean. If two groups are both statistically significantly higher than the overall mean, you cannnot make a comparison between those two groups without losing control over the overall Type I error rate.

The Analysis of Means, just like the Dunnett's test described in an earlier blog post, provides a simple approach to testing a different type of hypothesis. Because it reduces the number of comparisons from all possible pairwise differences to a comparison of each group to the overall mean, you gain some precision and can summarize your results in a simple easily understood graph.