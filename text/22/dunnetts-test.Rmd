---
title: "Dunnett's test"
author: "Steve Simon"
date: '2022-07-04'
output:
  word_document: default
  html_document: default
category: Blog post
tags: Analysis of means
source: new
---

I'm a big fan of Analysis of Variance (ANOVA). I use it all the time. I learn a lot from it. But sometimes I want something a bit different. And the difference comes down to how you specify the hypothesis.

### The traditional hypothesis

Recall the way in which the hypothesis test is commonly set up when you are comparing three of more groups using analysis of variable.

```{r, fig.width=5, fig.height=1, echo=FALSE}
h0 <- bquote(H[0] ~ ": " ~ mu[i] == mu[j] ~ " for all i,j")
h1 <- bquote(H[1] ~ ": " ~ mu[i] != mu[j] ~ " for at least one i,j")
par(mar=rep(0.1, 4))
plot(0:1, 0:1, axes=FALSE, type="n")
text(0.1, 0.7, h0, adj=0)
text(0.1, 0.3, h1, adj=0)
```

This is a very common approach in a comparison of k independent groups. Test for any deviation from the null hypothesis using an F test. If this is statistically significant, then use a Tukey follow-up test to see which pairs of means differ from one another.

This hypothesis specifies equality by specifying that every pair of means is equal. For four groups, this implies six equalities: 1=2, 1=3, 1=4, 2=3, 2=4, and 3=4. For six groups, you would have fifteen equalities; for ten you'd have forty five.

It gets messy very fast. That's okay. If you have a lot of groups that you are comparing, you have to make it has to involve a lot of comparisons. Or do you?

### The placebo hypothesis

If one of the groups is a control or placebo, then you might consider an alternative formulation. Here's what the hypothesis looks like, assuming that the control is group #1.

```{r, fig.width=5, fig.height=1, echo=FALSE}
h0 <- bquote(H[0] ~ ": " ~ mu[i] == mu[1] ~ " for all i=2,...,k")
h1 <- bquote(H[1] ~ ": " ~ mu[i] != mu[1] ~ " for at least one i")
par(mar=rep(0.1, 4))
plot(0:1, 0:1, axes=FALSE, type="n")
text(0.1, 0.7, h0, adj=0)
text(0.1, 0.3, h1, adj=0)
```

There's a procedure for this, Dunnett's test. It involves only three comparisons if you have four groups total, and only nine comparisons if you have ten groups total. You can quickly identify who is better than the control. This also gives you simplicity and a bit of extra power and precision.

Dunnett's test is easy to implement. Calculate the traditional measures in analysis of variance, including mean squared error (MSE). You can skip the traditional F Test that you see in most ANOVA tables. It won't hurt your overall alpha level, as long as you only look at each treatment versus the control. Simply compare each treatment mean minus the control mean to a cutoff value

Reject $H_0$ if $|\bar X_i-\bar X_1| \geq d(\alpha, g, n) \sqrt{2MSE/n}$

where d is obtained from a table such as [this one][bobb1] on the [Statology blog][bobb0]. The value of d depends on alpha (the desired Type I error rate for a two-sided test), g (the number of groups including the control group) and n (the number of data points in each group). Note that some tables define g as the number of groups excluding the control group.

Most statistical software packages will include Dunnett's test as an option for ANOVA. Just remember that you don't need to use the initial F test as a screen before jumping into the Dunnett's test.

[bobb1]: https://www.statology.org/dunnetts-table/

[bobb0]: https://www.statology.org/about/

### Caveats

Modifications of Dunnett's test are tricky when the group sizes are unequal.

Now you lose something when you simplify the hypothesis. Suppose you have six groups, a control and five different treatments. Now imagine that all of the treatments are significantly better than the control group. Jackpot! Every treatment is worth further study. But Dunnett's test won't allow you to see if some of the treatments are better than the others. There is no option for finding the best of the best.

Let's consider as a reminder that there is no such thing as a free lunch. There are always trade-offs. No approach is superior in all settings.