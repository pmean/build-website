---
title: Illustrating the lasso model using the Worcester Heart Attack Study
source: "New"
author: Steve Simon
date: 2024-05-07
categories:
- Blog post
tags:
- Survival analysis
- R programming
output: html_document
---

The lasso model allows you to effectively identify important features in a large regression model. Here is an illustration of how to use a the lasso model with survival data from the Worcester Heart Attack Study.

I've followed the [glmnet vignette for Cox regression][tay1] closely. You might find that explanation to be clearer than what I present below.

[tay1]: https://glmnet.stanford.edu/articles/Coxnet.html

<!---more--->

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(broom)
library(glmnet)
library(glue)
library(pspline)
library(readr)
library(rms)
library(survival)
library(tidyverse)
library(yaml)
g0 <- "https://raw.githubusercontent.com/pmean/"
r0 <- "datasets/master/"
s1 <- "yaml-functions.R"
f1 <- "whas500.yaml"

source(paste0(g0, r0, s1))
dd <- read_yaml(paste0(g0, r0, f1))
vnames <- extract_names(dd)
vlabels <- extract_labels(dd)
vscales <- extract_scales(dd)
```

Here is a brief description of the whas500 dataset, taken from the [data dictionary][sim3] on my github site.

`r dd$description`

[sim3]: https://raw.githubusercontent.com/pmean/datasets/master/whas100-data-dictionary.yaml

Here are the first few rows of data and the last few rows of data. Row 101 needs to be removed.

```{r}
vnames <- names(dd$vars)
f2 <- "whas500.dat"
w0 <- read_table(
  file=paste0(g0, r0, f2), 
  col_names=c(vnames),
  col_types="nnnnnnnnnnnnnnncccnnnn")
head(w0)
```

Here are a few descriptive statistics

```{r}
categorical <- 
  (vscales=="binary") |
  (vscales=="nominal") |
  (vscales=="ordinal")
for (i_vars in which(categorical)) {
  cat(glue("{vnames[i_vars]}: {vlabels[i_vars]}"), "\n")
  w0 %>%
    count(.data[[vnames[i_vars]]]) %>%
    data.frame %>%
    print
  cat("\n")
}
```

```{r}
continuous <- 
  (vscales=="interval") |
  (vscales=="ratio")
for (i_vars in which(continuous)) {
  cat(glue("{vnames[i_vars]}: {vlabels[i_vars]}"), "\n")
  print(summary(w0[vnames[i_vars]]))
  cat("\n")
}
```


Here is an overall survival curve.

```{r}
plot(Surv(w0$lenfol, w0$fstat))
```

Here is the traditional Cox model

```{r}
simple_fit <- coxph(
  Surv(w0$lenfol, w0$fstat) ~
      cvd +
      afb + 
      sho + 
      chf + 
      av3 +
      miord +
      mitype,
    data=w0)
coef(simple_fit)
```

You can find the lasso regression model in the glmnet package. One important difference with glmnet is that it does not use a formula to define your model. Instead you provide matrices for your dependent variable and your independent variables. In a Cox regression model, the dependent matrix has a column for time and a second column with an indicator variable (1 means the event occurred and 0 means censoring).

```{r}
ymat <- cbind(w0$lenfol, w0$fstat)
dimnames(ymat)[2] <- list(c("time", "status"))
xmat <- as.matrix(w0[ , 8:14])
dimnames(xmat)[2] <- list(vnames[8:14])
lasso_fit <- glmnet(xmat, ymat, family="cox", standardize=FALSE)
plot(lasso_fit, xvar="lambda", label=TRUE)
```

Let;s spend a bit of time looking at this graph. The left hand side of the graph represents a very small penalty.

```{r}
fit_left <- as.matrix(coef(lasso_fit, min(lasso_fit$lambda)))
fit_left
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(min(lasso_fit$lambda)), 
  fit_left, 
  unlist(dimnames(fit_left)[1]))
```

Notice how the coefficients are very close to the Cox regression model. There is very little shrinkage and only one variable, av3, are zeroed out, meaning that it is totally removed from the model.

```{r}
fit_median <- as.matrix(coef(lasso_fit, median(lasso_fit$lambda)))
fit_median
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(median(lasso_fit$lambda)), 
  fit_median, 
  unlist(dimnames(fit_median)[1]))
```

At the middle (median) value of lambda, several of the variables are zeroed out. Most other variables are shrunk towards zero, relative to the fit with the smallest value of lambda.

What happens at the largest value of lambda.

```{r}
fit_right <- as.matrix(coef(lasso_fit, max(lasso_fit$lambda)))
fit_right
plot(lasso_fit, xvar="lambda", label=TRUE)
text(
  log(max(lasso_fit$lambda)), 
  fit_right, 
  unlist(dimnames(fit_right)[1]))
```

Once lambda gets large enough, the penalty is so great that the algorithm zeroes out every single variable.

Now you need to decide what lambda works best. You can do this through cross-validation.

```{r}
cv_fit <- cv.glmnet(xmat, ymat, family = "cox", standardize=FALSE)
plot(cv_fit)
```

There are two choices that are commonly selected. 

You could set the value of lambda to the value that minimizes the criteria. The default criteria is the partial likelihood deviance, but you can use other criteria to pick out the "best" value of lambda.

```{r}
cv_fit$lambda.min
plot(lasso_fit, xvar="lambda")
abline(v=log(cv_fit$lambda.min))
fit_min <- as.matrix(coef(lasso_fit, cv_fit$lambda.min))
fit_min
text(log(cv_fit$lambda.min), fit_min, unlist(dimnames(fit_min)[1]))
```

The developers of glmnet felt that the value of lambda that minimized a fit criteria was not aggressive enough in zeroing out and shrinking coefficients. They suggested that instead of choosing the "best" value of lambda, choose the largest value of lambda that is still within one standard error of the criteria produced by the "best" lambda.

```{r}
plot(lasso_fit, xvar="lambda", label=TRUE)
abline(v=log(cv_fit$lambda.1se))
cv_fit$lambda.1se
fit_1se <- as.matrix(coef(lasso_fit, cv_fit$lambda.1se))
fit_1se
text(log(cv_fit$lambda.1se), fit_1se, unlist(dimnames(fit_1se)[1]))
```

Using this criteria, there are only two variables which help in predicting risk of death, and one of them is very weak because its coefficient is very close to zero.
