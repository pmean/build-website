---
title: "The hedging hyperprior for the exponential distribution"
source: "New"
date: "2023-02-28"
categories:
- Blog post
tags:
- Bayesian statistics
output: html_document
---

```{r setup}
suppressMessages(suppressWarnings(library(tidyverse)))
knitr::opts_chunk$set(echo=TRUE, fig.width=8, fig.height=2)
```

I want to write a paper about the hedging hyperprior. I need to work out some simple examples first. Here is an example involving the gamma distribution.

<!---more--->

### A simple Baysian analysis with an informative prior.

Consider a Bayesian analysis of an outcome variable that has an exponential distribution. The density of the gamma distribution is

$f(x) = \theta^{-1} e^{-x / \theta}$

You have an informative prior on $\theta$ that has an inverse gamma distribution

$g(\theta; \alpha, \beta)=\frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1}e^{-\beta / \theta}$

and you collect a sample of n data points, $\mathbf{X} = X_1, X_2, ..., X_n$. This gives you a likelihood of 

$L(\mathbf{X})=\prod_{i=1}^n f(X_i)=\theta^{-n} e^{-n \bar X / \theta}$

and a posterior distribution of 

$h(\theta | \mathbf{X}) \propto \frac{\beta^\alpha}{\Gamma(\alpha)}\theta^{-\alpha-1}e^{-\beta / \theta}\theta^{-n} e^{-n \bar X / \theta}$

which simplifies to

$h(\theta | \mathbf{X}) \propto \theta^{-\alpha-1-n}e^{-(\beta + n\bar X) / \theta}$

which, thanks to the miracle of conjugancy, is an inverse gamma distribution. The posterior mean

$\frac{\beta + n\bar X}{\alpha-1+n}$

can be written as a weighted average of the prior mean and the mean of the observed data.

$\Big(\frac{\alpha-1}{\alpha-1+n}\Big)\frac{\beta}{\alpha-1}+\Big(\frac{n}{\alpha-1+n}\Big)\bar X$

```{r}

a0 <- 10
b0 <- 30

n <- 30
xbar <- 0.7

a1 <- a0 + n
b1 <- b0 + n*xbar

posterior_alpha <- a0+n
posterior_beta <- b0+n*xbar

x_grid <- seq(0.01, 8, by=0.01)

f <- function(x, theta) {
  theta^(-1) * 
    exp(-x/theta)
}

g <- function(theta, alpha, beta) {
  beta^alpha/gamma(alpha) *
    theta^(-alpha-1) * 
    exp(-beta/theta)
}

prior05 <- 1/qgamma(c(0.95), a0, b0) 
prior95 <- 1/qgamma(c(0.05), a0, b0) 

post05 <- 1/qgamma(c(0.95), a1, b1) 
post95 <- 1/qgamma(c(0.05), a1, b1) 

```

Let's illustrate this with an informative prior with $\alpha$ = `r a0` and $\beta$ = `r b0`.

```{r}
g_grid <- g(x_grid, a0, b0)
sum(g_grid)*0.01 # Check the g is a density.
outside <- x_grid < prior05 | x_grid > prior95
x_color <- 
  ifelse(outside, "white", "gray")
data.frame(x=x_grid, y=g_grid) %>%
  ggplot(aes(x, y)) +
    geom_segment(aes(xend=x, yend=0), color=x_color) +
    geom_line() + xlab(" ") + ylab(" ")
```

This corresponds to a fairly informative prior distribution. This distribution places $\theta$ within `r sprintf("%.2f", prior05)` and `r sprintf("%.2f", prior95)` with 95% probability.

Let's see what happens with this informative prior if you observe `r n` values of x with a mean of `r xbar`. The likelihood is

```{r}
f_grid <- f(xbar, x_grid)^10
x_color <- "white"
data.frame(x=x_grid, y=f_grid) %>%
  ggplot(aes(x, y)) +
    geom_line() + xlab(" ") + ylab(" ")
```

Multiply these two together to get the posterior distribution.

```{r}
h_grid <- g(x_grid, a1, b1)
outside <- x_grid < post05 | x_grid > post95
x_color <- 
  ifelse(outside, "white", "gray")
data.frame(x=x_grid, y=h_grid) %>%
  ggplot(aes(x, y)) +
    geom_segment(aes(xend=x_grid, yend=0), color=x_color) +
    geom_line() + xlab(" ") + ylab(" ")
```

The posterior distribution is not close to either the prior distribution or the likelihood. It places $\theta$ within `r sprintf("%.2f", post05)` and `r sprintf("%.2f", post95)` with 95% probability. It excludes the prior mean of `r sprintf("%.2f", b0/(a0-1))` and it excludes `r xbar`, the mean of the data.

### The power prior

The power prior was developed to combat the excessive influence of an informative prior when the data conflicts with that prior.

Instead of the standard Bayesian approach where you multiply the Likelihood (L) by the prior (g), you use

$h(\theta|\mathbf{X}) \propto L(\mathbf{X}|\theta)g(\theta)^\tau g_0(theta)$

where $g_0$ is a non-informative prior and $\tau$ is between 0 and 1.

For $\tau$=1, the prior is used at full strength. When $\tau$=0, the informative prior effectively disappears, leaving you with just the non-informative prior. For values of $\tau$ between 0 and 1, the informative prior is weakened, but still exerts some pull on the posterior distribution.

For intermediate values of $\tau$, the formulation is not consistent with Bayes rule and you need to include a "fudge factor" of

$\int g(\theta)^{\tau}g_0(\theta)d\theta$

A simpler approach is to include $\tau$ in the prior distribution and then place a hyperprior on $\tau$.

The parameter $\alpha$ in an inverse gamma distribution can be thought of as a prior sample size. Large values of $\alpha$ produce informative priors that are equivalent to adding $\alpha$ pseudo-data values. If you multiply $\alpha$ by $\tau$, you can reduce the amount of pseudo data being added.

### The hedging hyperprior

